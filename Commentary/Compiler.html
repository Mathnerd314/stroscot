<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Compiler design &mdash; Stroscot  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/hexagon_favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Compiler output" href="CompilerOutput.html" />
    <link rel="prev" title="Code generation" href="Code-Generation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/hexagon_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/index.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../HowTo/index.html">How to</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Reference/index.html">Language Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Commentary</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Assembly.html">Assembly</a></li>
<li class="toctree-l2"><a class="reference internal" href="BuildSystem.html">Build system</a></li>
<li class="toctree-l2"><a class="reference internal" href="Checklist.html">Checklist</a></li>
<li class="toctree-l2"><a class="reference internal" href="Code-Generation.html">Code generation</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Compiler design</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#structure">Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization">Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cross-compilation">Cross compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bootstrapping">Bootstrapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="#complex-bootstrap">Complex bootstrap</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compile-time-code-execution">Compile-time code execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiler-ways">Compiler ways</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiler-memory-management">Compiler memory management</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-execution">Dynamic execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#debugger">Debugger</a></li>
<li class="toctree-l3"><a class="reference internal" href="#profiler">Profiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ir-dump">IR dump</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evolution">Evolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#incremental-compilation">Incremental compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hot-reloading">Hot reloading</a></li>
<li class="toctree-l3"><a class="reference internal" href="#literal-vm">Literal VM</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="CompilerOutput.html">Compiler output</a></li>
<li class="toctree-l2"><a class="reference internal" href="Concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoreSyntax.html">Core syntax</a></li>
<li class="toctree-l2"><a class="reference internal" href="Dispatch.html">Dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="Errors.html">Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="Evaluation-Strategy.html">Evaluation strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="Exceptions.html">Exceptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="Fastest.html">As fast as C</a></li>
<li class="toctree-l2"><a class="reference internal" href="IR.html">Intermediate representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="Intrinsics.html">Intrinsics</a></li>
<li class="toctree-l2"><a class="reference internal" href="Library.html">Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="Logic.html">Logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="LogicProgramming.html">Logic programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Macros.html">Macros</a></li>
<li class="toctree-l2"><a class="reference internal" href="Memory-Management.html">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="Meta.html">Meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="Modules.html">Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="Objects.html">Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="PackageManager.html">Package manager</a></li>
<li class="toctree-l2"><a class="reference internal" href="Posets.html">Posets</a></li>
<li class="toctree-l2"><a class="reference internal" href="Programs.html">Exemplary programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="Reduction.html">Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="Reduction-Example.html">Reduction example</a></li>
<li class="toctree-l2"><a class="reference internal" href="Resource-Management.html">Resource management</a></li>
<li class="toctree-l2"><a class="reference internal" href="Sets.html">Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="State.html">Stateful programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Syntax.html">Syntax</a></li>
<li class="toctree-l2"><a class="reference internal" href="TermRewriting.html">Term rewriting</a></li>
<li class="toctree-l2"><a class="reference internal" href="Time.html">Time API</a></li>
<li class="toctree-l2"><a class="reference internal" href="Tools.html">Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="Types.html">Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="Units.html">Units</a></li>
<li class="toctree-l2"><a class="reference internal" href="Verification.html">Verification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../zzreferences.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Stroscot</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Commentary</a></li>
      <li class="breadcrumb-item active">Compiler design</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/Mathnerd314/stroscot/edit/master/docs/Commentary/Compiler.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="compiler-design">
<h1>Compiler design<a class="headerlink" href="#compiler-design" title="Permalink to this heading"></a></h1>
<p>Dynamically typed languages are tricky to compile efficiently. There’s been lots of research on efficient JIT compilers for dynamic languages - SELF, Javascript, PyPy, Java - but these are quite involved, and still slower than C. Ahead-of-time compilation for dynamic languages is possible as well but not explored, and needs profile data to work properly. Currently Stroscot is aiming for AOT and JIT compilers with profiling.</p>
<section id="structure">
<h2>Structure<a class="headerlink" href="#structure" title="Permalink to this heading"></a></h2>
<p>The interpreter:</p>
<ul class="simple">
<li><p>A parser - this is written using nondeterminism. Likely the full syntax will not be fast enough for practical purposes until late in the project, so for now the parser uses a deterministic Lisp-like syntax. The parser records file and line number information, token start/end, call stack, and other debugging information.</p></li>
<li><p>Fexpr interpreter loop - this starts with the AST and produces a value. The main part is dispatching pattern matches. Uses the eval-apply model, similar to <span id="id1">[<a class="reference internal" href="../zzreferences.html#id42" title="Paul Downen, Zachary Sullivan, Zena M. Ariola, and Simon Peyton Jones. Making a faster curry with extensional types. In Proceedings of the 12th ACM SIGPLAN International Symposium on Haskell, Haskell 2019, 58–70. Berlin, Germany, August 2019. Association for Computing Machinery. URL: https://doi.org/10.1145/3331545.3342594 (visited on 2020-06-14), doi:10.1145/3331545.3342594.">DSAPJ19</a>]</span>.</p></li>
<li><p>Logic prover - a CDCL satisfiability search algorithm, handles nondeterminism such as dispatch, checking if a value is a member of a type (checking functions etc. is nondeterministic), explicit lub, etc.</p></li>
<li><p>Memory management</p></li>
<li><p>A dynamic assembler / JIT code generator</p></li>
</ul>
<p>The specializer:</p>
<ul class="simple">
<li><p>Supercompiler / partial evaluator: computes possible states of the program</p></li>
<li><p>Figures out how to represent space of program states efficiently (to avoid state explosion)</p></li>
<li><p>Code generation: converts state transition relation to assembly instructions of the code target</p></li>
<li><p>Static verification: Warns if error states are reachable, checks other specified properties to generate warnings</p></li>
</ul>
<p>The JIT:</p>
<ul class="simple">
<li><p>Interleaves specialized generated code and the interpreter</p></li>
<li><p>Counts state repetitions and specializes hot loops</p></li>
</ul>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading"></a></h2>
<p>For a lot of compilation decisions we have several choices and want to pick the best one based on some measure of “performance”. E.g. overloading/dispatch can be implemented in a variety of ways, specialized for call site - generally it boils down to branching on some condition (binary search), or doing a table lookup. The fastest solution depends on which clauses are relatively hot, but in general we don’t know which clauses are hot.</p>
<p>Profile-guided optimization is an effective solution to this lack of information: we instrument a binary with counters for the various questions we might ask, and generate a profile with the answers. We might need to run a binary several different times to get good coverage so we also need a way to combine profiles together, i.e. profiles form a commutative monoid. Profiles themselves introduce a “Heisenbug” problem: we cannot measure the detailed performance of an unprofiled program, and turning profiling off may change the performance significantly. The solution is to build with profiling support for almost all of the compilation pipeline. We should only omit profiling instructions for non-profiled builds at the assembly level. And if we use hardware-assisted sampling profiling then we don’t even need profiling instructions, in many cases, so profiling can simply be always enabled.</p>
<p>When trying to do a quick compile-run cycle, we still want to streamline hot paths so that the binary is not unusably slow, but cold spots can use a straightforward boilerplate translation that doesn’t require much CPU. More generally, there are various optimization criteria to minimize during compilation. Generally anything that can be measured is fair game:</p>
<ul class="simple">
<li><p>Compile/runtime total elapsed time, power usage, memory usage</p></li>
<li><p>Runtime executable size</p></li>
<li><p>Throughput at runtime</p></li>
<li><p>Request-response latency at runtime  - can be conditional, e.g. latency for a stop loss order on trading platform but latency for other types is not as important</p></li>
</ul>
<p>These are generally not hard numbers but probabilistic variables, because computer performance depends on many uncontrollable factors hence is best treated is nondeterministic. A simple mean or median estimator is generally sufficient, but doing statistical hypothesis testing is more interesting. Worst case execution time is of interest in real-time systems. Execution time may be modeled by a Gumbel distribution (<a class="reference external" href="http://www.lasid.ufba.br/publicacoes/artigos/Estimating+Execution+Time+Probability+Distributions+in+Component-based+Real-Time+Systems.pdf">ref</a>) or odd log-logistic generalized gamma (OLL-GG) or exponentiated Weibull (<a class="reference external" href="https://arxiv.org/pdf/2006.09864.pdf">ref</a>), although these experiments should probably be redone as we are measuring different programs. The testbench is <a class="reference external" href="https://mjsaldanha.com/sci-projects/3-prob-exec-times-1/">here</a> and <a class="reference external" href="https://github.com/matheushjs/ElfProbTET">here</a> and could be extended with <a class="reference external" href="https://www.rdocumentation.org/packages/evd/versions/2.3-6/topics/gev">gev</a>.</p>
<p>Obviously these have tradeoffs, so we need an overall objective function. For a focused objective like running static verification, all we want to see the error messages so total elapsed compile time is the only measurement. For production binaries, there will likely be a complex function for various runtime measurements based on actual costs and requirements, but compile time will be excluded from the criteria or minimally penalized. For debugging, running in a REPL, an edit-compile-test cycle, etc., both compile and runtime factors are important so the objective function becomes even more complex. gcc, clang, etc. have various optimization profiles like O0, O1, O2, O3, Og, On, Os, Oz, etc., which we can include presets for, but it’s not clear these are sufficient.</p>
<p>We use branch-and-bound to explore the possibilities. With good heuristics even the truncated search algorithm should give good results. The goal is to quickly find bottleneck code regions that have significant effects on performance and compute good optimizations quickly. Then another profiling build to test that the proposed changes were correct.</p>
<p>There is also ISA selection and tuning for specific machines and CPUs. ISA, timing, cache, and memory characteristics are available for specific CPUs, but compiling specifically for a single CPU is not done often. Usually for x86 the code is compiled to work on SSE2 (since it’s part of AMD64) and tuned for a “generic” CPU. The definition of this is vague - for <a class="reference external" href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81616">GCC</a> and <a class="reference external" href="https://reviews.llvm.org/D118534">LLVM</a> it seems to be Haswell with a few slow cases on other architectures patched. It is supposed to be “an average of popular targets”, so using a weighted sum of processors according to sales is most appropriate, but per-CPU-model sales data doesn’t seem to be available easily. <a class="reference external" href="https://www.cpubenchmark.net/share30.html">PassMark</a>, <a class="reference external" href="https://benchmarks.ul.com/compare/best-cpus?amount=0&amp;sortBy=POPULARITY&amp;reverseOrder=true&amp;types=MOBILE,DESKTOP&amp;minRating=0">3DMark</a>, and <a class="reference external" href="https://cpu.userbenchmark.com/">UserBenchmark</a> publish their list of most benchmarked processors, which is probably good enough.</p>
<p>Formally proving optimizations correct is a good idea, as they are often buggy.</p>
<p>Need optimizations for:
* avoiding intermediate structures and dead or redundantly duplicated computation
* storing arrays on the heap in the most efficient of a few straightforward ways
* boiling away higher-order functions into tedious boilerplate (inlining)
* custom optimizations</p>
<p>A <a class="reference external" href="http://venge.net/graydon/talks/CompilerTalk-2019.pdf">talk</a> by Graydon Hoare on compilers mentions the paper <span id="id2">[<a class="reference internal" href="../zzreferences.html#id5" title="Frances E Allen and John Cocke. A catalogue of optimizing transformations. IBM Research Center, pages 30, 1971. URL: https://www.clear.rice.edu/comp512/Lectures/Papers/1971-allen-catalog.pdf.">AC71</a>]</span>. He says we need 8 optimization passes to get 80% of GCC/LLVM performance:</p>
<ul class="simple">
<li><p>Common subexpression elimination - This starts from atomic expressions / closed connected components and then works up to identify opportunities for sharing. Because of unsharing fans it can share parents regardless of their other children; this doesn’t increase the graph size and may decrease code size/computation. Since the graph may be cyclic we need a partitioning algorithm like in <span id="id3">[<a class="reference internal" href="../zzreferences.html#id102" title="Laurent Mauborgne. Representation of Sets of Trees for Abstract Interpretation. PhD thesis, Ecole Polytechnique, November 1999. URL: http://software.imdea.org/~mauborgn/publi/t.pdf.">Mau99</a>]</span>.</p></li>
<li><p>Inlining - Going through <span id="id4">[<a class="reference internal" href="../zzreferences.html#id114" title="Simon Peyton Jones and Simon Marlow. Secrets of the Glasgow Haskell Compiler inliner. Journal of Functional Programming, 12(4):393–434, July 2002. URL: https://www.microsoft.com/en-us/research/wp-content/uploads/2002/07/inline.pdf (visited on 2020-07-01), doi:10.1017/S0956796802004331.">PJM02</a>]</span>, this is basically just reducing reducible expressions. The reason it’s hard is doing reduction across statement boundaries, inside recursive functions, etc., in combination with a strictness/termination analysis.</p></li>
<li><p>Constant Folding - more reduction of reducible expressions</p></li>
<li><p>Loop unrolling/vectorization - mutable variables can be normalized to SSA, so really this is about unrolling recursive functions. It’s a code size vs. code quality optimization, heavily dependent on scheduling.</p></li>
<li><p>Loop-invariant code motion (hoisting) - this is just reducing in a certain order, i.e. scheduling again.</p></li>
<li><p>Dead code elimination - Unused pure expressions aren’t connected to the main graph and so are trivially eliminated. But we also want to eliminate conditional branches that will never be taken; this requires a reachability analysis.</p></li>
<li><p>Peephole - this is instruction selection for the backend. We’re going the Unison integrated constraint-satisfaction approach.</p></li>
</ul>
</section>
<section id="cross-compilation">
<h2>Cross compilation<a class="headerlink" href="#cross-compilation" title="Permalink to this heading"></a></h2>
<p>In cross compilation we have not one system, but two systems. To use the newer <a class="reference external" href="https://clang.llvm.org/docs/CrossCompilation.html">Clang</a> terminology, there is the <strong>host</strong> system where the program is being built, and the <strong>target</strong> system where the program will run. When the host and target systems are the same, it’s a native build; otherwise it’s a cross build.</p>
<p>The older <a class="reference external" href="https://gcc.gnu.org/onlinedocs/gccint/Configure-Terms.html">GNU terminology</a> uses a triple, build/host/target; but the “target” there is really a configuration option, namely the supported target of the compiler that will run on the host. Only gcc need to specify the supported target, as Clang is generally built to support all supported targets. Since remembering whether the build system builds the host or vice-versa is tricky, overall the Clang terminology host/target/supported targets seems clearer than build/host/target.</p>
<p>the toolchain (gcc, llvm, as, ld, ar, strip, etc.) should be target-dependent, information stored in a YAML file or similar
the package set is also target-dependent. some packages that are pure data are target-independent</p>
</section>
<section id="bootstrapping">
<h2>Bootstrapping<a class="headerlink" href="#bootstrapping" title="Permalink to this heading"></a></h2>
<p>Bootstrapping is a 2-stage process. We start with the source <code class="docutils literal notranslate"><span class="pre">s</span></code> and bootstrap compiler <code class="docutils literal notranslate"><span class="pre">cB</span></code>, an old compiler using the old ABI. Then we build:</p>
<ul class="simple">
<li><p>stage 1: New compiler on old ABI <code class="docutils literal notranslate"><span class="pre">c1=run(cB,s)</span></code></p></li>
<li><p>stage 2: New compiler on new ABI <code class="docutils literal notranslate"><span class="pre">c2=run(c1,s)</span></code></p></li>
</ul>
<p>We can test stage 2 (the “compiler bootstrap test”) by building a new compiler <code class="docutils literal notranslate"><span class="pre">c3=run(c2,s)</span></code>. If the build is deterministic, <code class="docutils literal notranslate"><span class="pre">c3</span></code> should be bit-identical to <code class="docutils literal notranslate"><span class="pre">c2</span></code>. We can also run the test suite to compare outputs of <code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code>. But we cannot compare performance of <code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code>, because they use different ABIs, and also <code class="docutils literal notranslate"><span class="pre">cB</span></code> may be buggy so <code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code> may not behave exactly the same. We can also use diverse double-compiling <span id="id5">[<a class="reference internal" href="../zzreferences.html#id156" title="David A. Wheeler. Fully countering trusting trust through diverse double-compiling. arXiv:1004.5534 [cs], April 2010. Comment: PhD dissertation. Accepted by George Mason University, Fairfax, Virginia, USA's Volgenau School of Information Technology and Engineering in 2009. 199 single-side printed pages. URL: http://arxiv.org/abs/1004.5534 (visited on 2020-09-13), arXiv:1004.5534.">Whe10</a>]</span>, compiling with multiple bootstrap compilers <code class="docutils literal notranslate"><span class="pre">cB</span></code>, to increase our confidence in the correctness of the stage 2 compiler.</p>
<p>For cross-compiling, we build stage 1 for the host and stage 2 for the target.</p>
<p>The compiler depends on libraries. The bootstrap compiler does not provide updated libraries, so we must build the libraries for the Stage 1 compiler.</p>
<p>build stage 2 compiler with the stage 1 compiler using the stage 1 package database ship with the stage 2 compiler). As such, the compiler is built with the identical libraries that it ships with. When running / interpreting byte code, we need to dynamically link packages and this way we can guarantee that the packages we link are identical to the ones the compiler was built with. This it is also the reason why we don’t have GHCi or Template Haskell support in the stage 1 compiler.</p>
</section>
<section id="complex-bootstrap">
<h2>Complex bootstrap<a class="headerlink" href="#complex-bootstrap" title="Permalink to this heading"></a></h2>
<p>Actually bootstrapping is more complex. The compiler is really two components, an interpreter and a specializer. The input program can take arguments. The interpreter can take arguments (dialects, libraries). The specializer can take arguments (bytecode, optimization instructions, plugins). The output program can take arguments (compiled objects, runtime components such as libc or a garbage collector). All of these arguments and options aren’t handled easily.</p>
<p>We can think about this using the Futamura projections. We assume a primitive</p>
<div class="math notranslate nohighlight">
\[\newcommand{\run}[1]{⟦#1⟧}
\run{\cdot} : \text{program} \to \text{data} \to \text{result}\]</div>
<p>that can run programs written in any language, given input data, and produce an output result. We use a denotational notion of result where erroring / not halting is itself a result. Two programs are equal if <span class="math notranslate nohighlight">\(\run{p} d = \run{q} d\)</span> for all <span class="math notranslate nohighlight">\(d\)</span>; equivalence of results depends on context and ranges from literal comparison to more advanced semantics.</p>
<p>Definitions:</p>
<ul class="simple">
<li><p>An interpreter <span class="math notranslate nohighlight">\(i\)</span> has <span class="math notranslate nohighlight">\(\run{i} (p,d) = \run{p} d\)</span>.</p></li>
<li><p>A compiler <span class="math notranslate nohighlight">\(c\)</span> has <span class="math notranslate nohighlight">\(\run{\run{c} p} d = \run{p} d\)</span>.</p></li>
<li><p>A specializer <span class="math notranslate nohighlight">\(s\)</span> has <span class="math notranslate nohighlight">\(\run{\run{s} (p,x)} y = \run{p} (x,y)\)</span>.</p></li>
<li><p>A residual program is a program <span class="math notranslate nohighlight">\(p_x\)</span> such that <span class="math notranslate nohighlight">\(\run{p_x} y = \run{p} (x,y)\)</span>.</p></li>
<li><p>A generating extension <span class="math notranslate nohighlight">\(g_p\)</span> of a program <span class="math notranslate nohighlight">\(p\)</span> has <span class="math notranslate nohighlight">\(\run{g_p} x = p_x\)</span>, i.e. it produces residual programs of <span class="math notranslate nohighlight">\(p\)</span>.</p></li>
<li><p>A compiler generator <span class="math notranslate nohighlight">\(c\)</span> has <span class="math notranslate nohighlight">\(\run{\run{\run{c} p} x} y = \run{p} (x,y)\)</span>.</p></li>
<li><p>A runner <span class="math notranslate nohighlight">\(r\)</span> has <span class="math notranslate nohighlight">\(\run{\run{r} c} (p,x) = \run{\run{c} p} x\)</span></p></li>
</ul>
<p>1 specializer generates residual programs, <span class="math notranslate nohighlight">\(p_x = \run{s} (p,x)\)</span>.
2 specializers produces generating extensions, <span class="math notranslate nohighlight">\(g_p = \run{s_1} (s_2,p)\)</span>.
3 specializers produces a compiler generator, <span class="math notranslate nohighlight">\(c_{123} = \run{s_1} (s_2,s_3)\)</span>.
Similarly we can use a compiler generator: <span class="math notranslate nohighlight">\(\run{\run{c} p} x\)</span> for residual programs, <span class="math notranslate nohighlight">\(\run{c} p\)</span> for generating extensions, <span class="math notranslate nohighlight">\(c_{123} = \run{\run{\run{c} s_1} s_2} s_3\)</span> to obtain the same compiler generator as formed by applying the specializers.</p>
<p>A generating extension of an interpreter is a compiler; similarly passing an interpreter <span class="math notranslate nohighlight">\(i\)</span> to a compiler generator <span class="math notranslate nohighlight">\(c\)</span> produces a compiler <span class="math notranslate nohighlight">\(\run{c} i\)</span>. A generating extension of a string matcher is a matcher generator and a generating extension of a universal parser is a parser generator. Hence we should call a compiler generator a “generating extension generator”.</p>
<p>A generating extension of a specializer is a compiler generator. <span class="math notranslate nohighlight">\(\run{\run{\run{g_s}p}x}y = \run{\run{s}(p,x)} y = \run{p}(x,y)\)</span></p>
<p>In particular, assuming <span class="math notranslate nohighlight">\(c\)</span> is a compiler generator, <span class="math notranslate nohighlight">\(c' = \run{c} s\)</span> is a compiler generator iff <span class="math notranslate nohighlight">\(s\)</span> is a specializer. Proof: <span class="math notranslate nohighlight">\(run (\run{s} (p,x)) y = \run{\run{\run{\run{c} s} p} x} y = \run{\run{\run{c}' p} x} y = \run{p} (x,y)\)</span> to show <span class="math notranslate nohighlight">\(s\)</span> is a specializer, <span class="math notranslate nohighlight">\(\run{\run{\run{c'} p} x} y = run (\run{s} (p,x)) y = \run{p} (x,y)\)</span> to show <span class="math notranslate nohighlight">\(c'\)</span> is a compiler generator.</p>
<p>If <span class="math notranslate nohighlight">\(\run{c} s = c\)</span>, <span class="math notranslate nohighlight">\(c\)</span> is termed a self-generating compiler generator. <span class="math notranslate nohighlight">\(\run{s} (s,s) = \run{\run{\run{c} s} s} s = c\)</span>. Furthermore <span class="math notranslate nohighlight">\(s\)</span> is a specializer. OTOH if <span class="math notranslate nohighlight">\(s\)</span> is a specializer then <span class="math notranslate nohighlight">\(\run{s} (s,s)\)</span> is a compiler generator self-generating with <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>With a runner <span class="math notranslate nohighlight">\(r\)</span> we can turn a compiler generator <span class="math notranslate nohighlight">\(c\)</span> into a specializer <span class="math notranslate nohighlight">\(\run{r}c\)</span>. Self-applying this specializer gives a compiler generator with equivalent output to <span class="math notranslate nohighlight">\(c\)</span> after two arguments have been applied:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\run{\run{\run{\run{r}c}(\run{r}c,\run{r}c)}p}x &amp; = \run{\run{\run{\run{c}(\run{r}c)}(\run{r}c)}p}x \\
&amp; = \run{\run{\run{r}c}(\run{r}c,p)}x \\
&amp; = \run{\run{\run{c}\run{r}c}p}x \\
&amp; = \run{\run{r}c}(p,x) \\
&amp; = \run{\run{c}p}x\end{split}\]</div>
</section>
<section id="compile-time-code-execution">
<h2>Compile-time code execution<a class="headerlink" href="#compile-time-code-execution" title="Permalink to this heading"></a></h2>
<p>We want to execute code that runs at compile time, e.g. reading a blob of data to be included as a literal. Clearly this code executes on the host, with the same filesystem as the rest of the source code.</p>
<p>We also want to read configuration, e.g. the target platform properties (word size, endianness, etc.).</p>
<p>Also we want to do computations with no runtime inputs, like 1+2.</p>
</section>
<section id="compiler-ways">
<h2>Compiler ways<a class="headerlink" href="#compiler-ways" title="Permalink to this heading"></a></h2>
<p>GHC calls some options “compiler ways”. They can be combined (e.g. threaded + debugging). The main issue is they affect the ABI, so ways need be stored into ABI hashes in installed libraries to avoid mismatching incompatible code objects.</p>
<ul class="simple">
<li><p>use the multi-threaded runtime system or not</p></li>
<li><p>support profiling or not</p></li>
<li><p>use additional debug assertions or not</p></li>
<li><p>use different heap object representation (e.g. <code class="docutils literal notranslate"><span class="pre">tables_next_to_code</span></code>)</p></li>
<li><p>support dynamic linking or not</p></li>
</ul>
<p>Depending on the selected way, the compiler produces and links appropriate objects together. These objects are identified by a suffix: e.g. <code class="docutils literal notranslate"><span class="pre">*.p_o</span></code> for an object built with profiling enabled; <code class="docutils literal notranslate"><span class="pre">*.thr_debug_p.a</span></code> for an archive built with multi-threading, debugging, and profiling enabled. See the gory details on the <a class="reference external" href="https://gitlab.haskell.org/ghc/ghc/wikis/commentary/rts/compiler-ways">wiki</a>.</p>
<p>Installed packages usually don’t provide objects for all the possible ways as it would make compilation times and disk space explode for features rarely used. The compiler itself and its boot libraries must be built for the target way.</p>
</section>
<section id="compiler-memory-management">
<h2>Compiler memory management<a class="headerlink" href="#compiler-memory-management" title="Permalink to this heading"></a></h2>
<p>For the compiler itself, a trivial bump or arena allocator is sufficient for most purposes, as it is invoked on a single file and lasts a few seconds. With multiple files and large projects the issue is more complicated, as some amount of information must be shared between files. Optimization passes are also quite traversal-intensive and it may be more efficient to do in-place updates with a tracing GC rather than duplicating the whole AST and de-allocating the old one. Two other sources of high memory usage are macros and generics, particularly in combination with optimizations that increase code size such as inlining.</p>
<p>Overall I don’t see much of an opportunity, SSD and network speeds are sufficient to make virtual memory and compile farms usable, so the maximum memory is some large number of petabytes. The real issue is not total usage but locality, because compilers need to look up information about random methods, blocks, types etc. very often. But good caching/prefetching heuristics should not be too hard to develop. In practice the programs people compile are relatively small, and the bottleneck is the CPU because optimizations are similar to brute-force searching through the list of possible programs. Parallelization is still useful. Particularly when AMD has started selling 64-core desktop processors, it’s clear that optimizing for some level of that, maybe 16 or 32 cores, is worthwhile.</p>
</section>
<section id="dynamic-execution">
<h2>Dynamic execution<a class="headerlink" href="#dynamic-execution" title="Permalink to this heading"></a></h2>
<p>benefit: erases distinction between compile time and execution time. Hence optimizes for compile+execute time.</p>
<p>loading code at runtime
- typecheck, JIT compile, return function pointer
the function pointer doesn’t have to be machine code, it can be bytecode, so the function runs through an interpreter
Compiler from IR to bytecode
Saving snapshots of the VM state (images)
Tracing JIT compiler
Use libgccjit for code generation?
Optimized assembly interpreter a la LuaJIT and JavaScriptCore</p>
<p>everyone had two entry points.
if you came from the
interpreter you had to call the
interpreter entry point and you
came from JITed code you entered the
JITed code favorite entry point</p>
<p>the goal here was JITed calling JITed had minimal overhead
so an x86 call instruction with the JITed entry point’s address</p>
<p>so if a JITed calls interpreted there’s a
JITed entry point that shuffles the
arguments and jumps to the interpreter</p>
<p>and if the interpreter makes
a call, it’s a slow procedure that looks
up the interpreter endpoint or else
jumps to a trampoline that jumped to the JITed code</p>
<p>then there’s deoptimization
it’s tricky to stop running processors
from running code
if you try to
edit the method call buffers processors have
them cached
you
can’t actually stop it
so first you change the vtable to the interpreter
then you change the head of the method to jump to the interpreter</p>
<p>there’s also speculative optimization and escape analysis</p>
<p>Creating the compiled file consumes extra CPU time and storage vs the interpreter. The compiled version runs more efficiently. Some errors are only detected during compilation.</p>
<p>Julia - faster than Python, but JIT uses many slow trampolines</p>
<p>Javascript - V8 is a fast modern JIT</p>
<p>In a sea of nodes program dependence graph (PDG), nodes correspond to arithmetic/logic operations but also to control operations such as conditional jumps and loops. edges correspond to dependencies among operations.</p>
<p>graphs corresponding to relatively small programs turn quickly into a tangle that is quite difficult to grasp. PDGs cannot be read directly without assistance; this affects debugging speed. PDGs remain an obscure topic in advanced compiler courses.</p>
<p>In a CFG, nodes correspond to basic blocks, ordered sequences of operations that are always executed together. every operation belongs to a single basic block. edges correspond to control jumps across basic blocks. A CFG yields a structured, sequential view of the program that is easier to understand and debug, and is familiar for many systems engineers.</p>
<p>To turn a PDG into a CFG, compute an assignment of operations to basic blocks (global schedule) and an ordering of operations within each basic block (local schedule).</p>
<p>clustering basic blocks into (nested) loops, if-then-else structures, etc.
coloring the basic blocks that are executed most often</p>
<p>the value representation is optimized for the platform, and redundant checks are optimized out</p>
<p>The Implementation of Functional Programming Languages
Implementing functional languages: a tutorial
Implementing Lazy Functional Languages on Stock Hardware: The Spineless Tagless G-Machine
How to make a fast curry: push/enter vs eval/apply
GHC also does strictness analysis and optimistic evaluation.</p>
<p>a program is a dependency graph which is evaluated through a series of local reductions
the graph itself can be represented as code. In particular, we can represent a node as a function that when invoked, returns the desired value. The first time it is invoked, it asks the subnodes for their values and then operates on them, and then it overwrites itself with a new instruction that just says “return the result.”</p>
<p>JIT cache: need &gt;90% hit rate to pay off vs just doing normal JIT path of interpeting bytecode and optimizing. need profile data, otherwise optimizations will be different. The profile is a few megabytes but the compiled code may be 100s of megabytes since it has a lot of metadata.</p>
<p>rare methods don’t show up in the profile, but may still need to be fast.</p>
<p>the c2 strategy is a counter with an absolute threshold. so eventually, as long it is not dead code, it will be JITed. it guarantees enough samples so that you have a good profile. trying to do an exponential decay so only hot methods</p>
<p>L1 cache is cheaper than memory, so clean up bytecode as soon as it is generated</p>
</section>
<section id="debugger">
<h2>Debugger<a class="headerlink" href="#debugger" title="Permalink to this heading"></a></h2>
<p>The debugger’s view of the program’s state is as a large expression or term. This state evolves in steps, where each step applies a rule to a redex or calls into the OS to perform a primitive operation.</p>
<p>We allow reversible/omniscient debugging, meaning that one can step both forward from a state (the usual) and backward from a state (query on where a value came from etc.).</p>
<p>One debugging technique useful in combination with reversible debugging is to use a step counter that starts at 0 at the beginning of the program and increments every time a reduction step is performed. The exact step that triggers a behavior can be determined by binary search. When we are debugging a phase of the compiler, we can use “fuel” for each phase - this specifies how many transformations can be performed during the phase of interest before moving on to the next phase.</p>
<p>Let’s assume we have symbols, then there are lots of operations available from a debugger:</p>
<ul class="simple">
<li><p>breakpoints: set/clear/list, essentially a breakpoint is a true/false query on a state. can be syscall, call, return, signal injection, etc.</p></li>
<li><p>queries: print backtrace / call stack, evaluate pure expression in context of state, dump state, dump memory, disassemble memory</p></li>
<li><p>stepping: single step, step out, continue thread / all threads until breakpoint, run ignoring breakpoints until stopped with interactive commnad</p></li>
<li><p>patching: replace definition, jump to address, return early from function, evaluate code in current context (e.g. set memory to value). The debugger can only run forward from the patched state because it has no history.</p></li>
<li><p>IPC: send signal, modify files</p></li>
</ul>
</section>
<section id="profiler">
<h2>Profiler<a class="headerlink" href="#profiler" title="Permalink to this heading"></a></h2>
<p>Measure</p>
<ul class="simple">
<li><p>time and memory usage.</p></li>
<li><p>throughput (calls/second)</p></li>
<li><p>A/B testing of multiple implementations</p></li>
</ul>
<p>for functions, expressions, programs, etc.</p>
<p>Use statistical sampling and hardware performance counters to avoid overhead. Checkout criterion, papers on LLVM hardware sampling.</p>
</section>
<section id="ir-dump">
<h2>IR dump<a class="headerlink" href="#ir-dump" title="Permalink to this heading"></a></h2>
<p>A good compiler can get 80% of the code to a fast-enough state. But nontrivial hot spots will still need hand-optimizing and tuning. At first it can be good to tweak the original code to get it to generate IR differently, but eventually the algorithm is set and the micro-optimizations matter, so you want to bake in the low-level implementation.</p>
<p>With a wide-spectrum language the IR is the same language as the original, just using lower-level operations. So you can compile source-to-source or directly write in the IR. For example SQL is declarative but being able to write a functional program using the underlying sort, filter, merge anti-join, etc. operations would be useful.</p>
<p>There are many levels to the pipeline, and each one is useful. For an interpreted program the only step that can’t be represented is actually running the program, e.g. converting <code class="docutils literal notranslate"><span class="pre">print</span> <span class="pre">&quot;Hi&quot;</span> <span class="pre">exit</span></code> to output.</p>
</section>
<section id="evolution">
<h2>Evolution<a class="headerlink" href="#evolution" title="Permalink to this heading"></a></h2>
<p>Try as we might, no language design is perfect. Languages inevitably change or extend their semantics over time, resulting in ecosystem fragmentation where programs end up being written in different “dialects” of the language. The evolution process aims to minimize the disruption to existing code by evolving the language in a controlled manner, in particular in discrete units of “features”. The process guarantees a “compatibility promise” that the source code of existing programs written for an old language version can be automatically migrated to a new language version. Because the language evolves towards a standardized set of features, the langauge should avoid fragmentation.</p>
<p>A feature is a distinct chunk of compiler functionality, such as a change to the semantics of the language, a compiler plugin, or an external tool integration. A feature can be alpha, beta, or stable.</p>
<p>Alpha features are experimental features with little formal testing, released to get feedback. They may be documented informally or on an “alpha features” page. Alpha features have no compatibility guarantee and may be changed freely. Alpha features are kept behind feature toggles, which allow conditioning code on a feature. This allows testing features and integrating them on the main branch while isolating them from other tests and software releases. Alpha features will be removed from the compiler if they have not made any progress towards beta over the course of a year.</p>
<p>Beta features are implemented features that may change further. They must have a reasonable test suite and be documented in the commentary / reference in full detail, describing edge cases. They must also have a how-to if the feature’s usage is not obvious. Fundamental new features may affect the tutorial as well, although generally new features are too advanced. Beta features cannot be toggled off but have automigration functionality for old code that is enabled by specifying the language version. Automigration is distinct from a toggle because it is a source-to-source rewrite of the code. Beta features may still have significant bugs, such as the inability to migrate old code correctly, but these bugs should generate readable error messages mentioning the feature name rather than crashing the compiler or silently failing.</p>
<p>Stable features are frozen features - further changes will be done as new features. They are considered to have reached a level of stability sufficient for long-term use. There is no visible difference in the implementation code between beta features and stable features and the distinction is mainly for marketing purposes.</p>
<p>The list of features is centralized in the code to <a class="reference external" href="https://github.com/Mathnerd314/stroscot/blob/master/src/features.txt">this specific file</a>, to make finding them easier and to standardize handling. The scope of a feature may be identified by grep’ing the code for its identifier.</p>
<p>Moving a feature from alpha to beta should have a PR with documentation links and test case links. The PR should:</p>
<ul class="simple">
<li><p>change the feature list to set the feature’s status to beta released on the current date. This enables old code warnings, automigration, and compiler bootstrap workarounds.</p></li>
<li><p>implement automigration code if not already present</p></li>
<li><p>remove all uses of the feature toggle in the code by modifying to the case where the feature is present (avoiding toggle debt).</p></li>
</ul>
</section>
<section id="incremental-compilation">
<h2>Incremental compilation<a class="headerlink" href="#incremental-compilation" title="Permalink to this heading"></a></h2>
<p>Incremental compilation reduces rebuild time. With a good incremental build system, optimizations can be rechecked rather than rediscovered, so that the program</p>
</section>
<section id="hot-reloading">
<h2>Hot reloading<a class="headerlink" href="#hot-reloading" title="Permalink to this heading"></a></h2>
<p>Hot reloading or “edit and continue” is the ability to change code and resources of a live application without restarting it. It speeds up the edit-test cycle because you can stay on a certain state of the program without needing to spend time to recreate it. It can be useful for games, UI design, or data analysis.</p>
<p>Edit and continue is really a debugger feature, because usually you edit the code while paused on a breakpoint, rather than while the program is actually running. Integrating with omniscient debugging is probably best, so you can manually select an old state and then evolve it using the new transition rules. For example when editing the jump height for a jump’n’run game, you probably don’t want to continue from the game’s start, or even the first jump input, but rather to just before the one tricky jump in the middle of the level. There is no indication of this magic location in the code or program state besides the player’s x-coordinate being a certain value.</p>
<p>Erlang has hot code swapping, Smalltalks and Lisps have “live programming.” Assisting System Evolution: A Smalltalk Retrospective is a recommended read.</p>
<p>The most basic implementation is to patch functions calls so they call a new function instead of an old one. A JIT already does this kind of patching when switching from interpreted to optimized code, so can do it easily. With ahead-of-time you can compile a new DLL, duplicate it to avoid locking, load it, and swap out the function pointer, but it requires specially marking the hot-reloadable methods.</p>
<p>Functions generally assume a fixed set of types and a fixed memory representation for all types. Changing the types or their representation can break program invariants and cause memory corruption. But it is possible - there are some projects for live kernel patching that can patch in-memory data structures to the correct format.</p>
<p>State is also an issue because the memory manager must be aware of the local state of a piece that reloaded and avoid leaking memory. In the case of handles such as an OpenGL context the desirable behavior is to transfer them over to the new code, but if the initialization code is changed then the handle should instead be closed and re-initialized. So we see some sort of incremental program execution going on.</p>
</section>
<section id="literal-vm">
<h2>Literal VM<a class="headerlink" href="#literal-vm" title="Permalink to this heading"></a></h2>
<p>A virtual machine (VM) is a big while-switch-loop that reading opcodes and executes them. VMs are slow, because they’re interpreted - executing an opcode takes many machine code instructions. A literal VM is a VM where each opcode represents one machine code instruction. Because of this 1-1 mapping, functions can be easily JITed to machine code with little overhead. You might alternately call it an “assembler” because it takes an “assembly” language and generates machine code, but LLVM started using the term VM a long time ago. Also, unlike most traditional assemblers, a literal VM allows dynamic loading of new code, interpreting code and running it at a REPL, and introspection such as listing constants and functions from imports. It makes learning assembly pretty fun.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Code-Generation.html" class="btn btn-neutral float-left" title="Code generation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="CompilerOutput.html" class="btn btn-neutral float-right" title="Compiler output" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2022 Mathnerd314.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>