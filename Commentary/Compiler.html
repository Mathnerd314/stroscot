<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Compiler design &mdash; Stroscot  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/hexagon_favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=7f41d439"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Compiler library" href="Compiler-Library.html" />
    <link rel="prev" title="Code generation" href="Code-Generation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/hexagon_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/index.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../HowTo/index.html">How to</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Reference/index.html">Language Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Commentary</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Aspects.html">Aspects</a></li>
<li class="toctree-l2"><a class="reference internal" href="Assembly.html">Assembly</a></li>
<li class="toctree-l2"><a class="reference internal" href="BuildSystem.html">Build system</a></li>
<li class="toctree-l2"><a class="reference internal" href="Checklist.html">Checklist</a></li>
<li class="toctree-l2"><a class="reference internal" href="Code-Generation.html">Code generation</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Compiler design</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#definitions">Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#design">Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization">Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization-variables">Optimization variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#build-model">Build model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#complex-bootstrap">Complex bootstrap</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compile-time-code-execution">Compile-time code execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiler-ways">Compiler ways</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiler-memory-management">Compiler memory management</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-execution">Dynamic execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ir-dump">IR dump</a></li>
<li class="toctree-l3"><a class="reference internal" href="#incremental-compilation">Incremental compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hot-reloading">Hot reloading</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Compiler-Library.html">Compiler library</a></li>
<li class="toctree-l2"><a class="reference internal" href="CompilerOutput.html">Compiler output</a></li>
<li class="toctree-l2"><a class="reference internal" href="Concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoreSyntax.html">Core syntax</a></li>
<li class="toctree-l2"><a class="reference internal" href="Dispatch.html">Dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="Errors.html">Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="Evaluation-Strategy.html">Evaluation strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="Exceptions.html">Exceptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="Fastest.html">As fast as C</a></li>
<li class="toctree-l2"><a class="reference internal" href="FunctionalLogic.html">Functional logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="IR.html">Intermediate representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="Learning.html">Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="Logic.html">Logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="LogicProgramming.html">Logic programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Macros.html">Macros</a></li>
<li class="toctree-l2"><a class="reference internal" href="Memory-Management.html">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="Meta.html">Meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="Modules.html">Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="Objects.html">Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="OpPrims.html">Operational primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="PackageManager.html">Package manager</a></li>
<li class="toctree-l2"><a class="reference internal" href="Parsing.html">Parsing</a></li>
<li class="toctree-l2"><a class="reference internal" href="Posets.html">Posets</a></li>
<li class="toctree-l2"><a class="reference internal" href="Profiling.html">Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="Programs.html">Exemplary programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="Reduction.html">Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="Reduction-Example.html">Reduction example</a></li>
<li class="toctree-l2"><a class="reference internal" href="Resource-Management.html">Resource management</a></li>
<li class="toctree-l2"><a class="reference internal" href="Security.html">Security</a></li>
<li class="toctree-l2"><a class="reference internal" href="Sets.html">Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="Standard-Library.html">Standard library</a></li>
<li class="toctree-l2"><a class="reference internal" href="State.html">Stateful programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Symbolic.html">Symbolic computing</a></li>
<li class="toctree-l2"><a class="reference internal" href="Syntax.html">Syntax</a></li>
<li class="toctree-l2"><a class="reference internal" href="TermRewriting.html">Term rewriting</a></li>
<li class="toctree-l2"><a class="reference internal" href="Time.html">Time API</a></li>
<li class="toctree-l2"><a class="reference internal" href="Tools.html">Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="Types.html">Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="Units.html">Units</a></li>
<li class="toctree-l2"><a class="reference internal" href="Values.html">Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="Verification.html">Verification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../zzreferences.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Stroscot</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Commentary</a></li>
      <li class="breadcrumb-item active">Compiler design</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/Mathnerd314/stroscot/edit/master/docs/Commentary/Compiler.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="compiler-design">
<h1>Compiler design<a class="headerlink" href="#compiler-design" title="Link to this heading"></a></h1>
<p>The compiler should be fast and efficient and turn code into machine-readable instructions quickly.</p>
<section id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Link to this heading"></a></h2>
<p>We assume a mathematical function</p>
<div class="math notranslate nohighlight">
\[\newcommand{\run}[1]{⟦#1⟧}
\run{\cdot} : \text{program} \to \text{data} \to \text{result}\]</div>
<p>that can run programs written in any language, given input data, and produce an output result. We use a denotational notion of result where erroring / not halting is itself a result. Two programs are equal if <span class="math notranslate nohighlight">\(\run{p} d = \run{q} d\)</span> for all <span class="math notranslate nohighlight">\(d\)</span>; the definition of equivalence of results depends on context, and ranges from literal comparison to more advanced semantics.</p>
<p>Definitions:</p>
<ul class="simple">
<li><p>An interpreter <span class="math notranslate nohighlight">\(i\)</span> has <span class="math notranslate nohighlight">\(\run{i} (p,d) = \run{p} d\)</span>.</p></li>
<li><p>A compiler <span class="math notranslate nohighlight">\(c\)</span> has <span class="math notranslate nohighlight">\(\run{\run{c} p} d = \run{p} d\)</span>.</p></li>
<li><p>A specializer <span class="math notranslate nohighlight">\(s\)</span> has <span class="math notranslate nohighlight">\(\run{\run{s} (p,x)} y = \run{p} (x,y)\)</span>.</p></li>
<li><p>A residual program is a program <span class="math notranslate nohighlight">\(p_x\)</span> such that <span class="math notranslate nohighlight">\(\run{p_x} y = \run{p} (x,y)\)</span>.</p></li>
<li><p>A generating extension <span class="math notranslate nohighlight">\(g_p\)</span> of a program <span class="math notranslate nohighlight">\(p\)</span> has <span class="math notranslate nohighlight">\(\run{g_p} x = p_x\)</span>, i.e. it produces residual programs of <span class="math notranslate nohighlight">\(p\)</span>.</p></li>
<li><p>A generating extension generator (GNG) <span class="math notranslate nohighlight">\(n\)</span> has <span class="math notranslate nohighlight">\(\run{\run{\run{n} p} x} y = \run{p} (x,y)\)</span>.</p></li>
<li><p>A runner <span class="math notranslate nohighlight">\(r\)</span> has <span class="math notranslate nohighlight">\(\run{\run{r} c} (p,x) = \run{\run{c} p} x\)</span></p></li>
<li><p>A bundler <span class="math notranslate nohighlight">\(b\)</span> has <span class="math notranslate nohighlight">\(\run{\run{r} (c,p)} x = \run{\run{c} p} x\)</span></p></li>
</ul>
<p>We can think about specializers using the Futamura projections.</p>
<ul class="simple">
<li><p>1 specializer on a program and argument produces a residual program, <span class="math notranslate nohighlight">\(p_x = \run{s} (p,x)\)</span>.</p></li>
<li><p>2 specializers on a program produces a generating extension, <span class="math notranslate nohighlight">\(g_p = \run{s_1} (s_2,p)\)</span>.</p></li>
<li><p>3 specializers together produces a generating extension generator, <span class="math notranslate nohighlight">\(n_{123} = \run{s_1} (s_2,s_3)\)</span>.</p></li>
</ul>
<p>Applying a GNG to various things is useful:</p>
<ul class="simple">
<li><p>The generating extension of a string matcher is a matcher generator</p></li>
<li><p>The generating extension of a universal parser is a parser generator.</p></li>
<li><p>The generating extension of an interpreter <span class="math notranslate nohighlight">\(i`\)</span> is a compiler <span class="math notranslate nohighlight">\(c = \run{n} i\)</span>, <span class="math notranslate nohighlight">\(\run{\run{c} p} d = \run{\run{\run{n} i} p} d = \run{i} (p,d) = \run{p} d\)</span>. Hence GNGs are often called compiler generators.</p></li>
<li><p>The generating extension of a specializer is a GNG. <span class="math notranslate nohighlight">\(\run{\run{\run{g_s}p}x}y = \run{\run{s}(p,x)} y = \run{p}(x,y)\)</span></p></li>
</ul>
<p>GNGs can do pretty much everything specializers do, except that we need specializers in source form to generate a GNG from another GNG:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\run{\run{n} p} x\)</span> for residual programs</p></li>
<li><p><span class="math notranslate nohighlight">\(\run{n} p\)</span> for generating extensions</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{123} = \run{\run{\run{n} s_1} s_2} s_3\)</span> to obtain the same GNG as formed by applying the specializers, <span class="math notranslate nohighlight">\(\run{s_1} (s_2,s_3)\)</span>. The result is independent of the GNG used.</p></li>
</ul>
<p>Assuming <span class="math notranslate nohighlight">\(n\)</span> is a GNG, <span class="math notranslate nohighlight">\(n' = \run{n} s\)</span> is a GNG iff <span class="math notranslate nohighlight">\(s\)</span> is a specializer. Proof: <span class="math notranslate nohighlight">\(run (\run{s} (p,x)) y = \run{\run{\run{\run{n} s} p} x} y = \run{\run{\run{n'} p} x} y = \run{p} (x,y)\)</span> to show <span class="math notranslate nohighlight">\(s\)</span> is a specializer, <span class="math notranslate nohighlight">\(\run{\run{\run{n'} p} x} y = run (\run{s} (p,x)) y = \run{p} (x,y)\)</span> to show <span class="math notranslate nohighlight">\(n'\)</span> is a GNG.</p>
<p>If <span class="math notranslate nohighlight">\(\run{n} s = n\)</span>, <span class="math notranslate nohighlight">\(n\)</span> is termed a self-generating GNG. <span class="math notranslate nohighlight">\(\run{s} (s,s) = \run{\run{\run{n} s} s} s = n\)</span>. Furthermore <span class="math notranslate nohighlight">\(s\)</span> is a specializer. OTOH if <span class="math notranslate nohighlight">\(s\)</span> is a specializer then <span class="math notranslate nohighlight">\(\run{s} (s,s)\)</span> is a GNG self-generating with <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>With a runner <span class="math notranslate nohighlight">\(r\)</span> we can turn a GNG <span class="math notranslate nohighlight">\(n\)</span> into a specializer <span class="math notranslate nohighlight">\(\run{r}n\)</span>. Self-applying this specializer gives a GNG with equivalent output to <span class="math notranslate nohighlight">\(n\)</span> after two arguments have been applied:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\run{\run{\run{\run{r}c}(\run{r}c,\run{r}c)}p}x &amp; = \run{\run{\run{\run{c}(\run{r}c)}(\run{r}c)}p}x \\
&amp; = \run{\run{\run{r}c}(\run{r}c,p)}x \\
&amp; = \run{\run{\run{c}\run{r}c}p}x \\
&amp; = \run{\run{r}c}(p,x) \\
&amp; = \run{\run{c}p}x\end{split}\]</div>
</section>
<section id="design">
<h2>Design<a class="headerlink" href="#design" title="Link to this heading"></a></h2>
<p>I don’t really like how implicit building an executable is in most compiled languages.
There’s no obvious entry point where you can say “this is me invoking the compiler”, so it forces a 2-level system of shell and program.</p>
<p>I’d rather write something like:
writeFile (compileToExecutable main)</p>
<p>where it’s clear that compileToExecutable is doing the work.</p>
<p>the nanopass framework is pretty</p>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading"></a></h2>
<p>Stroscot’s main compiler uses a hybrid JIT model. Full list of execution engine features:</p>
<p>Precompiled code loading: The model loads precompiled code from a database when available.
Fast start interpretation: If precompiled code is not available, the model interprets the source code for a fast start.
On-the-fly compilation: The model compiles source code to machine code while the program is running, as needed to improve performance.
Compilation caching: The compiled machine code is saved to a database for future use.
Distributed compilation: Compilation can be offloaded and fetched from machines on the network, allowing sharing of compiles and using beefier machines.
Incremental compilation: Even if code is modified, unchanged functions can still be used.
Tiered optimization: The model can optimize instructions, basic blocks, functions, and interprocedural (tracing), depending on requirements.
Profile-guided optimization: Records fine-grained profiles and optimizes based on current and previous profiling runs.
Speculative optimizations: Inlining, specializing, prefetching, predictive evaluation, keeping cold code out of cache.
Reoptimization: The model may recompile code if observed runtime behavior is not as expected.
Image: An executable may be generated with reduced capabilities, such as no dynamic code execution, no profiling, and all code pre-loaded into the executable. Not clear which exact configurations should be supported, or what subset of the language will work.</p>
<p>How does it stack up?</p>
<p>Steady state throughput - AOT has to generate code for all possible executions, and cannot dynamically adapt to the current program’s execution pattern, hence is (in Mark Stoodley’s opinion, no real benchmarks) 50-90% as fast as JIT. Stroscot does on-the-fly compilation so can make assumptions based on observed execution data. On-the-fly compilation can implement all AOT optimizations, although traditionally JIT compilers have stopped at the 80/20 boundary so AOT compilers have had more effort invested into ensuring optimal code for some cases, e.g. vectorization, hence do better on microbenchmarks.
Adapting to changes - AOT cannot adapt at all. Per <a class="reference external" href="https://youtu.be/gx8DVVFPkcQ?t=2171">talk</a>, cached/AOT code is usually within 5-20% of peak JIT performance - not clear how to close gap besides reoptimization. Reoptimization should allow reaching peak performance regardless of starting state.
Ease of use - AOT is more complicated, two commands instead of one. Also requires specifying target platform, vs. autodetecting.
Start up time - AOT has minimal startup time, 20-50% than JIT without cache. With the compilation cache, Stroscot should be able offer startup as good as AOT for most programs, although maybe the disk access patterns will not be as optimized as AOT. The fast start interpreter means Stroscot has no compilation stall on a never-before-seen program, whereas AOT would have to stall while compiling.
Warm up time - AOT has minimal warm-up, but doesn’t get as high as a JIT. The cache allows Stroscot to have minimal warm-up time to reach performance similar to AOT, but reoptimization is enabled so there is still a warm-up time to reoptimize and reach peak performance.
Runtime footprint - The image has minimal footprint so is the most suitable for embedded / real-time cases. Dropping runtime code generation, profile collection, and network capabilities produces the smallest CPU / memory footprint, at the cost of some language capabilities. If runtime code generation is needed, profiling and distributed compilation can be enabled in the image. Compilation memory/CPU usage is spiky and transient. Doing it on a beefy server makes the client machine’s memory footprint not much more than the application load, hence much more predictable. CPU usage for sending data over the network interface may still be significant, but the client’s profiling data is forwarded so there is no loss of optimization capability. A split debug/release model does however introduce the issue of heisenbugs, e.g. profile collection influencing performance and making the compiled profile for the image inaccurate.
Debugging - Stroscot should be easy to debug in-process because all the metadata is in memory and close to hand. Images require separate debug data, not clear if DWARF is sufficient.
Cross-compiling - the distributed and image models both allow offloading optimization to the host and profiling on the target.</p>
<p>SELF, Javascript, PyPy, Java, luajit</p>
<p>Image formats - Per numerous benchmarks of shared vs static, shared libraries are essentially a stupid legacy format; anything embedded should use a statically linked self-contained image. But it still makes sense to support them as an image target for compatibility.
The equivalent of “object files” in the JIT model are the profiling data and compile cache. Rather than ld, there is the JIT or the image generator.</p>
<p>I’m going to skip having a bytecode format like Java - the user provides textual source code files, and the compile cache includes processed AST checksums. If disk bandwidth is an issue, gzip compression is fine. Java bytecode is barely optimized, and it’s easily decompiled. Javascript has shown that source-based distribution works fine, and obfuscators have been written for closed-source applications. The image capability is probably what closed-source applications will gravitate towards though.</p>
<p>The interpreter:</p>
<ul class="simple">
<li><p>A parser - this is written using nondeterminism. Likely the full syntax will not be fast enough for practical purposes until late in the project, so for now the parser uses a deterministic Lisp-like syntax. The parser records file and line number information, token start/end, call stack, and other debugging information. Produces IR.</p></li>
<li><p>Fexpr interpreter loop - this starts with the AST in the IR and produces a value. The main part is dispatching pattern matches. Uses the eval-apply model, similar to <span id="id1">[<a class="reference internal" href="../zzreferences.html#id48" title="Paul Downen, Zachary Sullivan, Zena M. Ariola, and Simon Peyton Jones. Making a faster curry with extensional types. In Proceedings of the 12th ACM SIGPLAN International Symposium on Haskell, Haskell 2019, 58–70. Berlin, Germany, August 2019. Association for Computing Machinery. URL: https://www.microsoft.com/en-us/research/publication/making-a-faster-curry-with-extensional-types/ (visited on 2020-06-14), doi:10.1145/3331545.3342594.">DSAPJ19</a>]</span>.</p></li>
<li><p>Logic prover - a CDCL satisfiability search algorithm, handles nondeterminism such as dispatch, checking if a value is a member of a type (checking functions etc. is nondeterministic), explicit lub, checking property of program, etc.</p></li>
<li><p>Memory management - uses logic prover</p></li>
<li><p>A dynamic assembler / JIT code generator</p></li>
</ul>
<p>The specializer:</p>
<ul class="simple">
<li><p>Supercompiler / partial evaluator: computes possible states of the program</p></li>
<li><p>Figures out how to represent space of program states efficiently (to avoid state explosion)</p></li>
<li><p>Optimizer: inlining method calls, eliminating redundant code, and pipelining instructions</p></li>
<li><p>Code generation: converts state transition relation to assembly instructions of the code target</p></li>
<li><p>Static verification: Warns if error states are reachable, checks other specified properties to generate warnings</p></li>
</ul>
<p>The JIT:</p>
<ul class="simple">
<li><p>Maintains tiered caches of IR: instruction, block, method, trace. Either empty, profiling, or compiled.</p></li>
<li><p>Interleaves specialized generated machine code and the interpreter</p></li>
<li><p>Profiler: gathering runtime statistics (branches, calls) to identify hotspots and make better optimization decisions.</p></li>
<li><p>Specializes hot loops. To improve overall execution speed, assuming full CPU utilization, the speedup (in ms) times the number of executions must be higher than the time spent compiling. Generally this means the code must be executed at least 1000+ more times. Fortunately most real word apps (and benchmarks) are like that (run more than a second with high code reuse factor). With an old profile we can guess that the total number of runs will be the same, but without data a good predictor is the observed number of executions so far. And with on-stack replacement back-branches are also useful to measure re-executed basic blacks. For estimation purposes it would be good to know the probability distribution for the number of time a function is executed. ChatGPT says that the distribution is heavily application-dependent - some applications follow the Pareto distribution, but others are more like a log-normal distribution (0 at 0, right-biased hump), and some applications have several humps.</p></li>
</ul>
<p>Methods can be prioritized in a list by (rate + 1) * (i + 1) * (b + 1), rate = d(i + b) / dt. d is deoptimizations, giving those methods an advantage. There is a cutoff at i + b &gt;= 1500 so low-execution methods are not compiled. The highest N interpreted methods go to C1 with detailed profiling for eventual C2, the rest go to C1 with only counters so not too many methods are profiling simultaneously. A compiler thread running concurrently with execution threads processes compilation requests. While compilation is in progress, interpreted execution continues, including for methods in the process of being JIT’ed. Once the compiled code is available, the interpreter branches off to it. Methods may be pre-empted from detailed profiling by hotter methods. C2 similarly compiles from the top of the queue. Trivial methods or methods that C2 fails to analyze go into a perma-C1 state without profiling. Methods that fail or de-opt in C1 may go directly from interpreter to C2, if the profile is sufficient. Both C1 and C2 optimizations rely on speculative assumptions, so “de-optimizations” where a function’s optimized code is discarded after hitting a trap can (and will) happen as the code learns which speculations stick. But after a while, deopts will be rare.</p>
<p>Methods are compiled so deoptimization is only possible at locations known as safepoints. Indeed, on deoptimization, the virtual machine has to be able to reconstruct the state of execution so the interpreter can resume the thread at the point in the method where compiled execution stopped. At a safepoint, a mapping exists between elements of the interpreter state (locals, locked monitors, and so on) and their location in compiled code—such as a register, stack, etc.</p>
<p>In the case of a synchronous deoptimization (or uncommon trap), a safepoint is inserted at the point of the trap and captures the state needed for the deoptimization. In the case of an asynchronous deoptimization, the thread in compiled code has to reach one of the safepoints that were compiled in the code in order to deoptimize.</p>
<p>Re-ordering operations across a safepoint would cause the state at the safepoint to differ from the original state. As a consequence, a compiled method only includes a few safepoints (on return, at calls, and in loops), rather than for every bytecode of a method.</p>
<p>Profile data consists of several collection of info:
* per-method counters:</p>
<blockquote>
<div><ul class="simple">
<li><p>invocation_counter - Incremented before each activation of the method - used to trigger frequency-based optimizations</p></li>
<li><p>backedge_counter - Incremented before each backedge taken - used to trigger frequency-based optimizations</p></li>
<li><p>Previous time the rate was acquired</p></li>
<li><p>Events (invocation and backedge counter increments) per millisecond</p></li>
<li><p>invoke_mask per-method</p></li>
<li><p>backedge_mask per-method</p></li>
<li><p>Total number of events saved at previous callback</p></li>
<li><p>Count of times method was exited via exception while interpreting</p></li>
<li><p>number_of_breakpoints, for fullspeed debugging support</p></li>
<li><p>Highest compile/OSR level this method has ever seen.</p></li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>detailed: instruction-level counts, several invocation/backends counts with timestamp, data on branches, call receiver types, typechecks (checkcast, instanceof, aastore). but collecting it adds 35% overhead over just per-method counters</p></li>
</ul>
<p>Whole-Program Compilation - all code must be available at compile-time. This allows several optimizations
• Enables monomorphization which increases inlining opportunities and avoids the need to box primitives.
• Enables aggressive dead code elimination and tree shaking which significantly reduces code size.
• Enables cross namespace/module optimizations.</p>
<p>In the past, requiring access to the entire source code of a program may been impractical. Today, systems are sufficiently performant that JavaScript, Python, PHP, and Rust have ecosystems where there is no separate compilation, and arguably Java pioneered this model with JIT compilation not paying any attention to module boundaries. Similarly Google and Facebook use monolithic repositories of source code, but have caching optimizations so that developers may use the cloud.</p>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Link to this heading"></a></h2>
<p>For a lot of compilation decisions we have several choices and want to pick the best one based on some criterion. Generally, there are various measurements to try to minimize. E.g. at compile time, there are various relevant metrics: execution time, memory usage, power usage. Similarly at runtime, there are more metrics: execution time, power usage, memory usage, executable size, throughput (work/time), latency (time from request to response). The runtime stuff is pretty loose - pretty much anything that can be estimated is fair game.</p>
<p>Complicating optimization, these criteria are not hard numbers but probabilistic variables, because computer performance depends on many uncontrollable factors hence is best treated is nondeterministic. We can consider simple statistics such as worst-case, best-case, average/mean, percentiles/quartiles, median, and mode, and differences such as range (worst-best). We can also consider moment-based values such as variance, standard deviation, coefficient of variation, skewness, and kurtosis. Going further, we can fit a probability distribution. According to the literature, execution time may be modeled by a Gumbel distribution (<a class="reference external" href="http://www.lasid.ufba.br/publicacoes/artigos/Estimating+Execution+Time+Probability+Distributions+in+Component-based+Real-Time+Systems.pdf">ref</a>) or odd log-logistic generalized gamma (OLL-GG) or exponentiated Weibull (<a class="reference external" href="https://arxiv.org/pdf/2006.09864.pdf">ref</a>), although these experiments should probably be redone as we are measuring different programs. The testbench is <a class="reference external" href="https://mjsaldanha.com/sci-projects/3-prob-exec-times-1/">here</a> and <a class="reference external" href="https://github.com/matheushjs/ElfProbTET">here</a> and could be extended with <a class="reference external" href="https://www.rdocumentation.org/packages/evd/versions/2.3-6/topics/gev">gev</a>.</p>
<p>It would be great to support optimizing the code for any objective function based on some combination of these criteria. But that’s hard. So let’s look at some use cases:</p>
<ul class="simple">
<li><p>For a focused objective like running static verification, all we want to see the error messages so total elapsed compile time is the only measurement. Maybe we even want to disable outputting a binary, and all associated tasks.</p></li>
<li><p>For a compile-run cycle run locally, e.g. a REPL or debugging session, we most likely just care about compile time plus run time execution time.</p></li>
<li><p>For release builds, the main optimization criteria is some runtime criterion, like latency, execution time, etc. As a second constraint there is probably a compile time budget - although the binary will be used for some time, a 3 week compile time is probably not feasible. Thirdly maybe some “cost to compile” calculation.</p></li>
<li><p>For CI builds on PRs, done in a cloud environment with 1000s of builds a day, “total cost to test” (compile+run) is most important. The main contributor to cost is power usage, but there could also be some  “machine rent / hour” cost.</p></li>
<li><p>For compiling on Raspberry Pi, we mainly just want to get a build at all, but also it would good if it was fast. Maximum amount of memory, minimize some linear combination of compile time and runtime.</p></li>
<li><p>For embedded, we want a small executable size (not the smallest possible though, there is probably a known budget like 64K), and to minimize runtime and compile time.</p></li>
</ul>
<p>It seems the main objective function is always a weighted linear combination, and then we may want to add hard limit constraints (inequalities). So that’s what we’ll support initially, it’s already better than GCC / Clang because you can tune the weights explicitly.</p>
<p>We use branch-and-bound to explore the possibilities. With good heuristics even the truncated search algorithm should give good results. The goal is to quickly find bottleneck code regions that have significant effects on performance and compute good optimizations quickly. Then another profiling build to test that the proposed changes were correct.</p>
<p>There is also ISA selection and tuning for specific machines and CPUs. ISA, timing, cache, and memory characteristics are available for specific CPUs, but compiling specifically for a single CPU is not done often. Usually for x86 the code is compiled to work on SSE2 (since it’s part of AMD64) and tuned for a “generic” CPU. The definition of this is vague - for <a class="reference external" href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81616">GCC</a> and <a class="reference external" href="https://reviews.llvm.org/D118534">LLVM</a> it seems to be Haswell with a few slow cases on other architectures patched. It is supposed to be “an average of popular targets”, so using a weighted sum of processors according to sales is most appropriate, but per-CPU-model sales data doesn’t seem to be available easily. <a class="reference external" href="https://www.cpubenchmark.net/share30.html">PassMark</a>, <a class="reference external" href="https://benchmarks.ul.com/compare/best-cpus?amount=0&amp;sortBy=POPULARITY&amp;reverseOrder=true&amp;types=MOBILE,DESKTOP&amp;minRating=0">3DMark</a>, and <a class="reference external" href="https://cpu.userbenchmark.com/">UserBenchmark</a> publish their list of most benchmarked processors, which is probably good enough.</p>
<p>Formally proving optimizations correct is a good idea, as they are often buggy.</p>
<p>E.g. overloading/dispatch can be implemented in a variety of ways, specialized for call site - generally it boils down to branching on some condition (binary search), or doing a table lookup. The fastest solution depends on which clauses are relatively hot, but in general we don’t know which clauses are hot.</p>
<p>Profile-guided optimization is an effective solution to this lack of information: we instrument a binary with counters for the various questions we might ask, and generate a profile with the answers. We might need to run a binary several different times to get good coverage so we also need a way to combine profiles together, i.e. profiles form a commutative monoid. Profiles themselves introduce a “Heisenbug” problem: we cannot measure the detailed performance of an unprofiled program, and turning profiling off may change the performance significantly. The solution is to build with profiling support for almost all of the compilation pipeline. We should only omit profiling instructions for non-profiled builds at the assembly level. And if we use hardware-assisted sampling profiling then we don’t even need profiling instructions, in many cases, so profiling can simply be always enabled. Still, if we are using profile information all the time and making major decisions based on it, it is important to be mostly accurate even on the initial run, so a good approximation is also key. (TODO: approximation of profiles is probably a whole research area, explore)</p>
<p>Direct Method Resolution: Optimizing method calls to assembly jumps to specific addresses during execution</p>
</section>
<section id="optimization-variables">
<h2>Optimization variables<a class="headerlink" href="#optimization-variables" title="Link to this heading"></a></h2>
<p>The variables controlled by the optimization criteria include the standard optimization flags and more. Speculative inlining possibilities, register allocation, instruction scheduling, instruction selection, lifetimes of various compile-time caches,</p>
</section>
<section id="build-model">
<h2>Build model<a class="headerlink" href="#build-model" title="Link to this heading"></a></h2>
<p>We have several complicating features:</p>
<ul>
<li><p>Cross compilation: In general, we have not one system, but two systems. To use the newer <a class="reference external" href="https://clang.llvm.org/docs/CrossCompilation.html">Clang</a> terminology, there is the <strong>host</strong> system where the program is being built, and the <strong>target</strong> system where the program will run. When the host and target systems are the same, it’s a native build; otherwise it’s a cross build.</p>
<p>The older <a class="reference external" href="https://gcc.gnu.org/onlinedocs/gccint/Configure-Terms.html">GNU terminology</a> uses a triple, build/host/target; but the “target” there is really a configuration option, namely the supported target of the compiler that will run on the host. It is a gcc-ism to specify the supported target, as Clang is generally built to support all supported targets. Since remembering whether the build system builds the host or vice-versa is tricky, overall the Clang terminology host/target/supported targets seems clearer than build/host/target.</p>
</li>
<li><p>Bootstrapping: We start with the source <code class="docutils literal notranslate"><span class="pre">s</span></code> and bootstrap compiler <code class="docutils literal notranslate"><span class="pre">cB</span></code>, an old compiler using the old ABI. Then we build stage 1 <code class="docutils literal notranslate"><span class="pre">c1=run(cB,s)</span></code>, new compiler on old ABI (targeting the host), and stage 2 <code class="docutils literal notranslate"><span class="pre">c2=run(c1,s)</span></code>, new compiler on new ABI (targeting the target). We can test stage 2 (the “compiler bootstrap test”) by building a new compiler <code class="docutils literal notranslate"><span class="pre">c3=run(c2,s)</span></code>. If the build is deterministic, <code class="docutils literal notranslate"><span class="pre">c3</span></code> should be bit-identical to <code class="docutils literal notranslate"><span class="pre">c2</span></code>. With multiple bootstrap compilers <code class="docutils literal notranslate"><span class="pre">cB</span></code>, we can use diverse double-compiling <span id="id2">[<a class="reference internal" href="../zzreferences.html#id170" title="David A. Wheeler. Fully countering trusting trust through diverse double-compiling. arXiv:1004.5534 [cs], April 2010. Comment: PhD dissertation. Accepted by George Mason University, Fairfax, Virginia, USA's Volgenau School of Information Technology and Engineering in 2009. 199 single-side printed pages. URL: http://arxiv.org/abs/1004.5534 (visited on 2020-09-13), arXiv:1004.5534.">Whe10</a>]</span> to increase our confidence in the correctness of the stage 2 compiler.</p></li>
</ul>
<p>The toolchain (gcc, llvm, as, ld, ar, strip, etc.) should be target-dependent, information stored in a YAML file or similar
the package set is also target-dependent. some packages that are pure data are target-independent</p>
<blockquote>
<div><p>We can also run the test suite to compare outputs of <code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code>. But we cannot compare performance of <code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code>, because they use different ABIs, and also <code class="docutils literal notranslate"><span class="pre">cB</span></code> may be buggy so <code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code> may not behave exactly the same.</p>
</div></blockquote>
<p>The compiler depends on libraries. The bootstrap compiler does not provide updated libraries, so we must build the libraries for the Stage 1 compiler.</p>
<p>build stage 2 compiler with the stage 1 compiler using the stage 1 package database ship with the stage 2 compiler). As such, the compiler is built with the identical libraries that it ships with. When running / interpreting byte code, we need to dynamically link packages and this way we can guarantee that the packages we link are identical to the ones the compiler was built with. This it is also the reason why we don’t have GHCi or Template Haskell support in the stage 1 compiler.</p>
</section>
<section id="complex-bootstrap">
<h2>Complex bootstrap<a class="headerlink" href="#complex-bootstrap" title="Link to this heading"></a></h2>
<p>Software is bootstrappable when it does not depend on a binary seed, i.e. a seed that is not built from source. The “trusting trust” attack is only a symptom of an incomplete or missing bootstrap story - if every program is built from source, the attack is impossible. In practice, every software needs some bootstrap binaries, but the number and size of binary seeds should be a bare minimum.</p>
<p>For example Guix uses bootstrap-seeds (hex0 binaries), bootar (extract tar), and a static build of GNU Guile 2.0.9 (for build scripts / utilities). Then it builds gash (Scheme implementation of bash), <a class="reference external" href="https://github.com/oriansj/stage0-posix">https://github.com/oriansj/stage0-posix</a>, and GNU Mes. Mes is a mutually self-hosting Scheme interpreter, C compiler, and C runtime library. Maybe you don’t trust GNU Guile as the bootstrap. You can use “diverse double-compiling” and substitute the Scheme implementation of your choice as the bootstrap host implementation. For example GNU Mes itself. As the build is reproducible and depends minimally on the build host, the resulting GNU Mes should be identical regardless. GNU Mes can thus be regarded as a high-assurance bootstrap seed, that pretty much verifies itself. From GNU Mes, Guix then builds tcc (patched TinyCC), old gzip/make/patch, gcc 2.95 + GNU tools, gcc 4.9.4 + GNU tools, and finally modern gcc and the rest of the software stack.</p>
<p>So that is interesting and all, but how do we bootstrap Stroscot? Building a “self-hosted” compiler is a real challenge. You need to maintain at least two compilers (one to bootstrap your self-hosted compiler, and the self-hosted compiler itself). There is really a combination of strategies:</p>
<ul class="simple">
<li><p>Chaining a prior build - we see from the gcc build that chaining prior builds is a valid strategy whenever there is a fundamental change in the build requirements / compiler language (such as GCC changing from C to C++). In fact it is technically valid to use the “natural bootstrap process” - build each commit from the version of the previous commit, down to the initial bootstrap. But it is a bit slow - to reproduce a build at commit N you have to build roughly N binaries. Also fragile, as what do you do with a commit that breaks the build. It is better to have a manually-specified custom chain. It is important to specify the bootstrap chain within the compiler repo, directly or as a commit hash of a different repo, so you don’t run into git bisect issues like “I checked out an old commit but it uses a different bootstrap process so it doesn’t build”.</p></li>
<li><p>Seed compiler code - We can generate lower-level code from the source code, such as C, Java, Haskell, WASM, or a custom bytecode. The code can be generated automatically from the main compiler’s source, as a backend target, but it is not clear if this is sufficiently verifiable - I guess it depends on how readable the code is and whether it can be matched efficiently with the original code. For example, much of the code is devoted to optimizing, backends, error messages, caching, and langauge server which is not necessary for bootstrapping. It is also possible to write this seed compiler code by hand, but then you have to maintain two compilers.</p></li>
<li><p>Seed interpreter/VM - Bootstrapping from machine code with Hex0 is possible but it makes a lot more sense for portability and sanity to use a higher-level language as the initial seed. We could use GNU Mes, GCC, the JVM, WASM, Haskell, etc. as the seed language. The key is that the interpreter/VM can process the seed compiler output. It does not need to be particularly optimized, it just has to bootstrap an initial self-hosted version - e.g. it most likely does not have to free memory. Practically it will be a recent self-hosted optimized build that is used as the final step of the chain, for git bisect etc.</p></li>
</ul>
<p>Actually bootstrapping is more complex. The compiler is really two components, an interpreter and a specializer. The input program can take arguments. The interpreter can take arguments (dialects, libraries). The specializer can take arguments (bytecode, optimization instructions, plugins). The output program can take arguments (compiled objects, runtime components such as libc or a garbage collector). All of these arguments and options aren’t handled easily. Like platforms, probably it is easiest to bootstrap x86 first and then build other platforms by cross-compiling.</p>
</section>
<section id="compile-time-code-execution">
<h2>Compile-time code execution<a class="headerlink" href="#compile-time-code-execution" title="Link to this heading"></a></h2>
<p>We want to execute code that runs at compile time, e.g. reading a blob of data to be included as a literal. Clearly this code executes on the host, with the same filesystem as the rest of the source code.</p>
<p>We also want to read configuration, e.g. the target platform properties (word size, endianness, etc.).</p>
<p>Also we want to do computations with no runtime inputs, like 1+2.</p>
</section>
<section id="compiler-ways">
<h2>Compiler ways<a class="headerlink" href="#compiler-ways" title="Link to this heading"></a></h2>
<p>GHC calls some options “compiler ways”. They can be combined (e.g. threaded + debugging). The main issue is they affect the ABI, so ways need be stored into ABI hashes in installed libraries to avoid mismatching incompatible code objects.</p>
<ul class="simple">
<li><p>use the multi-threaded runtime system or not</p></li>
<li><p>support profiling or not</p></li>
<li><p>use additional debug assertions or not</p></li>
<li><p>use different heap object representation (e.g. <code class="docutils literal notranslate"><span class="pre">tables_next_to_code</span></code>)</p></li>
<li><p>support dynamic linking or not</p></li>
</ul>
<p>Depending on the selected way, the compiler produces and links appropriate objects together. These objects are identified by a suffix: e.g. <code class="docutils literal notranslate"><span class="pre">*.p_o</span></code> for an object built with profiling enabled; <code class="docutils literal notranslate"><span class="pre">*.thr_debug_p.a</span></code> for an archive built with multi-threading, debugging, and profiling enabled. See the gory details on the <a class="reference external" href="https://gitlab.haskell.org/ghc/ghc/wikis/commentary/rts/compiler-ways">wiki</a>.</p>
<p>Installed packages usually don’t provide objects for all the possible ways as it would make compilation times and disk space explode for features rarely used. The compiler itself and its boot libraries must be built for the target way.</p>
</section>
<section id="compiler-memory-management">
<h2>Compiler memory management<a class="headerlink" href="#compiler-memory-management" title="Link to this heading"></a></h2>
<p>For the compiler itself, a trivial bump or arena allocator is sufficient for most purposes, as it is invoked on a single file and lasts a few seconds. With multiple files and large projects the issue is more complicated, as some amount of information must be shared between files. Optimization passes are also quite traversal-intensive and it may be more efficient to do in-place updates with a tracing GC rather than duplicating the whole AST and de-allocating the old one. Two other sources of high memory usage are macros and generics, particularly in combination with optimizations that increase code size such as inlining.</p>
<p>Overall I don’t see much of an opportunity, SSD and network speeds are sufficient to make virtual memory and compile farms usable, so the maximum memory is some large number of petabytes. The real issue is not total usage but locality, because compilers need to look up information about random methods, blocks, types etc. very often. But good caching/prefetching heuristics should not be too hard to develop. In practice the programs people compile are relatively small, and the bottleneck is the CPU because optimizations are similar to brute-force searching through the list of possible programs. Parallelization is still useful. Particularly when AMD has started selling 64-core desktop processors, it’s clear that optimizing for some level of that, maybe 16 or 32 cores, is worthwhile.</p>
</section>
<section id="dynamic-execution">
<h2>Dynamic execution<a class="headerlink" href="#dynamic-execution" title="Link to this heading"></a></h2>
<p>benefit: erases distinction between compile time and execution time. Hence optimizes for compile+execute time.</p>
<p>loading code at runtime
- typecheck, JIT compile, return function pointer
the function pointer doesn’t have to be machine code, it can be bytecode, so the function runs through an interpreter
Compiler from IR to bytecode
Saving snapshots of the VM state (images)
Tracing JIT compiler
Use libgccjit for code generation?
Optimized assembly interpreter a la LuaJIT and JavaScriptCore</p>
<p>everyone had two entry points.
if you came from the
interpreter you had to call the
interpreter entry point and you
came from JITed code you entered the
JITed code favorite entry point</p>
<p>the goal here was JITed calling JITed had minimal overhead
so an x86 call instruction with the JITed entry point’s address</p>
<p>so if a JITed calls interpreted there’s a
JITed entry point that shuffles the
arguments and jumps to the interpreter</p>
<p>and if the interpreter makes
a call, it’s a slow procedure that looks
up the interpreter endpoint or else
jumps to a trampoline that jumped to the JITed code</p>
<p>then there’s deoptimization
it’s tricky to stop running processors
from running code
if you try to
edit the method call buffers processors have
them cached
you
can’t actually stop it
so first you change the vtable to the interpreter
then you change the head of the method to jump to the interpreter</p>
<p>there’s also speculative optimization and escape analysis</p>
<p>Creating the compiled file consumes extra CPU time and storage vs the interpreter. The compiled version runs more efficiently. Some errors are only detected during compilation.</p>
<p>Julia - faster than Python, but JIT uses many slow trampolines</p>
<p>Javascript - V8 is a fast modern JIT</p>
<p>In a sea of nodes program dependence graph (PDG), nodes correspond to arithmetic/logic operations but also to control operations such as conditional jumps and loops. edges correspond to dependencies among operations.</p>
<p>graphs corresponding to relatively small programs turn quickly into a tangle that is quite difficult to grasp. PDGs cannot be read directly without assistance; this affects debugging speed. PDGs remain an obscure topic in advanced compiler courses.</p>
<p>In a CFG, nodes correspond to basic blocks, ordered sequences of operations that are always executed together. every operation belongs to a single basic block. edges correspond to control jumps across basic blocks. A CFG yields a structured, sequential view of the program that is easier to understand and debug, and is familiar for many systems engineers.</p>
<p>To turn a PDG into a CFG, compute an assignment of operations to basic blocks (global schedule) and an ordering of operations within each basic block (local schedule).</p>
<p>clustering basic blocks into (nested) loops, if-then-else structures, etc.
coloring the basic blocks that are executed most often</p>
<p>the value representation is optimized for the platform, and redundant checks are optimized out</p>
<p>The Implementation of Functional Programming Languages
Implementing functional languages: a tutorial
Implementing Lazy Functional Languages on Stock Hardware: The Spineless Tagless G-Machine
How to make a fast curry: push/enter vs eval/apply
GHC also does strictness analysis and optimistic evaluation.</p>
<p>a program is a dependency graph which is evaluated through a series of local reductions
the graph itself can be represented as code. In particular, we can represent a node as a function that when invoked, returns the desired value. The first time it is invoked, it asks the subnodes for their values and then operates on them, and then it overwrites itself with a new instruction that just says “return the result.”</p>
<p>JIT cache: need &gt;90% hit rate to pay off vs just doing normal JIT path of interpeting bytecode and optimizing. need profile data, otherwise optimizations will be different. The profile is a few megabytes but the compiled code may be 100s of megabytes since it has a lot of metadata.</p>
<p>rare methods don’t show up in the profile, but may still need to be fast.</p>
<p>the c2 strategy is a counter with an absolute threshold. so eventually, as long it is not dead code, it will be JITed. it guarantees enough samples so that you have a good profile. trying to do an exponential decay so only hot methods</p>
<p>L1 cache is cheaper than memory, so clean up bytecode as soon as it is generated</p>
</section>
<section id="ir-dump">
<h2>IR dump<a class="headerlink" href="#ir-dump" title="Link to this heading"></a></h2>
<p>A good compiler can get 80% of the code to a fast-enough state. But nontrivial hot spots will still need hand-optimizing and tuning. At first it can be good to tweak the original code to get it to generate IR differently, but eventually the algorithm is set and the micro-optimizations matter, so you want to bake in the low-level implementation.</p>
<p>With a wide-spectrum language the IR is the same language as the original, just using lower-level operations. So you can compile source-to-source or directly write in the IR. For example SQL is declarative but being able to write a functional program using the underlying sort, filter, merge anti-join, etc. operations would be useful.</p>
<p>There are many levels to the pipeline, and each one is useful. For an interpreted program the only step that can’t be represented is actually running the program, e.g. converting <code class="docutils literal notranslate"><span class="pre">print</span> <span class="pre">&quot;Hi&quot;</span> <span class="pre">exit</span></code> to output.</p>
</section>
<section id="incremental-compilation">
<h2>Incremental compilation<a class="headerlink" href="#incremental-compilation" title="Link to this heading"></a></h2>
<p>Incremental compilation reduces rebuild time. With a good incremental build system, optimizations can be rechecked rather than rediscovered, so that the program doesn’t actually spend much time optimizing even though it has expensive optimizations.</p>
</section>
<section id="hot-reloading">
<h2>Hot reloading<a class="headerlink" href="#hot-reloading" title="Link to this heading"></a></h2>
<p>Hot reloading or “edit and continue” is the ability to change code and resources of a live application without restarting it. It speeds up the edit-test cycle because you can stay on a certain state of the program without needing to spend time to recreate it. It can be useful for games, UI design, or data analysis.</p>
<p>Edit and continue is really a debugger feature, because usually you edit the code while paused on a breakpoint, rather than while the program is actually running. Integrating with omniscient debugging is probably best, so you can manually select an old state and then evolve it using the new transition rules. For example when editing the jump height for a jump’n’run game, you probably don’t want to continue from the game’s start, or even the first jump input, but rather to just before the one tricky jump in the middle of the level. There is no indication of this magic location in the code or program state besides the player’s x-coordinate being a certain value.</p>
<p>Erlang has hot code swapping, Smalltalks and Lisps have “live programming.” Assisting System Evolution: A Smalltalk Retrospective is a recommended read.</p>
<p>The most basic implementation is to patch functions calls so they call a new function instead of an old one. A JIT already does this kind of patching when switching from interpreted to optimized code, so can do it easily. With ahead-of-time you can compile a new DLL, duplicate it to avoid locking, load it, and swap out the function pointer, but it requires specially marking the hot-reloadable methods.</p>
<p>Functions generally assume a fixed set of types and a fixed memory representation for all types. Changing the types or their representation can break program invariants and cause memory corruption. But it is possible - there are some projects for live kernel patching that can patch in-memory data structures to the correct format.</p>
<p>State is also an issue because the memory manager must be aware of the local state of a piece that reloaded and avoid leaking memory. In the case of handles such as an OpenGL context the desirable behavior is to transfer them over to the new code, but if the initialization code is changed then the handle should instead be closed and re-initialized. So we see some sort of incremental program execution going on.</p>
<p>live-patching: depending on optimizations, all callers maybe impacted, therefore need to be patched as well.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Code-Generation.html" class="btn btn-neutral float-left" title="Code generation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Compiler-Library.html" class="btn btn-neutral float-right" title="Compiler library" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2022 Mathnerd314.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>