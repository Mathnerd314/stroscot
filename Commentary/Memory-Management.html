<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Memory management &mdash; Stroscot  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/hexagon_favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=7f41d439"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Meta" href="Meta.html" />
    <link rel="prev" title="Macros" href="Macros.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/hexagon_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/index.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../HowTo/index.html">How to</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Reference/index.html">Language Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Commentary</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Aspects.html">Aspects</a></li>
<li class="toctree-l2"><a class="reference internal" href="Assembly.html">Assembly</a></li>
<li class="toctree-l2"><a class="reference internal" href="BuildSystem.html">Build system</a></li>
<li class="toctree-l2"><a class="reference internal" href="Checklist.html">Checklist</a></li>
<li class="toctree-l2"><a class="reference internal" href="Code-Generation.html">Code generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="Compiler.html">Compiler design</a></li>
<li class="toctree-l2"><a class="reference internal" href="Compiler-Library.html">Compiler library</a></li>
<li class="toctree-l2"><a class="reference internal" href="CompilerOutput.html">Compiler output</a></li>
<li class="toctree-l2"><a class="reference internal" href="Concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoreSyntax.html">Core syntax</a></li>
<li class="toctree-l2"><a class="reference internal" href="Dispatch.html">Dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="Errors.html">Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="Evaluation-Strategy.html">Evaluation strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="Exceptions.html">Exceptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="Fastest.html">As fast as C</a></li>
<li class="toctree-l2"><a class="reference internal" href="FunctionalLogic.html">Functional logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="IR.html">Intermediate representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="Learning.html">Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="Logic.html">Logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="LogicProgramming.html">Logic programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Macros.html">Macros</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Memory management</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#memory-models">Memory models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pointers">Pointers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#storage-vs-memory">Storage vs. memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#you-choose-allocation">“You-choose” Allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#optimizing-access">Optimizing access</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cheri">CHERI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#persistent-memory">Persistent memory</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#references">References</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pointer-conversion">Pointer conversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#finalizers">Finalizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#formal-definition">Formal definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="#non-prompt-finalization">Non-prompt finalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#subsuming-manual-memory-management">Subsuming manual memory management</a></li>
<li class="toctree-l4"><a class="reference internal" href="#finalization-order">Finalization order</a></li>
<li class="toctree-l4"><a class="reference internal" href="#freed-on-exit">Freed on exit</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sloppy-frees">Sloppy frees</a></li>
<li class="toctree-l4"><a class="reference internal" href="#evaluation-order">Evaluation order</a></li>
<li class="toctree-l4"><a class="reference internal" href="#on-borrowing">On borrowing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#timeliness">Timeliness</a></li>
<li class="toctree-l4"><a class="reference internal" href="#design">Design</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#allocator">Allocator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pieces">Pieces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mutator">Mutator</a></li>
<li class="toctree-l4"><a class="reference internal" href="#collector">Collector</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">Allocator</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dumping-ground">Dumping ground</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Meta.html">Meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="Modules.html">Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="Objects.html">Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="OpPrims.html">Operational primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="PackageManager.html">Package manager</a></li>
<li class="toctree-l2"><a class="reference internal" href="Parsing.html">Parsing</a></li>
<li class="toctree-l2"><a class="reference internal" href="Posets.html">Posets</a></li>
<li class="toctree-l2"><a class="reference internal" href="Profiling.html">Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="Programs.html">Exemplary programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="Reduction.html">Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="Reduction-Example.html">Reduction example</a></li>
<li class="toctree-l2"><a class="reference internal" href="Resource-Management.html">Resource management</a></li>
<li class="toctree-l2"><a class="reference internal" href="Security.html">Security</a></li>
<li class="toctree-l2"><a class="reference internal" href="Sets.html">Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="Standard-Library.html">Standard library</a></li>
<li class="toctree-l2"><a class="reference internal" href="State.html">Stateful programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Symbolic.html">Symbolic computing</a></li>
<li class="toctree-l2"><a class="reference internal" href="Syntax.html">Syntax</a></li>
<li class="toctree-l2"><a class="reference internal" href="TermRewriting.html">Term rewriting</a></li>
<li class="toctree-l2"><a class="reference internal" href="Time.html">Time API</a></li>
<li class="toctree-l2"><a class="reference internal" href="Tools.html">Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="Types.html">Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="Units.html">Units</a></li>
<li class="toctree-l2"><a class="reference internal" href="Values.html">Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="Verification.html">Verification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../zzreferences.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Stroscot</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Commentary</a></li>
      <li class="breadcrumb-item active">Memory management</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/Mathnerd314/stroscot/edit/master/docs/Commentary/Memory-Management.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="memory-management">
<h1>Memory management<a class="headerlink" href="#memory-management" title="Link to this heading"></a></h1>
<p>The language should have automatic memory management. Manual memory management is slow, tedious, and error prone. Automatic memory management is better in all respects, but the implementation has to be flexible enough to be usable for all the things manual memory management is.</p>
<section id="memory-models">
<h2>Memory models<a class="headerlink" href="#memory-models" title="Link to this heading"></a></h2>
<p>Per <span id="id1">[<a class="reference internal" href="../zzreferences.html#id89" title="Jeehoon Kang, Chung-Kil Hur, William Mansky, Dmitri Garbuzov, Steve Zdancewic, and Viktor Vafeiadis. A formal C memory model supporting integer-pointer casts. In Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation, 326–335. Portland OR USA, June 2015. ACM. URL: https://dl.acm.org/doi/10.1145/2737924.2738005 (visited on 2021-06-14), doi:10.1145/2737924.2738005.">KHM+15</a>]</span> there are pretty much two models of memory, pointers and references. Pointers model memory as an integer-indexed array of 2^32 or 2^64 words, accessed by the OS/hardware APIs. References model memory as an associative array from symbolic “references” (potentially infinite in number) to “cells”, values (stored in some unspecified format, but with lossless storage).</p>
<p>Kang describes how combinations of these can be made, for example the “quasi-concrete model” which uses a data type that starts out containing a reference, implements various arithmetic operations symbolically, but switches to a pointer once an integer address is requested. You can also imagine the other direction, a pointer that masquerades as a reference but errors when attempting to store a value larger than the allocation. But references and pointers are the fundamental ideas and serve to implement all other possibilities.</p>
<p><span id="id2">[<a class="reference internal" href="../zzreferences.html#id24" title="Programming Language Ideas That Work And Don't Work. June 2022. URL: https://www.youtube.com/watch?v=y7KWGv_t-MU (visited on 2023-07-19).">bri22</a>]</span> brings up the old x86 16-bit pointer model. There were data, code, stack, and extra segment registers. A near pointer simply adds an offset to the appropriate segment register. Far and huge pointers set the segment register first, allowing access to other segments. Far pointers were unnormalized, while huge points were normalized to a canonical segment+offset pair. Nowadays, in x86-64, pointers are just represented as a uniform 64-bit absolute address. The only residue of segment addressing is there are some “load relative” instructions that take offsets instead of absolute pointers.</p>
<p>Bright suggests that the lesson is to only have one type of pointer. But I disagree. The lesson is really to ensure that a pointer is self-contained, in that it always points to the same location, and unique, in that no other pointer value refers to that location. In the 16-bit pointer model, only far and huge pointers were self-contained. And far and huge pointers had the issue of allowing multiple representations of the same address. The normalization solved this, but there were disagreements on how to normalize and it was often skipped for performance reasons. Comparatively, the 64-bit model has a unique pointer value for every address. Turning now to modern models, the concrete and symbolic models are both fine in this regard; integers and symbols are self-contained and unique.</p>
<p>Bright also raises the specter that “You will wind up with two versions of every function, one with manually managed pointers and one with garbage collected pointers (references). The two versions will require different implementations. You’ll be sorry.” How worrisome is this?</p>
<p>Well, first let’s try to use a pointer as a reference. There are many issues to consider:</p>
<ul class="simple">
<li><p>Allocation size: Generally it is assumed the pointer points to some fixed-size buffer of bytes. But this means we can’t store arbitrary-sized values; they just don’t fit. Usually this is solved by restricting the possible values to a finite set, then the storage is fixed.</p></li>
<li><p>Serialization: To mimic the ability of a reference to store heterogeneous types of data, strings, numbers, lists, functions, and so on, we need a universal serialization function, that e.g. stores a type tag. We can probably expose such a serialization function from the compiler, as the compiler needs such a function to implement references. Alternatively, for a restricted type, this is solved by writing a custom serialization function.</p></li>
<li><p>Ownership - Pointers can just be calculated out of thin air, so some other function could overwrite our buffer. The format could be corrupted, or the memory could be deallocated altogether. Usually this is solved by making a private copy of the buffer at some isolated address that no other part of the program uses, and only writing back changes at the end in one atomic operation.</p></li>
</ul>
<p>Is someone really going to work through these issues and write a second version of the function? When they could just make the pointer into a reference with <code class="docutils literal notranslate"><span class="pre">newRef</span> <span class="pre">(deserialize</span> <span class="pre">(readBytes</span> <span class="pre">10</span> <span class="pre">ptr))</span></code> and let the compiler do all the work? References should have decent performance so there will be no reason to try to shoehorn a pointer into a reference-like API. Pointers are really a low-level, byte-based abstraction whose only business is interfacing with C code. As evidence that they are needed I offer <a class="reference external" href="https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/unsafe-code#pointer-types">C#</a> which has included them since 1.0.</p>
<p>As far as using a reference as a pointer, as long as we don’t want to do pointer arithmetic, we can just store an array of bytes in the reference. Such a pattern is common in Java, e.g. the ArrayList class. But when we want to materialize some bytes at a given memory address, there is no way to do it with references. References just don’t support interfacing with C code.</p>
<p>I guess it is possible that someone will have two versions of a function, one that implements it in pure Stroscot via references and one that calls out a C library with pointers. But externally, I think both of them should interact with the rest of the code using references. Using pointers with the C code might avoid a few conversion calls, but references are a lot cleaner to use, e.g. avoiding the use of callbacks, and there is the guaranteed optimization that you can use a reference as a pointer with zero-cost. So I don’t think this poses an issue. Even if the C wrapper did use pointers because it was easier than converting to/from references all the time, that’s a judgement call on the part of the library author and I don’t think there is a solution that would let everyone standardize on one universal type. The best a “pointerOrRef” type can support, even restricted to a small type like <code class="docutils literal notranslate"><span class="pre">int8</span></code>, is get/set like a regular reference.</p>
</section>
<section id="pointers">
<h2>Pointers<a class="headerlink" href="#pointers" title="Link to this heading"></a></h2>
<p>Pointers are the low-level API, they can interface with the OS or other languages (mainly C). I did a study of Windows/Linux memory APIs and concluded that memory is best modeled as the global mutable array <code class="docutils literal notranslate"><span class="pre">Memory</span> <span class="pre">=</span> <span class="pre">Map</span> <span class="pre">(Word,BitIdx)</span> <span class="pre">Status</span></code>. The status allows storing metadata, it’s <a class="reference external" href="https://github.com/Mathnerd314/stroscot/blob/master/src/model/MemoryStatus.hs">a complex ADT</a> which has various states like unallocated, committed, etc. The array is indexed at the bit level because that’s the granularity <a class="reference external" href="https://valgrind.org/docs/manual/mc-manual.html#mc-manual.machine">Valgrind’s Memcheck</a> uses, but most of the status will be the same for a byte or page as the memory allocators / OS operations work at higher granularity.</p>
<p>It is simple enough to maintain “extra” status bits, and instrument memory functions to check the status of memory before operating. This is essentially what Valgrind does. With this it is possible to identify many common errors, like double free, use after free, access to undefined memory, and null pointer dereferencing. But there is still the possibility of overflowing a buffer into an adjacent allocation, or more generally <a class="reference external" href="https://en.wikipedia.org/wiki/Type_punning">type punning</a> by reading some memory as a format it was not written with. These sorts of possibilities are intrinsic to the “big array of bits” model, and many low-level hacks rely on such functionality, so I would say to use references if you want to avoid such things. But of course someone can easily add bounds-checking etc. on top of the basic pointer model as a library.</p>
<p>Most addresses will not be allocated (status Free), hence the array is sparse in some sense. It is in fact possible to implement the typical <a class="reference external" href="https://developer.android.com/reference/android/util/SparseArray">sparse array operations</a>. There are functions to directly allocate memory at an address. Reading and writing are done directly in assembly. The list of currently mapped pages can be had from <code class="docutils literal notranslate"><span class="pre">/proc/self/maps</span></code> and <a class="reference external" href="https://reverseengineering.stackexchange.com/questions/8297/proc-self-maps-equivalent-on-windows/8299">VirtualQueryEx</a>, although this has to be filtered to remove pages reserved by the kernel and internal pages allocated by the runtime, and looks slow - it’s easier to wrap the allocation functions and maintain a separate list of user-level allocations. Clearing mappings, hashing memory, and indexing by mapped pages all work when restricted to the list of user pages. It’s a little more complicated than simple sparsity because there are many different statuses and the operations overlap.</p>
<section id="storage-vs-memory">
<h3>Storage vs. memory<a class="headerlink" href="#storage-vs-memory" title="Link to this heading"></a></h3>
<p>In practice, the path from cloud to CPU is long, and accessible storage is not just RAM. Some latency numbers and the programming API:</p>
<ul class="simple">
<li><p>Physical registers (0.3 ns): managed by the CPU</p></li>
<li><p>Logical registers (0.3 ns): assembly read/write</p></li>
<li><p>Memory Ordering Buffers (MOB), L1/L2/L3 Cache (0.5-7 ns): Managed by the CPU</p></li>
<li><p>Main Memory (0.1us-4us): assembly read/write</p></li>
<li><p>GPU memory (0.2us-0.5us): assembly read/write, driver ioctl’s</p></li>
<li><p>NVRAM (200us-250us): assembly read/write, special calls</p></li>
<li><p>SSD (250-500us): kernel file APIs</p></li>
<li><p>LAN (0.5-500ms): kernel network stack, driver bypass</p></li>
<li><p>HDD (3 ms): kernel file APIs</p></li>
<li><p>WAN (150ms): kernel network stack, driver bypass</p></li>
</ul>
<p>Not all applications will use all of these, but all will use some and there is an application that uses each. So all of these have to be modeled in order to create a performant application. Ideally the memory management system would be a “storage management system” that combines all of these into a single pointer-like abstraction and allows copying data between locations as appropriate. But it’s a leaky abstraction, I’m not sure it can be pulled off except as a library.</p>
</section>
<section id="you-choose-allocation">
<h3>“You-choose” Allocation<a class="headerlink" href="#you-choose-allocation" title="Link to this heading"></a></h3>
<p>In practice, fixed-address allocation / assignment is not commonly used. Instead, there are <code class="docutils literal notranslate"><span class="pre">mmap</span> <span class="pre">NULL</span></code>, <code class="docutils literal notranslate"><span class="pre">malloc</span></code>, and the C library API alloc/realloc, which allocate memory with system-chosen / allocator-chosen location. For verifying behavior, the right model for this is adversarial, i.e. the allocator chooses the worst possible location, subject to restrictions such as that the allocation must be suitably aligned and disjoint from all unrevoked allocations. More formally, the behavior of a correct program should not depend on what addresses the system picks, i.e. all choices should be observationally equivalent. (The system can also return an out of memory error, but this doesn’t have to result in equivalent behavior.)</p>
<p>Of course, the actual allocation strategy should not be the worst, rather it should try to achieve the best performance. For the most part, people do not seem to pay much attention to allocator design, because it is pretty cheap. For example <a class="reference external" href="https://www.forrestthewoods.com/blog/benchmarking-malloc-with-doom3/">in Doom 3</a> the median time for is 31 nanoseconds, ranging from 21 nanoseconds to 201 microseconds, and free is comparable.</p>
<p>But, speeding up allocation is actually fairly important. Combining operations into a single larger operation (allocate a larger buffer, call <code class="docutils literal notranslate"><span class="pre">close_range</span></code> to close several open FD’s than to iterate over them individually) by pushing allocations forward and delaying frees, as long as there is sufficient memory or resource capacity available, can be a big win. In contrast, reads and writes are always real work, and besides SIMD there is not much way to optimize it.</p>
<p>There are also a lot of locality and cache effects from the address allocation algorithm. In the trivial case, the memory usage can be predicted in advance and allocations given fixed assignments, giving zero cost memory allocation. In more practical applications, variable allocations will need to be tracked, but there are still tricks for grouping allocations based on access patterns, avoiding fragmentation. Most research has been on runtime allocation optimization, but many of these optimizations can be precomputed at compile time. For example:</p>
<ul class="simple">
<li><p>A loop that allocates and deallocates a scratch buffer in the body is much more performant if the buffer is allocated to the same location every time - the allocation/deallocation code can even be pulled out of the loop.</p></li>
<li><p>Grouping hot variables into a page, so the page is always loaded and ready</p></li>
<li><p>Grouping things that will be freed together (pools/arenas)</p></li>
</ul>
</section>
<section id="optimizing-access">
<h3>Optimizing access<a class="headerlink" href="#optimizing-access" title="Link to this heading"></a></h3>
<p>Generally, optimizations are allowed to eliminate possibilities allowed by the memory model, but there could also be an option to strictly preserve the set of possibilities.</p>
<p>Eliminating a pointer read amounts to tracking down the matching pointer write and propagating the value directly, which can be accomplished by tracing control flow. There is the issue of data races with concurrent writes, but the memory model dictates which values a read may resolve to, and the verifier already handles nondeterminism, so it is not much harder than normal value propagation. There is also modeling foreign code, specifically determining whether the foreign code can write a pointer (i.e, whether the pointer is shared or not).</p>
<p>Eliminating a pointer write requires proving that the address is never read before deallocation or another pointer write. Again there are the issues of data races and foreign code.</p>
</section>
<section id="cheri">
<h3>CHERI<a class="headerlink" href="#cheri" title="Link to this heading"></a></h3>
<p>CHERI pointers are 129-bit, consisting of a 1-bit validity tag, bounds, permissions, object type, and actual pointer. Valid pointers may only be materialized in a register or memory by transforming an initial unbounded pointer obtained from the OS. This means that the simple model of pointers as integers is no longer valid. Instead, a pointer is the combination of an integer address and a capability. The <a class="reference external" href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-947.pdf">CHERI C/C++ API</a> represents capabilities as <code class="docutils literal notranslate"><span class="pre">void*</span></code> and addresses as <code class="docutils literal notranslate"><span class="pre">vaddr_t</span></code>.</p>
<p>I tried to read further, but the model is complicated, essentially implementing a GC to avoid dangling pointers, so I am not sure it will ever become mainstream.</p>
</section>
<section id="persistent-memory">
<h3>Persistent memory<a class="headerlink" href="#persistent-memory" title="Link to this heading"></a></h3>
<p>The pointer API, assembly wrapping, and OS calls cover using persistent memory via standard file APIs or memory-mapped DAX. Memory is volatile while persistent memory is not, so persistent memory is faster storage, not weird RAM. And storage is complex enough that it seems best handled by libraries. Making the memory management system memkind-aware seems possible, like memory bound to NUMA nodes.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p>A reference is a symbolic index into a global associative array of objects, <code class="docutils literal notranslate"><span class="pre">Map</span> <span class="pre">Reference</span> <span class="pre">Object</span></code>. The array allows allocating new references, deleting them, and reading/writing the reference. Reference symbols can be compared for equality, hashed to an integer, and packed/unpacked to/from an integer.</p>
<p>The packing and hashing requires a little explanation. Packing the same reference always returns the same value during a program execution, and the packed value is distinct from the packed value of any other reference. But the exact value is internal to the memory system - it is an “adversarial” model similar to pointers where if the program’s behavior depends on the choice of packed value it is incorrect. The hashing is similar to packing, it is again the same value for the same reference, it is just that there is no distinctiveness constraint (so the program must have the same behavior even if all references hash to 0), and also no way to unhash the value, so there is no need to worry about resolving unpack invocations.</p>
<p>There are higher-level types like immutable references and reference wrappers, but those all translate away to normal references or pointer access and don’t need involvement from the compiler.</p>
<section id="pointer-conversion">
<h3>Pointer conversion<a class="headerlink" href="#pointer-conversion" title="Link to this heading"></a></h3>
<p>The location of the data of a reference is not fixed. If it’s small enough it could just be in a register, or there could be multiple copies of the data in memory. Also GC can move/copy the reference. The data could be produced on-demand and be represented by a thunk. All that can really be said is that the compiler will respect the semantics of storing and retrieving data.</p>
<p>Foreign operations like OS calls require a pointer to a memory address, because references don’t necessarily exist in memory. The canonical way of doing this is simply reading the reference value and storing it in a buffer represented by a pointer (“materializing” it in memory). Internally, when compiling away the reference, the compiler tries to find a good way to store the reference - if it’s lucky, it can backpropagate the pointer request and store the data there from the beginning, so that the “read and store” operation is actually a no-op that makes zero copies.</p>
<p>But, in the fallback case of storing a few words, where a memory allocation is appropriate, the reference translates directly to a pointer allocation. The memory is configured to trap on stray user-level access, so that only the compiler-generated code has access. Even in this case, though, the reference’s internal value is not the pointer itself, rather there is a more complex strategy of using a “handle” identifier that allows moving the data around after it is allocated.</p>
</section>
</section>
<section id="finalizers">
<span id="id3"></span><h2>Finalizers<a class="headerlink" href="#finalizers" title="Link to this heading"></a></h2>
<p>So far we have just seen manual memory management. For both pointers and references, you allocate the memory, and then “when you are done with it”, you have to free the pointer or delete the reference. Many other resources work the same way, like thread handles, file handles, and sockets. But of course, forgetting to free resources (leaking) is a common error. Finalizers are inspired by ASAP and allow the prompt freeing of allocated memory and resources, RAII-style. They assume there is exactly one best place to free the resource, “immediately after the last use of the operation”. The compiler determines this location and automatically inserts the free operation there. Such an analysis is more precise than traditional GC, because GC looks at what references are “in scope” and cannot free an unused reference embedded in a structure whose other parts are in use. But semantics-wise, finalization is a static, completely solvable problem. It is just of a high complexity <span class="math notranslate nohighlight">\(\Sigma^0_1\)</span>. So if we are willing to accept potentially long compile times, we can eliminate memory errors.</p>
<section id="formal-definition">
<h3>Formal definition<a class="headerlink" href="#formal-definition" title="Link to this heading"></a></h3>
<p>More formally, a finalizer is a magic value created with the one-argument function <code class="docutils literal notranslate"><span class="pre">newFinalizer</span> <span class="pre">:</span> <span class="pre">(free</span> <span class="pre">:</span> <span class="pre">Command)</span> <span class="pre">-&gt;</span> <span class="pre">Op</span> <span class="pre">Finalizer</span></code>. It supports equality, hashing, and command <code class="docutils literal notranslate"><span class="pre">use</span> <span class="pre">:</span> <span class="pre">Finalizer</span> <span class="pre">-&gt;</span> <span class="pre">Command</span></code> and <code class="docutils literal notranslate"><span class="pre">useForever</span> <span class="pre">:</span> <span class="pre">Finalizer</span> <span class="pre">-&gt;</span> <span class="pre">Op</span> <span class="pre">Command</span></code>. The semantics is that <code class="docutils literal notranslate"><span class="pre">free</span></code> will be called as soon as it is known that <code class="docutils literal notranslate"><span class="pre">use</span></code> and <code class="docutils literal notranslate"><span class="pre">useForever</span></code> will not be called. Calling <code class="docutils literal notranslate"><span class="pre">use</span></code> delays finalization until after the <code class="docutils literal notranslate"><span class="pre">use</span></code>, and <code class="docutils literal notranslate"><span class="pre">useForever</span></code> cancels the finalizer and returns the free operation. The general transformation:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">reduce</span><span class="w"> </span><span class="p">(</span><span class="kt">NewFinalizer</span><span class="w"> </span><span class="n">free</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="ow">=</span>
<span class="w">  </span><span class="n">f</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">freshSymbol</span>
<span class="w">  </span><span class="n">transform</span><span class="w"> </span><span class="p">(</span><span class="n">c</span><span class="w"> </span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="n">free</span><span class="p">,</span><span class="n">f</span><span class="p">}</span>
<span class="nf">reduce</span><span class="w"> </span><span class="p">(</span><span class="kt">Use</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">c</span>
<span class="nf">reduce</span><span class="w"> </span><span class="p">(</span><span class="kt">UseForever</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="n">free</span>

<span class="nf">transform</span><span class="w"> </span><span class="kt">:</span><span class="w"> </span><span class="kt">Task</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">Task</span>
<span class="nf">transform</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="ow">=</span>
<span class="w">  </span><span class="kr">if</span><span class="w"> </span><span class="o">!</span><span class="p">(</span><span class="n">could_call</span><span class="w"> </span><span class="p">(</span><span class="kt">Use</span><span class="w"> </span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">could_call</span><span class="w"> </span><span class="p">(</span><span class="kt">UseForever</span><span class="w"> </span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="n">c</span><span class="p">)</span>
<span class="w">    </span><span class="n">reduce</span><span class="w"> </span><span class="p">(</span><span class="n">free</span><span class="w"> </span><span class="p">{</span><span class="n">continuation</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">c</span><span class="p">})</span>
<span class="w">  </span><span class="kr">else</span><span class="w"> </span><span class="kr">if</span><span class="w"> </span><span class="n">will_definitely_call</span><span class="w"> </span><span class="p">(</span><span class="kt">UseForever</span><span class="w"> </span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="n">c</span>
<span class="w">    </span><span class="n">reduce</span><span class="w"> </span><span class="n">c</span>
<span class="w">  </span><span class="kr">else</span>
<span class="w">    </span><span class="kr">if</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="p">(</span><span class="n">will_definitely_call</span><span class="w"> </span><span class="p">(</span><span class="kt">Use</span><span class="w"> </span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="n">c</span><span class="p">)</span>
<span class="w">      </span><span class="n">info</span><span class="p">(</span><span class="s">&quot;Delaying finalizer due to conditional usage&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="kr">let</span><span class="w"> </span><span class="n">c&#39;</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">continuation</span><span class="w"> </span><span class="n">c</span>
<span class="w">    </span><span class="n">reduce</span><span class="w"> </span><span class="p">(</span><span class="n">c</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">continuation</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">transform</span><span class="w"> </span><span class="n">c&#39;</span><span class="w"> </span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="non-prompt-finalization">
<h3>Non-prompt finalization<a class="headerlink" href="#non-prompt-finalization" title="Link to this heading"></a></h3>
<p>Finalizers do not really free memory “immediately after the last use”, as the <cite>info(“Delaying finalizer due to conditional usage”)`</cite> message points out. This situation happens when the location to free depends on further input:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">af</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;a&quot;</span>
<span class="nf">a</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">newFinalizer</span><span class="w"> </span><span class="n">af</span>
<span class="nf">b</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="kt">Bool</span>
<span class="kr">if</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="kr">then</span>
<span class="w">  </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;b&quot;</span>
<span class="w">  </span><span class="n">exit</span>
<span class="kr">else</span>
<span class="w">  </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;c&quot;</span>
<span class="w">  </span><span class="n">use</span><span class="w"> </span><span class="n">a</span>
<span class="w">  </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;d&quot;</span>
<span class="w">  </span><span class="n">exit</span>
</pre></div>
</div>
<p>Because <code class="docutils literal notranslate"><span class="pre">a</span></code> might be used in the else branch, it cannot be freed between the <code class="docutils literal notranslate"><span class="pre">newFinalizer</span> <span class="pre">af</span></code> and <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">Bool</span></code> statements, even though this would be the earliest place to free for a “true” input. Instead, <code class="docutils literal notranslate"><span class="pre">a</span></code> is freed as soon as it is known it will (unconditionally) not be used, hence this program is equivalent to:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">af</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;a&quot;</span>
<span class="nf">b</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="kt">Bool</span>
<span class="kr">if</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="kr">then</span>
<span class="w">  </span><span class="n">af</span>
<span class="w">  </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;b&quot;</span>
<span class="w">  </span><span class="n">exit</span>
<span class="kr">else</span>
<span class="w">  </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;c&quot;</span>
<span class="w">  </span><span class="n">af</span>
<span class="w">  </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;d&quot;</span>
<span class="w">  </span><span class="n">exit</span>
</pre></div>
</div>
<p>Non-prompt finalization can be made into an error/warning if prompt memory management is desired. I was calling prompt finalizers “destructors” for a while, because they behave a lot more like C++-style RAII. The API was slightly different, instead of a <code class="docutils literal notranslate"><span class="pre">free</span></code> operation that gets called at random times, destructors had this operation <code class="docutils literal notranslate"><span class="pre">lastUse</span> <span class="pre">:</span> <span class="pre">Destructor</span> <span class="pre">-&gt;</span> <span class="pre">Op</span> <span class="pre">Bool</span></code>, that returns false for every call except the last. The pattern for using this API is to make each use check if it’s the <code class="docutils literal notranslate"><span class="pre">lastUse</span></code>, like <code class="docutils literal notranslate"><span class="pre">use</span> <span class="pre">(PromptFinalizer</span> <span class="pre">free</span> <span class="pre">d)</span> <span class="pre">=</span> <span class="pre">{</span> <span class="pre">l</span> <span class="pre">=</span> <span class="pre">lastUse</span> <span class="pre">d;</span> <span class="pre">if</span> <span class="pre">l</span> <span class="pre">{</span> <span class="pre">free</span> <span class="pre">}</span> <span class="pre">}</span></code>. But eventually I proved that this destructor API is equivalent to just using finalizers with the error enabled: the <code class="docutils literal notranslate"><span class="pre">use</span></code> function I presented lets you wrap a destructor as a finalizer, and destructors can be implemented using finalizers by:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">isFinalized</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">mut</span><span class="w"> </span><span class="n">false</span>
<span class="nf">f</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">newFinalizer</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">isFinalized</span><span class="w"> </span><span class="kt">:=</span><span class="w"> </span><span class="n">true</span><span class="w"> </span><span class="p">}</span>
<span class="nf">lastUse</span><span class="w"> </span><span class="ow">=</span>
<span class="w">  </span><span class="n">use</span><span class="w"> </span><span class="n">f</span>
<span class="w">  </span><span class="n">read</span><span class="w"> </span><span class="n">isFinalized</span>
</pre></div>
</div>
<p>This implementation of destructors works just fine in programs where the warning is not triggered. But with non-prompt finalization, delaying until known, the contract is not valid because <code class="docutils literal notranslate"><span class="pre">lastUse</span></code> could return false even though it is the last use (delaying the finalizer after the read).</p>
</section>
<section id="subsuming-manual-memory-management">
<h3>Subsuming manual memory management<a class="headerlink" href="#subsuming-manual-memory-management" title="Link to this heading"></a></h3>
<p>By construction, finalizers without the warning are as prompt as finalizers with the warning, on the programs where the warning does not trigger. In particular, finalizers subsume prompt finalizers subsume manual memory management. Taking a program written with standard <code class="docutils literal notranslate"><span class="pre">malloc/free</span></code>, we can gradually change it:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">malloc</span></code> is wrapped to return a tuple with <code class="docutils literal notranslate"><span class="pre">newFinalizer</span></code>, <code class="docutils literal notranslate"><span class="pre">free</span></code> is replaced with <code class="docutils literal notranslate"><span class="pre">use</span></code></p></li>
<li><p>every operation is modified to call <code class="docutils literal notranslate"><span class="pre">use</span></code></p></li>
<li><p>the finalizer warning is turned off</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">use</span></code> calls corresponding to <code class="docutils literal notranslate"><span class="pre">free</span></code> are removed</p></li>
</ol>
<p>Up until step 4, the finalizer program compiles identically to the original. It’s step 4 that’s a bit fragile - the lifetime of the finalizer could be shortened and, depending on the program structure, the point at which <code class="docutils literal notranslate"><span class="pre">free</span></code> should be called may be hard to compute. But hopefully the analysis will be fairly robust and able to handle most cases. At worst, the programmer will have to provide additional help to the finalizer analysis in the form of inserting the <code class="docutils literal notranslate"><span class="pre">use</span></code> statements corresponding to <code class="docutils literal notranslate"><span class="pre">free</span></code>. Either way, since all operations call <code class="docutils literal notranslate"><span class="pre">use</span></code>, the program behavior is not changed, only its resource management.</p>
</section>
<section id="finalization-order">
<h3>Finalization order<a class="headerlink" href="#finalization-order" title="Link to this heading"></a></h3>
<p>If multiple finalizers simultaneously become able to call <code class="docutils literal notranslate"><span class="pre">free</span></code>, then finalizer instruction insertions are run in the order of creation, first created first. This means the free calls will execute most recent first.</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">a</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">newFinalizer</span><span class="w"> </span><span class="p">(</span><span class="n">print</span><span class="w"> </span><span class="s">&quot;a&quot;</span><span class="p">)</span>
<span class="nf">b</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="n">newFinalizer</span><span class="w"> </span><span class="p">(</span><span class="n">print</span><span class="w"> </span><span class="s">&quot;b&quot;</span><span class="p">)</span>

<span class="kr">if</span><span class="w"> </span><span class="n">randomBool</span><span class="w"> </span><span class="kr">then</span>
<span class="w">  </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;c&quot;</span>
<span class="w">  </span><span class="n">exit</span>
<span class="kr">else</span>
<span class="w">  </span><span class="n">print</span><span class="w"> </span><span class="s">&quot;c&quot;</span>
<span class="w">  </span><span class="n">use</span><span class="w"> </span><span class="n">a</span>
<span class="w">  </span><span class="n">use</span><span class="w"> </span><span class="n">b</span>
<span class="w">  </span><span class="n">exit</span>

<span class="o">#</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">bool</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">false</span><span class="kt">:</span><span class="w"> </span><span class="n">cab</span>
<span class="o">#</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">bool</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">true</span><span class="kt">:</span><span class="w"> </span><span class="n">bac</span>
</pre></div>
</div>
</section>
<section id="freed-on-exit">
<h3>Freed on exit<a class="headerlink" href="#freed-on-exit" title="Link to this heading"></a></h3>
<p>Many resources are automatically freed by the OS on exit: memory, file handles, etc. This automatic freeing is generally more efficient than releasing each resource one by one. So as an optimization, one would like to <em>not</em> free these resources earlier, but rather hold on to them until the program exits and the OS frees them itself. So what we need is an analysis that determines at what point in the program there are sufficient spare resources that any further allocation can be satisfied without deallocation. This measure “the remaining amount of additional memory the program might use” will not necessarily be compared against the remaining memory amount of free physical memory actually available, but more likely a configurable parameter like 2MB. Once this point is determined the compiler can insert <code class="docutils literal notranslate"><span class="pre">useForever</span></code> calls to mark all the in-use resources as not needing manual finalization.</p>
</section>
<section id="sloppy-frees">
<h3>Sloppy frees<a class="headerlink" href="#sloppy-frees" title="Link to this heading"></a></h3>
<p>GC is more composable and it can also be faster than manual memory management <span id="id4">[<a class="reference internal" href="../zzreferences.html#id9" title="Andrew W. Appel. Garbage collection can be faster than stack allocation. Information Processing Letters, 25(4):275–279, June 1987. URL: https://www.cs.princeton.edu/~appel/papers/45.pdf (visited on 2020-07-24), doi:10.1016/0020-0190(87)90175-X.">App87</a>]</span>. As Appel points out, even if freeing an individual object is a single machine instruction, such as a stack pop, freeing a lot of objects still has significant overhead compared to copying out a small amount of useful data and just marking a whole region of objects as free. In a similar vein, sometimes we do not actually want the finalizer to run as promptly as possible, but rather batch it with other allocations and free it all in one go. The opportunities for batching are hard to detect and even harder to implement by hand. Setting some “slop factor” of memory that can be delayed-freed is quite useful - the only downside is that if the program is pushing the limits of memory maybe it will crash at 1.9GB instead of 2GB.</p>
<p>Really, we are distinguishing “unused” or “dead” memory from memory that is released back to the OS or the rest of the program.</p>
</section>
<section id="evaluation-order">
<h3>Evaluation order<a class="headerlink" href="#evaluation-order" title="Link to this heading"></a></h3>
<p>There are also “space leaks” where memory could be freed earlier by evaluating expressions in a specific order but some other order is chosen. Certainly there is some evaluation order that results in minimum RAM usage, but maybe a less compact order is more time-efficient. So there is some amount of time-space tradeoff for this category. Finalizers kind of skirt this issue by being completely imperative, but with unsafePerformIO this becomes relevant again.</p>
</section>
<section id="on-borrowing">
<h3>On borrowing<a class="headerlink" href="#on-borrowing" title="Link to this heading"></a></h3>
<p>Rust has gotten much attention with the borrow checker, documented in <span id="id5">[]</span>. Similar to finalizers, Rust also has a concept of the “lifetime” of each reference. But, whereas in Stroscot the lifetime is simply the set of program states during which the reference is not dead, in Rust a lifetime is a <em>region</em> consisting of annotating each program point with the set of <em>loans</em> of the reference, where each loan is either unique or shared. At each point, a reference may have no loans, one unique loan, or many shared loans - no other possibilities are allowed. This restrictive set of allowed access patterns means that Rust does not allow simple cyclic pointer patterns such as doubly-linked lists.</p>
<p>Similarly, Val’s <a class="reference external" href="https://www.jot.fm/issues/issue_2022_02/article2.pdf">mutable value semantics</a> is even more restrictive than Rust, dropping references altogether and simply using the function parameter annotation <code class="docutils literal notranslate"><span class="pre">inout</span></code>. But it once again cannot represent any sort of cyclic pointer structure. It is really just the trick for representing state as the type <code class="docutils literal notranslate"><span class="pre">s</span> <span class="pre">-&gt;</span> <span class="pre">(a,s)</span></code>, and doesn’t handle memory management at all.</p>
<p>In practice, Rust developers have a variety of escapes from the borrow checker.  code frequently switches to the <code class="docutils literal notranslate"><span class="pre">Rc</span></code> reference counted type, which besides cycles has the semantics of GC. There is even a <a class="reference external" href="https://github.com/Others/shredder">library</a> for a <code class="docutils literal notranslate"><span class="pre">Gc</span></code> type that does intrusive scanning.</p>
<p>Per <span id="id6">[<a class="reference internal" href="../zzreferences.html#id131" title="Raphaël L Proust. ASAP: As Static As Possible memory management. Technical Report UCAM-CL-TR-908, University of Cambridge Computer Laboratory, July 2017. URL: https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.pdf.">Pro17</a>]</span>, finalizers and the “free after last use” criterion subsume both region-based memory management and reference counting. <span id="id7">[<a class="reference internal" href="../zzreferences.html#id37" title="Nathan Corbyn. Practical Static Memory Management. Bachelor's Thesis, King’s College, May 2020. URL: http://nathancorbyn.com/nc513.pdf.">Cor20</a>]</span> implemented a buggy incomplete version and showed even that version is comparable to Rust.</p>
</section>
<section id="timeliness">
<h3>Timeliness<a class="headerlink" href="#timeliness" title="Link to this heading"></a></h3>
<p>If doing automatic static memory management is so easy, why hasn’t it been tried before? Well, it has. For example, <span id="id8">[]</span> has a similar notion of automatically inserting frees, and they report good results. But that paper focused on reachability, rather than lack of use, and their analysis was local to function blocks, rather than global. So it didn’t see much adoption.</p>
<p><span id="id9">[<a class="reference internal" href="../zzreferences.html#id131" title="Raphaël L Proust. ASAP: As Static As Possible memory management. Technical Report UCAM-CL-TR-908, University of Cambridge Computer Laboratory, July 2017. URL: https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.pdf.">Pro17</a>]</span> presented the theory and the formulation of the problem fairly well, but he fell into the trap of thinking that since the complexity of determining “waste blocks” was <span class="math notranslate nohighlight">\(\Sigma_0^1\)</span>, any analysis had to be approximate. There are techniques for solving such high-complexity problems precisely, as evidenced in the TERMCOMP termination analysis competition, but such techniques really only got started in 2007 or so. From his citations list, Proust didn’t really get into this area of the literature.</p>
<p>So the answer is, it seems novel to try to apply techniques from formal verification to memory management, and that’s the only technique that seems powerful enough to implement finalizers in the way presented here, where the point of finalization is guaranteed. All previous approaches have focused on approximate analyses that aren’t powerful enough to subsume manual memory management.</p>
<p>Certainly there is some risk involved in implementing a novel analysis. But it doesn’t seem like a <a class="reference external" href="https://www.youtube.com/watch?v=8uE6-vIi1rQ">“cursed problem”</a> where even trying to solve it is a waste of time - <span id="id10">[<a class="reference internal" href="../zzreferences.html#id37" title="Nathan Corbyn. Practical Static Memory Management. Bachelor's Thesis, King’s College, May 2020. URL: http://nathancorbyn.com/nc513.pdf.">Cor20</a>]</span> got decent results with just 8 months or so of part-time work. I’d rather be spending a lot of effort on solving the right problem, even if it’s hard, than getting sidetracked solving the wrong easy problem.</p>
</section>
<section id="design">
<h3>Design<a class="headerlink" href="#design" title="Link to this heading"></a></h3>
<p>Java’s finalizers have inherent problems because they are associated with GC. In particular, because the GC may not run, Java’s finalizers have no guarantee of timeliness, and hence cannot be used to free resources. In contrast Stroscot’s finalizers free as soon as it is statically known that they are no longer used. Java’s finalizers have no ordering; Stroscot’s run in the order defined. Java’s finalizers do not report exceptions; Stroscot’s finalizer methods are inserted into the program at the point the finalizer is run and can report exceptions. But like Java, the finalizer is considered done regardless of whether it throws an exception. Stroscot’s finalizers are functions and are not directly associated with objects, so there is no possibility of resurrection like in Java.</p>
<p>local (“arena”) allocators speed up short-running programs, keep long–running ones from slowing down over time. All global allocators eventually exhibit diffusion–i.e., memory initially dispensed and therefore (coincidentally) accessed contiguously, over time, ceases to remain so, hence runtime performance invariably degrades. This form of degradation has little to do with the runtime performance of the allocator used, but rather is endemic to the program itself as well as the underlying computer platform, which invariably thrives on locality of reference.”
diffusion should not be confused with fragmentation–an entirely different phenomenon pertaining solely to (“coalescing”) allocators (not covered in this paper) where initially large chunks of contiguous memory decay into many smaller (non-adjacent) ones, thereby precluding larger ones from subsequently being allocated –even though there is sufficient total memory available to accommodate the request. Substituting a pooling allocator, such as theone used in this benchmark (AS7), is a well-known solution to the fragmentationproblems that might otherwise threaten long-running mission-critical systems.”</p>
<p>Arena-based bump allocator for objects
Cheap write barrier in the common case
Mark-and-compact collection for oldest generation
Copying generational collection for younger generations
Special space (in cache?) for nursery generation
State Transitions</p>
<p>I think it’s better to write a faster GC than to try to special-case various types of allocation. The GC itself can special case things. Optimizing requires global information and only the GC has a global view.</p>
<p>Static immutable data should be interned.</p>
<p>Compress strings with shoco <a class="reference external" href="https://github.com/Ed-von-Schleck/shoco">https://github.com/Ed-von-Schleck/shoco</a> or  the sequitur algorithm <a class="reference external" href="http://www.sequitur.info/">http://www.sequitur.info/</a>. Maybe can fit into a 64-bit word. Cleaning the dictionary periodically would probably have to happen to avoid resource leaks, which might have to recompress every relevant string. Fortunately, long strings tend to be fairly long-lived.</p>
<p><a class="reference external" href="https://github.com/ollef/sixten">https://github.com/ollef/sixten</a> talks about being able to represent intrusive lists. I experimented with allowing the decision of pointer vs direct storage to be made in pack, but it really simplifies the code a lot to require all pack functions to produce flat blobs of data.</p>
<p>Destructors are inspired by C++ RAII destructors, hence the name. Admittedly the actual API doesn’t bear much resemblance. <a class="reference external" href="https://en.wikipedia.org/wiki/Finalizer">Finalizers</a> can resurrect objects and don’t have deterministic execution, hence would be a bad name. Go’s defer statement and try-finally are related, but they only work locally and have imprecise execution semantics.</p>
<p>Portable mmap:
* Yu virtualalloc <a class="reference external" href="https://github.com/alpha123/yu/tree/master/src/platform">https://github.com/alpha123/yu/tree/master/src/platform</a>
* Go: <a class="reference external" href="https://github.com/edsrzf/mmap-go">https://github.com/edsrzf/mmap-go</a>
* C: mmap on windows <a class="reference external" href="https://github.com/alitrack/mman-win32">https://github.com/alitrack/mman-win32</a>
* C++: <a class="reference external" href="https://github.com/mandreyel/mio">https://github.com/mandreyel/mio</a>
* Rust: <a class="reference external" href="https://github.com/RazrFalcon/memmap2-rs">https://github.com/RazrFalcon/memmap2-rs</a></p>
</section>
</section>
<section id="representation">
<h2>Representation<a class="headerlink" href="#representation" title="Link to this heading"></a></h2>
<p>A lot of languages have a fixed or default memory representation for values, e.g. a C struct, a Haskell ADT, and a Python object are always laid out in pretty much the same way. The more systems-level languages allow controlling the layout with flags, for example Rust has <a class="reference external" href="https://doc.rust-lang.org/reference/type-layout.html">type layout</a> and also C compatibility. Layout is then defined by its size, alignment, padding/stride, and field offsets. Now it’s great to have a compact representation of the memory layout - but only if you can actually write the memory layout you want using these features. But these flags are’t really that powerful. Here’s some examples of what can’t generally be done with the current memory DSL’s:</p>
<ul class="simple">
<li><p>specify the in-memory order of fields differently from their logical order</p></li>
<li><p>specifying how to encode enumeration constants (per struct it appears in)</p></li>
<li><p>turn array-of-structs into struct-of-arrays</p></li>
<li><p>flattening a datatype, like <code class="docutils literal notranslate"><span class="pre">Either</span> <span class="pre">Bool</span> <span class="pre">Int</span></code> into <code class="docutils literal notranslate"><span class="pre">(Bool,Int)</span></code>, or representing a linked list as a contiguous series of records.</p></li>
<li><p>storing some parts via pointer indirections (non-contiguous memory layout)</p></li>
<li><p>NaN-boxing and NuN-boxing (<a class="reference external" href="https://wingolog.org/archives/2011/05/18/value-representation-in-javascript-implementations">ref</a> <a class="reference external" href="https://searchfox.org/mozilla-central/source/js/public/Value.h#526">2</a>), representing the JS <code class="docutils literal notranslate"><span class="pre">Any</span></code> type as a single 64-bit word.</p></li>
<li><p>parsing network packets into structured data</p></li>
</ul>
<p>Maybe some of these could be addressed by flags, but from the last two, it is clear that we are really looking for a general-purpose memory serialization interface. I looked at <a class="reference external" href="https://hackage.haskell.org/package/binary-0.8.9.1/docs/src/Data.Binary.Get.Internal.html#Decoder">Data.Binary</a>, <a class="reference external" href="https://github.com/mgsloan/store/blob/master/store-core/src/Data/Store/Core.hs">store</a>, and <span id="id11">[<a class="reference internal" href="../zzreferences.html#id41" title="Benjamin Delaware, Sorawit Suriyakarn, Clément Pit-Claudel, Qianchuan Ye, and Adam Chlipala. Narcissus: correct-by-construction derivation of decoders and encoders from binary formats. Proceedings of the ACM on Programming Languages, 3(ICFP):1–29, July 2019. URL: https://www.cs.purdue.edu/homes/bendy/Narcissus/narcissus.pdf (visited on 2020-07-26), doi:10.1145/3341686.">DSPitClaudel+19</a>]</span>. Narcissus is too complex IMO:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="kt">Format</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="kt">Set</span><span class="w"> </span><span class="p">(</span><span class="kt">S</span><span class="p">,</span><span class="w"> </span><span class="kt">St</span><span class="p">,</span><span class="w"> </span><span class="kt">T</span><span class="p">,</span><span class="w"> </span><span class="kt">St</span><span class="p">)</span>
<span class="kt">Encode</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="kt">S</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">St</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">Option</span><span class="w"> </span><span class="p">(</span><span class="kt">T</span><span class="p">,</span><span class="w"> </span><span class="kt">St</span><span class="p">)</span>
<span class="kt">Decode</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="kt">T</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">St</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">Option</span><span class="w"> </span><span class="p">(</span><span class="kt">S</span><span class="p">,</span><span class="w"> </span><span class="kt">St</span><span class="p">)</span>
</pre></div>
</div>
<p>The state parameter can be gotten rid of by defining <code class="docutils literal notranslate"><span class="pre">S</span> <span class="pre">=</span> <span class="pre">(S,St),</span> <span class="pre">T</span> <span class="pre">=</span> <span class="pre">(T,St)</span></code>:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="kt">Format</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="kt">Set</span><span class="w"> </span><span class="p">(</span><span class="kt">S</span><span class="p">,</span><span class="w"> </span><span class="kt">T</span><span class="p">)</span>
<span class="kt">Encode</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="kt">S</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">Option</span><span class="w"> </span><span class="kt">T</span>
<span class="kt">Decode</span><span class="w"> </span><span class="ow">=</span><span class="w"> </span><span class="kt">T</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">Option</span><span class="w"> </span><span class="kt">S</span>
</pre></div>
</div>
<p>And we can make encode/decode total by defining <code class="docutils literal notranslate"><span class="pre">S</span> <span class="pre">=</span> <span class="pre">{s</span> <span class="pre">|</span> <span class="pre">exists</span> <span class="pre">t.</span> <span class="pre">(s,t)</span> <span class="pre">in</span> <span class="pre">Format}</span></code>, <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">=</span> <span class="pre">{t</span> <span class="pre">|</span> <span class="pre">exists</span> <span class="pre">s.</span> <span class="pre">(s,t)</span> <span class="pre">in</span> <span class="pre">Format}</span></code>.</p>
<p>I thought about letting <code class="docutils literal notranslate"><span class="pre">pack</span></code> narrow the range of values, e.g. rounding 1.23 to 1.2, but concluded that it would be surprising if storing a value to memory changed it. The rounding can be defined as a pre-pass over the data to convert it to a <code class="docutils literal notranslate"><span class="pre">Measurement</span></code> type that then has optimized storage.</p>
<p>One tricky part is that the naive way to specify types interferes with overloading, subtyping and implicit conversions. <code class="docutils literal notranslate"><span class="pre">pack</span> <span class="pre">(Int8</span> <span class="pre">1)</span></code> can give a byte as expected, but it can also implicitly convert to an <code class="docutils literal notranslate"><span class="pre">Int32</span></code> and give 4 bytes. Since we have dependent types this isn’t a real issue, just make sure the code generated after representation specialization passes the type explicitly: <code class="docutils literal notranslate"><span class="pre">pack</span> <span class="pre">Int32</span> <span class="pre">(Int8</span> <span class="pre">1)</span></code>.</p>
<p>A few things need to optimize away for reasonable performance.  <code class="docutils literal notranslate"><span class="pre">length</span> <span class="pre">.</span> <span class="pre">pack</span></code> should optimize to something like <code class="docutils literal notranslate"><span class="pre">const</span> <span class="pre">20</span></code> for most values, or at least something that doesn’t allocate, so that field accesses are independent and values can be allocated sanely. These functions might have to be hacked in, specializing to constant-sized values.</p>
<p>Since writing these serialization functions all the time would be tedious, we can make a format DSL that specifies the functions in a nicer way. Although one of these DSL’s will be the standard / default, it’ll be some kind of macro / constraint system, so defining new format DSLs for specific purposes shouldn’t be hard.</p>
<p>The translation to use pack is pretty simple: every value is wrapped in a call to pack, the result is stored as a tuple <code class="docutils literal notranslate"><span class="pre">(cell,unpack)</span></code>, and every usage applies unpack to the cell. The translation uses whatever pack is in scope; pack can be overridden like any other implicit parameters. The unpack functions will end up getting passed around a lot, but function pointers are cheap constants, and constant propagation is a thing, so it shouldn’t be an issue.</p>
<p>So finally the most general API is <code class="docutils literal notranslate"><span class="pre">Write</span> <span class="pre">=</span> <span class="pre">Alloc</span> <span class="pre">(Size,Align)</span> <span class="pre">(Addr</span> <span class="pre">-&gt;</span> <span class="pre">Write)</span> <span class="pre">|</span> <span class="pre">Store,</span> <span class="pre">Store</span> <span class="pre">=</span> <span class="pre">Map</span> <span class="pre">Addr</span> <span class="pre">MaskedWord</span></code> and <code class="docutils literal notranslate"><span class="pre">Unpack</span> <span class="pre">a</span> <span class="pre">=</span> <span class="pre">Maybe</span> <span class="pre">Addr</span> <span class="pre">-&gt;</span> <span class="pre">Read</span> <span class="pre">-&gt;</span> <span class="pre">a,</span> <span class="pre">Read</span> <span class="pre">=</span> <span class="pre">Map</span> <span class="pre">Addr</span> <span class="pre">Word</span></code>. This allows masked writes and multiple or fixed allocation addresses, but does not allow failing to read the value back. Also the <code class="docutils literal notranslate"><span class="pre">pack</span></code> function allows passing arbitrary side-band data to the <code class="docutils literal notranslate"><span class="pre">unpack</span></code> function. Maybe though, it is still not general enough, we should just have lens-like functions like <code class="docutils literal notranslate"><span class="pre">write</span> <span class="pre">:</span> <span class="pre">Memory</span> <span class="pre">-&gt;</span> <span class="pre">a</span> <span class="pre">-&gt;</span> <span class="pre">Memory</span></code> and <code class="docutils literal notranslate"><span class="pre">read</span> <span class="pre">::</span> <span class="pre">Memory</span> <span class="pre">-&gt;</span> <span class="pre">a</span></code>. There still need to be constraints though, like that you get back what you wrote and non-interference of writes.</p>
<p>Now we also want to allow optimization of the memory representation. Consider some data points - if there is only one possible value, then the compiler should optimize this to a constant and not store it at all. If there are two possible values, the compiler should probably use a boolean flag and again hard-code the values as constants. If the potential values include all values of a given type (and nothing else), then the compiler should use the representation for that type. If the potential values include a given type, and also members of another type, then the compiler should use the most narrowly-defined representation that contains both of those types. And it should consider whether it can choose the representation of the union type so as to minimize the amount of conversion needed for the more commonly used type (as in NaN/NuN-boxing). If the potential values can be anything, then the compiler should use the universal representation.</p>
<p>The process of fixing the memory representation of a program can be modeled as follows. We start with a program that passes around values. Then we insert conversion operations: on every declaration, we insert a conversion to binary, and on every use, we insert a conversion from binary. As the binary representation is defined so that a read of a write is is the identity, this transformation does not change the meaning of the program. Then we additionally write this binary representation to memory on the declaration, and read this binary representation from memory on use. Again this does not change the semantics due to the non-interference of writes property. Although, in reality it could change the semantics: maybe a cosmic ray or something could change what we have written. But at this point, our program operates purely on memory and does not have any values floating around.</p>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading"></a></h2>
<p>For memory management we have to consider values, called objects. Pointers are manually freed and hence don’t need to be managed.</p>
<p>An invalidate queue is more like a store buffer, but it’s part of the memory system, not the CPU. Basically it is a queue that keeps track of invalidations and ensures that they complete properly so that a cache can take ownership of a cache line so it can then write that line. A load queue is a speculative structure that keeps track of in-flight loads in the out of order processor. For example, the following can occur</p>
<blockquote>
<div><p>CPU speculatively issue a load from X
That load was in program order after a store to Y, but the address of Y is not resolved yet, so the store does not proceed.
Y is resolved and it turns out to be equal to X. At the time that the store to Y is resolved, that store searches the load queue for speculative loads that have issued, but are present after the store to Y in program order. It will notice the load to X (which is equal to Y) and have to squash those instructions starting with load X and following.</p>
</div></blockquote>
<p>A store buffer is a speculative structure that exists in the CPU, just like the load queue and is for allowing the CPU to speculate on stores. A write combining buffer is part of the memory system and essentially takes a bunch of small writes (think 8 byte writes) and packs them into a single larger transaction (a 64-byte cache line) before sending them to the memory system. These writes are not speculative and are part of the coherence protocol. The goal is to save bus bandwidth. Typically, a write combining buffer is used for uncached writes to I/O devices (often for graphics cards). It’s typical in I/O devices to do a bunch of programming of device registers by doing 8 byte writes and the write combining buffer allows those writes to be combined into larger transactions when shipping them out past the cache.</p>
</section>
<section id="allocator">
<h2>Allocator<a class="headerlink" href="#allocator" title="Link to this heading"></a></h2>
<p>ultimate allocator - steal features from all other allocators. It’s one of those well-researched areas where a few percent lives. Substitution isn’t really an option but maybe some components could be pluggable. Thread safe but values are pure and references can be determined to be thread-local so lots of optimizations.</p>
<p>We want to automatically determine the number of allocation regions and their size to maximize locality.</p>
<p>locate memory leaks - places where allocated memory is never getting freed - memory usage profiling</p>
<p>Handling OOM gracefully - non-allocating subset of language. Should be enough to implement “Release some resources and try again” and “Save the user’s work and exit” strategies. Dumping core is trivial so doesn’t need to be considered.</p>
<p>A derived pointer is a reference plus an offset. When the address and layout of the object is known we can store the derived pointer as the sum of the value address and offset, allowing direct pointer dereferencing. But since the address is known we could also just store the derived pointer as the offset, so it’s only useful if computing the sum is necessary and expensive.</p>
<p>An object can be treated as an array, N[i] and N.length.</p>
<p>The array part of shared memory is necessary because there is a double-word CAS operation on x86 (CMPXCHG16B), and also for efficiency.</p>
<p>With persistent memory only word-sized stores are atomic, hence the choice of shared memory as an array of words. <a class="reference external" href="https://stackoverflow.com/questions/46721075/can-modern-x86-hardware-not-store-a-single-byte-to-memory">https://stackoverflow.com/questions/46721075/can-modern-x86-hardware-not-store-a-single-byte-to-memory</a> says that there are in fact atomic x86 load/store instructions on the byte level.</p>
<dl class="simple">
<dt>word</dt><dd><p>An integer <code class="docutils literal notranslate"><span class="pre">i</span></code> with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">i</span> <span class="pre">&lt;</span> <span class="pre">MAX</span></code>.</p>
</dd>
</dl>
<p>Ternary: in current computers all words are some number of bits. Most discussion of ternary uses pure ternary, but IMO words will be a mixture of trits and bits - the mixture allows approximating the magic radix e more effectively. IDK. Whatever the case, the bit/trit (digit) is the smallest unit of memory, and all other data is a string of digits.</p>
<p>Since no commercially available computers support ternary it is not worth supporting explicitly in the language. But for future-proofing, we must ensure that anytime there is a binary string, the APi can be extended to use a mixed binary/ternary string.</p>
<p>Eliminating pointers entirely is not possible. But we can minimize the lifetime of pointers in the standard library to the duration of the call, and use values / references everywhere else.</p>
</section>
<section id="pieces">
<h2>Pieces<a class="headerlink" href="#pieces" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Safe - no dangling pointers (freeing object from live set)</p></li>
<li><p>Complete - no memory leaks (never freeing object from dead set). There is also excessive memory usage, where a program continually uses ever-growing arrays, e.g. an ever-growing Game of Life configuration. But this is not something the compiler can fix. The best the compiler can do is to optimize the program to remove large objects in cases where they aren’t necessary.</p></li>
<li><p>Promptness - time from object being dead to it being freed</p></li>
<li><p>Throughput - time to execute program including memory management</p></li>
<li><p>Pause time - time spent in memory manager with all other threads locked</p></li>
</ul>
<section id="mutator">
<h3>Mutator<a class="headerlink" href="#mutator" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">src</span>&#160; <span class="pre">=</span> <span class="pre">New</span></code> - an explicit API in the language, adding to the set of ever-allocated objects <code class="docutils literal notranslate"><span class="pre">O</span></code> and allocated objects <code class="docutils literal notranslate"><span class="pre">A</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">val</span> <span class="pre">=</span> <span class="pre">Read</span> <span class="pre">src</span></code> - reading the value of a cell</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Write</span> <span class="pre">src</span> <span class="pre">val</span></code> - changing the value of a cell. The unpack function may also change but it’s a constant-sized function pointer so can be stored easily.</p></li>
<li><p>Roots - objects with easily accessible references</p></li>
<li><p>Live objects will be accessed after the current state, <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">in</span> <span class="pre">A</span> <span class="pre">and</span> <span class="pre">Access(s,z)</span> <span class="pre">=</span> <span class="pre">yes</span></code></p></li>
</ul>
</section>
<section id="collector">
<h3>Collector<a class="headerlink" href="#collector" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Deallocation/reclamantion - removing an object <code class="docutils literal notranslate"><span class="pre">o</span> <span class="pre">in</span> <span class="pre">O</span></code> from <code class="docutils literal notranslate"><span class="pre">A</span></code></p></li>
<li><p>A dead object is not live, <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">in</span> <span class="pre">O</span> <span class="pre">and</span> <span class="pre">(Access(s,z)</span> <span class="pre">=</span> <span class="pre">no</span> <span class="pre">or</span> <span class="pre">z</span> <span class="pre">notin</span> <span class="pre">A)</span></code>.</p></li>
<li><p>A freed object is in <code class="docutils literal notranslate"><span class="pre">O</span> <span class="pre">\</span> <span class="pre">A</span></code></p></li>
<li><p>Dead reachable objects are called cruft.</p></li>
<li><p>Unreachable but not freed objects are called floating garbage.</p></li>
<li><p>Mark-sweep: mark all reachable objects as live, free all unreachable objects</p></li>
</ul>
<div class="graphviz"><object data="../_images/graphviz-87f42a96c02da868ca5b2dca903459e147a18085.svg" type="image/svg+xml" class="graphviz">
<p class="warning">digraph G {
  black [fillcolor=black,fontcolor=white,style=filled,label=&quot;Presumed live&quot;]
  grey [fillcolor=grey22,fontcolor=white,style=filled,label=&quot;grey&quot;]
  white [label=&quot;Possibly dead&quot;]

  initial -&gt; white
  white -&gt; grey [label=&quot;mark push&quot;]
  grey -&gt; black [label=&quot;mark pop&quot;]
  white -&gt; dead [label=&quot;sweep&quot;]
  black -&gt; white [label=&quot;sweep&quot;]
}</p></object></div>
</section>
<section id="id12">
<h3>Allocator<a class="headerlink" href="#id12" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>allocate - reserves the underlying memory storage for an object</p></li>
<li><p>free - returns that storage to the allocator for subsequent re-use</p></li>
</ul>
<p>free list, buddy system, bump pointer, mmap/munmap</p>
<p>garbage collection</p>
<ul class="simple">
<li><p>pauses</p></li>
<li><p>bandwidth for tracing</p></li>
<li><p>design complexity</p></li>
<li><p>simple user code</p></li>
</ul>
<dl class="simple">
<dt>Root set</dt><dd><p>Any object references in the local variables and stack of any stack frame and any object references in the global object.</p>
</dd>
</dl>
<p>RC: count for reference</p>
<p>The count is changed when:</p>
<blockquote>
<div><p>When object first created it has one reference count
When any other variable is assigned a reference to that object, the object’s count is incremented.
When object reference does exit the current scope or assigned to the new value its reference count is decreased by one
when some object has zero reference count it is considered dead and object is instantly freed.
When an object is garbage collected, any objects that it refers to have their reference counts decremented.</p>
<p>does not detect cycles: two or more objects that refer to each other. An simple example of cycle in JS code:</p>
</div></blockquote>
<p>o = ref {} // count of object is 1
f := unpack o; // count of object is 2
o = null; // reference count of object is 1</p>
<p>mark &amp; sweep - reachable/unreachable objects</p>
<p>moving - move reachable object, updating all references to object</p>
<p>semi-space: objects are allocated in “to space” until it becomes full, then “to space” becomes the “from space”, and vice versa. reachable objects moved from the “from space” to the “to space”. new objects are once again allocated in the “to space” until it is once again full and the process is repeated.</p>
<p>requires 2x address space, lots of copying</p>
<p>Mark-compact: relocates reachable objects towards the beginning of the heap area. can be sliding, arbitrary, or optimize for locality</p>
<p>lazy sweep: when allocate memory and free list is empty, allocator
sweeps unsweeped chunk of memory.</p>
<p>generations: two or more sub-heaps, “generations” of objects. objects allocated to youngest, swept often. promoted to the next generation once sufficient sweep count. Each progressively older generation is swept less often than the next younger generation.</p>
<ol class="arabic simple">
<li><p>Write barrier: catch writes of new objects to already marked objects.</p></li>
</ol>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">function</span><span class="w"> </span><span class="n">writeBarrier</span><span class="p">(</span><span class="n">object</span><span class="p">,</span><span class="n">field</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="kr">if</span><span class="w"> </span><span class="p">(</span><span class="n">isMarked</span><span class="p">(</span><span class="n">object</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">isNotMarked</span><span class="p">(</span><span class="n">field</span><span class="p">))</span>
<span class="w">        </span><span class="n">gcMark</span><span class="p">(</span><span class="n">field</span><span class="p">);</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">mark</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">field</span>

<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Read barriers: Read barriers are used when collector is moving. They help to get correct reference to the object when collection is running:</p></li>
</ol>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">function</span><span class="w"> </span><span class="n">readBarrier</span><span class="p">(</span><span class="n">object</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="o">//</span><span class="w"> </span><span class="kr">if</span><span class="w"> </span><span class="n">gc</span><span class="w"> </span><span class="n">moved</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">return</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">it</span>
<span class="w">      </span><span class="kr">if</span><span class="w"> </span><span class="p">(</span><span class="n">moved</span><span class="p">(</span><span class="n">object</span><span class="p">))</span><span class="w"> </span><span class="n">return</span><span class="w"> </span><span class="n">newLocationOf</span><span class="p">(</span><span class="n">object</span><span class="p">);</span>
<span class="w">      </span><span class="n">return</span><span class="w"> </span><span class="n">object</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Concurrent/incremental GC:
interleave program and GC, GC on separate thread</p>
<p>write barriers or RC increases make every assigment with a heap object on the right hand side a bit more costly. In this case copying the live set can be faster. (related: Appel’s Garbage Collection Can Be Faster Than Stack Allocation)
but this introduces memory churn from allocation, and the dominant portion of the execution time is waiting for the cache lines to be loaded or pre-loaded.
You can actually see this exact behavior when profiling Java applications with high allocation rates, for example. You get weird stats that show that allocation is taking no significant time and GC is taking no significant time, but throughput still sucks. By eliminating the in-the-hot-loop allocations, you can see the throughput go up by a significant factor, sometimes by over an order of magnitude, because it avoids stalling.</p>
<p>The issue is latency for a single request and that can be as little as 150 clocks. In managed GC every allocation manipulates some internal data structure. In JVM it’s a bump of a top of “thread local allocation block” (TLAB) pointer. In cases of high allocation rates, it is very likely that this pointer will have been evicted thus forcing a round trip to main memory. The factor here is rate of allocations, as opposed to rate of bytes allocated which is reported by typical tools.</p>
<p>Measuring memory performance is tricky. The same workload on the same binary can give 2x changes in performance. It’s very sensitive to load order and memory layout. E.g. showing slides during a talk caused the JVM to load in a different memory segment, which changed the results of a timing sensitive calculation which in turn prevented the JVM from doing a memory adaptation that would drop CPU from 100% to 20% and improve latency dramatically.</p>
<p>Generational GC with a tiny live set can win on microbenchmarks, but real programs have large live sets that don’t fit in cache and the data will overflow the young generation before it dies. It’s been tried repeatedly at Sun and failed miserably. You’re better off with the largest young-gen you can get and sucking on the cache misses, OR doing the allocation “by hand” to force rapid (L1-cache-sized) reuse.</p>
<blockquote>
<div><p>Functions means making closures, new scopes, copy/pass parameters, do indirections on returns, memory allocations, etc</p>
</div></blockquote>
<p>In Rust, this: let x = Vec::with_capacity(10000); for i in 0..10000 {c.push(x)}
avoids resizing/reallocating the vec x because with_capacity specifies the size.
for a_iter_source.iter().collect() there is an optional .size_hint function on the iter that tells how many items it has</p>
<p>in practice asymptotics are BS, and performance depends strongly on memory management. Modeling memory access as O(1) is not correct, due to cache hierarchies - <span id="id13">[<a class="reference internal" href="../zzreferences.html#id87" title="Tomasz Jurkiewicz and Kurt Mehlhorn. The Cost of Address Translation. arXiv:1212.0703 [cs], April 2014. Comment: A extended abstract of this paper was published in the proceedings of ALENEX13, New Orleans, USA. URL: http://arxiv.org/abs/1212.0703 (visited on 2022-01-04), arXiv:1212.0703.">JM14</a>]</span> ends up with a log(n) overhead for random access, and similarly <a class="reference external" href="https://news.ycombinator.com/item?id=12388244">this thread</a> says it’s more like O(N^{1/3}) (3D memory architecture), until you near the Bekenstein bound at which point it’s O(N^{1/2}) by the holographic principle. Which of these approximations is right? Who knows, power law fitting is <a class="reference external" href="http://bactra.org//weblog/491.html">hard</a>, none of the articles does a convincing job with the empirical data. But generally, the point is that the effects of memory hierarchies can outweigh the asymptotics. Empirically trees are terrible for caches, indirect lookups hit memory hard.</p>
<p>For pointers, can optimize Maybe&lt;T&gt; to still fit into a pointer (null). Then converting T[] to Maybe&lt;T&gt;[] is a no-op.</p>
<p>The GC can use several pages of stack once it is triggered. It needs a separate stack. Similarly crawling the stack allocates on the stack. Again, use a separate stack, tighten up invariants, and add stack probes.</p>
<p>stack is reserved when the thread is created, and can be committed as well. It’s inadequate to only reserve because Windows has the unfortunate behavior that committing a page of stack can fail even if plenty of memory is available. If the swap file needs to be extended on disk, the attempt to commit can actually time out during this period, giving you a spurious fault.  If you want a robust application, you should always commit your stack reservations eagerly.</p>
<p>The end of the stack reservation consists of an unmapped page that’s a trap for runaway processes, a 1-page buffer for executing stack-overflow backout, and a normal page that generates stack overflow exception when it is allocated past, which will only have a few free bits in an SO condition. So you can really only rely on 1 page to handle SO, which is inadequate. Conclusion: reserve/commit an alternate stack at the beginning to handle SO conditions.</p>
<p>There is a guard bit set on all reserved but uncommitted stack pages.  The stack allocation/deallocation routines must touch/restore stack pages a page at a time, so that these uncommitted pages can be committed and de-committed in order.</p>
<p>getPointer is like C#’s <code class="docutils literal notranslate"><span class="pre">fixed</span></code> block but it allows interleaving. Pinned blocks remain where they are during GC, forcing GC generations to start at awkward locations and causing fragmentation. Pinned objects can be moved to a separate GC area before pinning but this could make fragmentation worse if the pin lifetimes are unpredictable. Efficient patterns are:
- pin for a time shorter than a GC cycle, then GC is unaffected
- pin an old object, then it can be stored statically or in a mature generation
- pin a bunch of objects as a group, then you can use an arena
- pin in a LIFO manner, then you can use a stack in an arena
- pin same-sized objects, then you can use a free list in an arena</p>
<p>A very inefficient pattern is to randomly allocate and pin a large number of randomly-sized objects.</p>
<p>I concluded after looking at it again that sharing parts of data structures should be pure, so my plan to use immutable references wasn’t going to work because allocating a reference would be impure. So instead there is an allocation interface.</p>
</section>
</section>
<section id="dumping-ground">
<h2>Dumping ground<a class="headerlink" href="#dumping-ground" title="Link to this heading"></a></h2>
<p>Copying, quad-color incremental, generational garbage collector
Arena-based bump allocator for heap-allocated values
Memory allocator API
Virtual memory API
* POSIX mmap+posix_madvise
* Windows VirtualAlloc</p>
<p>File and network APIs are generally managed by user-level code. So the point of the memory system is to assign a storage location for every value, insert moves / frees where necessary, and overall minimize the amount of resources consumed.</p>
<p>For more advanced programming there is the need to avoid the use of slow storage mechanisms as much as possible by addressing the fast storage mechanisms directly. (Really?)</p>
<p>Memory hierarchy - Place more commonly used items in faster locations - register/cache/memory/disk/recalculate. Items accessed closely together in time should be placed in related locations. Rematerialization recalculates a value instead of loading it from a slow location.</p>
<p>Higher order functions usually require some form of GC as the closures are allocated on the heap. But once you accept GC it is not too tricky, just perform closure conversion or lambda lifting (<a class="reference external" href="https://pp.ipd.kit.edu/uploads/publikationen/graf19sll.pdf">https://pp.ipd.kit.edu/uploads/publikationen/graf19sll.pdf</a>). There is room for optimization as many algorithms work.</p>
<p>Polymorphism requires a uniform representation for types (pointer/box), or templates like C++. Functional languages use a uniform representation so pay an overhead for accessing via the indirection. Unboxing analysis reduces this cost for primitive types - it works pretty well. GHC doesn’t have a particularly good data layout though, because it’s all uniform.</p>
<ul class="simple">
<li><p>Alias analysis - changing memory references into values</p></li>
<li><p>tail call optimization, Stack height reduction - stack optimizations</p></li>
<li><p>deforestation - remove data structure</p></li>
</ul>
<p>API for requesting memory in an async fashion - memory starvation is often the result of contention, so waiting could get you the memory.
* if you request more than is physically on the system, the request will fail immediately, because that much memory will never become available, barring hot-swapping RAM.
* the API would have to ignore swapping. Otherwise, excessive paging occurs before actual memory exhaustion. So the request succeeds but because it’s backed by the disk performance completely tanks and the user ends up killing the process.
Unfortunately, no OS APIs allow requesting memory asynchronously. The closest you get is Linux’s on-demand backing allocation.</p>
<p>Azul GC: <span id="id14">[<a class="reference internal" href="../zzreferences.html#id32" title="Cliff Click, Gil Tene, and Michael Wolf. The pauseless GC algorithm. In Proceedings of the 1st ACM/USENIX International Conference on Virtual Execution Environments - VEE '05, 46. Chicago, IL, USA, 2005. ACM Press. URL: https://static.usenix.org/events/vee05/full_papers/p46-click.pdf (visited on 2021-04-25), doi:10.1145/1064979.1064988.">CTW05</a>]</span> <span id="id15">[<a class="reference internal" href="../zzreferences.html#id161" title="Gil Tene, Balaji Iyengar, and Michael Wolf. C4: the continuously concurrent compacting collector. ACM SIGPLAN Notices, 46(11):79–88, June 2011. URL: https://doi.org/10.1145/2076022.1993491 (visited on 2022-06-08), doi:10.1145/2076022.1993491.">TIW11</a>]</span>
Shenandoah “low pause” is 10ms which is the same order of magnitude as NUMA memory map stuff (&gt;10ms if misconfigured)</p>
<p>cache misses are the most important performance metric for memory management, but not usually measured</p>
<p>escape detection - even with 70% of allocations on stack, not good enough to beat Azul
escape analysis - all or nothing
separate allocator has to be as fast as main allocator</p>
<p>compile time garbage collection is detecting when an allocation becomes unused and freeing it
structure reuse is detecting when an allocation becomes unused and reusing the memory for a new allocation
destructive assignment is when a memory address is passed in and modified by the function</p>
<p>Goroutines have little overhead beyond the memory for the stack, which is just a few kilobytes. Go’s run-time uses resizable, bounded stacks. A newly minted goroutine is given a few kilobytes, which is almost always enough. When it isn’t, the run-time grows (and shrinks) the memory for storing the stack automatically, allowing many goroutines to live in a modest amount of memory. The CPU overhead averages about three cheap instructions per function call. It is practical to create hundreds of thousands of goroutines in the same address space. If goroutines were just threads, system resources would run out at a much smaller number.</p>
<p>The programmer cannot generally predict whether a given code sample will allocate and hence potentially throw an OOM, because of implicit allocations. Here are some examples:</p>
<ul class="simple">
<li><p>Implicit boxing, causing value types to be instantiated on the heap.</p></li>
<li><p>marshaling and unmarshaling for the FFI</p></li>
<li><p>immutable array operations</p></li>
<li><p>graph reduction</p></li>
<li><p>JITing a method or basic block, generating VTables or trampolines</p></li>
</ul>
<p>But if the compiler is able to eliminate all allocations and hence eliminate the possibility of OOM, then these will most likely be consistently eliminated on every compile. So asserting that a function or block can’t OOM is possible. .NET had Constrained Execution Regions which implemented this, with various hacks such JITing the region at load time rather than when the region was first executed. So there’s precedent.</p>
<p>GCC (and later LLVM) added MemorySSA</p>
<p>A scratch buffer, as exemplified by GNU C’s <a class="reference external" href="https://www.gnu.org/software/libc/manual/html_node/Obstacks.html">obstack</a> seems to be an array variable plus metadata. They don’t require any special support AFAICT. But a stack regime is too restrictive.</p>
<p>The TelaMalloc paper seems like a good memory regime. Basically you have allocations (Start Clock Cycle, End Clock Cycle, Size in bytes) and the compiler has to be able to statically determine how to pack them into a memory space. TelaMalloc uses a limited space of size 𝑀, which is a good model for embedded. With a small limit like 8k, you can track the state of each byte and it’s not too much overhead. With a larger limit like the 2GB on video cards you will have to use a tree-like structure to track allocations. On desktop, the allocation limit is generally total physical RAM plus swap minus other running processes. 32-bit also has a limit of 2-3 GB due to virtual addressing.</p>
<p>Allocation randomization such as that found in DieHard or OpenBSD does an isolated allocation for each object at a random address. This allows probabilistically detecting bad pointer arithmetic and buffer overflows because they will access a guard page or fence-post before or after the isolated allocation. It also improves security because malicious code will have to determine the addresses of important objects in order to proceed with an exploit; with careful pointer structure the addresses will not be directly accessible and a brute-force search will have to be used. A “trap page” can detect this brute-force search and abort the program.</p>
<p>Control Flow Guard (CFG) builds a bit map table at compile time of all the legitimate target locations the code can jump too. During the execution of the application, whenever there is a call to jump to a new target location, CFG verifies that the new location was in the bit map table of valid jump locations. If the requested jump location is not listed in the “Valid Jump Locations”, Windows terminates the process thereby preventing the malware from executing.</p>
<p>Data Execution Prevention (DEP) marks various pages as non-executable, specifically the default heap, stack, and memory pools.</p>
<p>Structured Exception Handling Overwrite Protection (SEHOP): terminate process if SEH chain does not end with correct symbolic record. Also, the linker will produce a table of each image’s safe exception handlers, and the OS will check each SEH entry against these tables.</p>
<p>Address Space Layout Randomization (ASLR) mainly refers to randomizing the locations of executable and library images in memory, but can also randomize allocations.</p>
<p>DieHard segregates metadata from allocations by putting them in different address ranges. Most allocators store metadata adjacent to the allocation, which improves memory locality but makes it easy to corrupt allocation information.</p>
<p>Many allocators attempt to increase spatial locality by placing objects that are allocated at the same time near each other in memory [11, 14, 25, 40]. DieHard’s random allocation algorithm instead makes it likely that such objects will be distant. This spreading out of objects has little impact on L1 locality because typical heap objects are near or larger than the L1 cache line size (32 bytes on the x86). However, randomized allocation leads to a large number of TLB misses in one application (see Section 7.2.1), and leads to higher resident set sizes because it can induce poor page-level lo- cality. To maintain performance, the in-use portions of the DieHard heap should fit into physical RAM</p>
<p>DieHard’s complete randomization is key to provably avoiding a range of errors with high probability. It reduces the worst-case odds that a buffer overflow has any impact to 50%. The actual likelihood is even lower when the heap is not full. DieHard also avoids dangling pointer errors with very high probability (e.g., 99.999%), making it nearly impervious to such mistakes. You can read the PLDI paper for more details and formulae.</p>
<blockquote>
<div><p>On 64-bit the address space is so large that there is no need to worry about packing page allocations, you just allocate some number of pages at a random address and deal with packing things into that.</p>
<p>But it is possible to allocate pages at fixed addresses, so conceptually the randomization is just an efficient strategy for when allocations are sparse. The randomization does allow detecting allocation errors via unmapped page access, but Valgrind detects that too.</p>
</div></blockquote>
<p>Program memory usage can be segmented into several types:</p>
<ul class="simple">
<li><p>frequently accessed data, where reducing the size increases cache performance but they cannot be freed</p></li>
<li><p>infrequently accessed data, where they can be paged to disk but space optimizations don’t have much use. It may be better to free the pages and recreate them when needed.</p></li>
<li><p>dead data, which will be determined to not be needed by a traversal and eventually freed</p></li>
<li><p>leaks, data which has been allocated but will never be used</p></li>
</ul>
<p>Memory usage affects various measurements: paging, cache misses, OOM aborts, and the physical and virtual memory consumption reported in Task Manager.</p>
<p>Basically you track at each program point what is allocated. Then for each allocation you try to find an unused space of sufficient size - this is where the solver comes in because there are constraints. For phi nodes you just combine allocations, so the state is whatever is allocated in either. And for recursion you require the starting and ending allocations to be identical, or warn about unbounded memory usage.
The UI is essentially the compiler says “ok, I know how to compile your program”, or else it errors with an allocation that the compiler can’t figure out how to fit.
there is also the end time… somehow you have to propagate the information of how long a block lives back to the allocation point, so that the allocator can make a good decision. I guess it’s your standard backwards dataflow pass. The hard part is figuring out a good representation of “cycle time” that can handle loops and stuff</p>
<p>well you’ve very helpful so far with the references. let me summarize my position to wrap this up:
* Most programmers do not know or care about atomics at all. If they do use atomics, they will most likely use whatever guarantees sequential consistency. Per the volatile-by-default papers, with Hotspot’s standard barrier coalescing and read caching optimizations, this has a geometric mean overhead of around 50%, with a maximum observed overhead of 167%. For many purposes this magnitude is not significant enough to matter; it’s about the hit you get moving from C++ to Java. And the relaxed annotations significantly reduced the max overhead from 81% to 34%. The fences needed for SC are well-understood and don’t have many optimizations.
* If someone needs performance, most likely they are already at the level where they have picked a platform to optimize for and are looking at the assembly. So the most natural memory model is the hardware’s. These models are fully specified, well-tested to match actual hardware, and have even (in most cases) been signed off by the hardware manufacturers. Using the hardware memory model does mean the programmer has to learn about the different fences and decide which one to use, but I don’t think this task can be avoided if it’s really important to optimize a concurrent program. Implementing compiler optimizations based on a hardware MM seems doable.
* Lastly there are the “portable” C++ and Java memory models (and maybe others like LLVM). These allow execution behavior / optimizations not possible on certain platforms, and maybe even “thin air” behavior of execution not possible on any supported platform. In the thin air case it seems it’s a bug, but apparently in the x86 vs ARM case it’s intended behavior. To avoid confusion, my strategy is to have a literal assembly translation for each atomic operation, so that the behavior is maybe not the fastest but is predictable and consistent.</p>
<p>memory access optimizations: (unconditionally safe only for unshared RAM, but may be allowed depending on memory model and access patterns)
- alias analysis
- reorder loads and stores
- narrowing a store into a bitfield
- rematerializing a load
- Dead Store Elimination (per <span id="id16">[]</span> need to have “secret variables” that can be cleared using “scrubbing stores” that don’t get DSE’d)
- turning loads and stores into a memcpy call
- introducing loads and stores along a codepath where they would not otherwise exist</p>
<p>Counting Immutable Beans: Reference Counting Optimized for Purely Functional Programming
<a class="reference external" href="https://arxiv.org/abs/1908.05647">https://arxiv.org/abs/1908.05647</a>
Perceus: Garbage Free Reference Counting with Reuse
<a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2020/11/perceus-tr-v1.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2020/11/perceus-tr-v1.pdf</a>
Perceus was inspired by the Counting Immutable Beans paper.
Both system uses reference counting with static analysis to find reusable memory cells.
Perceus algorithm is used in the Koka language <a class="reference external" href="https://github.com/koka-lang/koka">https://github.com/koka-lang/koka</a>
ASAP (As Static As Possible) is a compile-time automatic memory management system using whole program analysis.
<a class="reference external" href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.pdf">https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.pdf</a>
Micro-mitten: ASAP implementation for a simple Rust like language. Some promising results but his implementation was quite flawed so overall results were negative.
<a class="reference external" href="https://github.com/doctorn/micro-mitten">https://github.com/doctorn/micro-mitten</a>
ASAP is not ready for production use, it needs more research.
But it fills a hole in the memory management design space.</p>
<p>There is no concise built-in syntax for dereferencing pointers, because there are many different flavours of memory accesses: aligned or unaligned, cached or uncached, secure or non-secure, etc. and it is critical that every memory access is explicit about its flavor. A side effect of putting more information in the access operation is that pointers are untyped, simply a wrapper around bitvectors.</p>
<p>The ‘load’ and ‘store’ instructions are specifically crafted to fully resolve to an element of a memref. These instructions take as arguments n+1 indices for an n-ranked tensor. This disallows the equivalent of pointer arithmetic or the ability to index into the same memref in other ways (something which C arrays allow for example). Furthermore, for the affine constructs, the compiler can follow use-def chains (e.g. through <a class="reference external" href="../Dialects/Affine.md/#affineapply-affineapplyop">affine.apply operations</a> or through the map attributes of <a class="reference external" href="../Dialects/Affine.md/#operations">affine operations</a>) to precisely analyze references at compile-time using polyhedral techniques. This is possible because of the <a class="reference external" href="../Dialects/Affine.md/#restrictions-on-dimensions-and-symbols">restrictions on dimensions and symbols</a>.</p>
<p>A scalar of element-type (a primitive type or a vector type) that is stored in memory is modeled as a 0-d memref. This is also necessary for scalars that are live out of for loops and if conditionals in a function, for which we don’t yet have an SSA representation – <a class="reference external" href="#affineif-and-affinefor-extensions-for-escaping-scalars">an extension</a> to allow that is described later in this doc.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Macros.html" class="btn btn-neutral float-left" title="Macros" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Meta.html" class="btn btn-neutral float-right" title="Meta" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2022 Mathnerd314.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>