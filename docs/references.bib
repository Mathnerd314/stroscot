
@article{abramskyGeometryInteractionLinear2002,
  title = {Geometry of {{Interaction}} and {{Linear Combinatory Algebras}}.},
  author = {Abramsky, Samson and Haghverdi, Esfandiar and Scott, Philip},
  year = {2002},
  month = oct,
  volume = {12},
  pages = {625--665},
  doi = {10.1017/S0960129502003730},
  url = {https://www.researchgate.net/profile/Samson_Abramsky/publication/220173613_Geometry_of_Interaction_and_Linear_Combinatory_Algebras/links/0c96052560eec33e21000000/Geometry-of-Interaction-and-Linear-Combinatory-Algebras.pdf},
  abstract = {this paper was quite di\#erent, stemming from the axiomatics of categories of tangles (although the authors were aware of possible connections to iteration theories. In fact, similar axiomatics in the symmetric case, motivated by flowcharts and "flownomials" had been developed some years earlier by Stefanescu (Stefanescu 2000).) However, the first author realized, following a stimulating discussion with Gordon Plotkin, that traced monoidal categories provided a common denominator for the axiomatics of both the Girard-style and Abramsky-Jagadeesan-style versions of the Geometry of Interaction, at the basic level of the multiplicatives. This insight was presented in (Abramsky 1996), in which Girard-style GoI was dubbed "particle-style", since it concerns information particles or tokens flowing around a network, while the Abramsky-Jagadeesan style GoI was dubbed "wave-style", since it concerns the evolution of a global information state or "wave". Formally, this distinction is based on whether the tensor product (i.e. the symmetric monoidal structure) in the underlying category is interpreted as a coproduct (particle style) or as a product (wave style). This computational distinction between coproduct and product interpretations of the same underlying network geometry turned out to have been partially anticipated, in a rather di\#erent context, in a pioneering paper by E. S. Bainbridge (Bainbridge 1976), as observed by Dusko Pavlovic. These two forms of interpretation, and ways of combining them, have also been studied recently in (Stefanescu 2000). He uses the terminology "additive" for coproduct-based (i.e. our "particle-style") and "multiplicative" for product-based (i.e. our "wave-style"); this is not suitable for our purposes, because of the clash with Linear Logic term...},
  journal = {Mathematical Structures in Computer Science}
}
% == BibTeX quality report for abramskyGeometryInteractionLinear2002:
% ? Title looks like it was stored in title-case in Zotero

@article{albertResourceAnalysisDriven2019,
  title = {Resource {{Analysis}} Driven by ({{Conditional}}) {{Termination Proofs}}},
  author = {Albert, Elvira and Bofill, Miquel and Borralleras, Cristina and {Martin-Martin}, Enrique and Rubio, Albert},
  year = {2019},
  month = sep,
  volume = {19},
  pages = {722--739},
  issn = {1471-0684, 1475-3081},
  doi = {10.1017/S1471068419000152},
  url = {http://arxiv.org/abs/1907.10096},
  urldate = {2020-06-22},
  abstract = {When programs feature a complex control flow, existing techniques for resource analysis produce cost relation systems (CRS) whose cost functions retain the complex flow of the program and, consequently, might not be solvable into closed-form upper bounds. This paper presents a novel approach to resource analysis that is driven by the result of a termination analysis. The fundamental idea is that the termination proof encapsulates the flows of the program which are relevant for the cost computation so that, by driving the generation of the CRS using the termination proof, we produce a linearly-bounded CRS (LB-CRS). A LB-CRS is composed of cost functions that are guaranteed to be locally bounded by linear ranking functions and thus greatly simplify the process of CRS solving. We have built a new resource analysis tool, named MaxCore, that is guided by the VeryMax termination analyzer and uses CoFloCo and PUBS as CRS solvers. Our experimental results on the set of benchmarks from the Complexity and Termination Competition 2019 for C Integer programs show that MaxCore outperforms all other resource analysis tools. Under consideration for acceptance in TPLP.},
  archivePrefix = {arXiv},
  eprint = {1907.10096},
  eprinttype = {arxiv},
  journal = {Theory and Practice of Logic Programming},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  number = {5-6}
}

@article{allenCatalogueOptimizingTransformations1971,
  title = {A {{Catalogue}} of {{Optimizing Transformations}}},
  author = {Allen, Frances E and Cocke, John},
  year = {1971},
  pages = {30},
  url = {https://www.clear.rice.edu/comp512/Lectures/Papers/1971-allen-catalog.pdf},
  journal = {IBM Research Center},
  language = {en}
}
% == BibTeX quality report for allenCatalogueOptimizingTransformations1971:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{amorimDeclarativeSpecificationIndentation2018,
  ids = {amorimDeclarativeSpecificationIndentation2018a},
  title = {Declarative Specification of Indentation Rules: A Tooling Perspective on Parsing and Pretty-Printing Layout-Sensitive Languages},
  shorttitle = {Declarative Specification of Indentation Rules},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN International Conference}} on {{Software Language Engineering}}  - {{SLE}} 2018},
  author = {Amorim, Luís Eduardo de Souza and Steindorfer, Michael J. and Erdweg, Sebastian and Visser, Eelco},
  year = {2018},
  pages = {3--15},
  publisher = {{ACM Press}},
  address = {{Boston, MA, USA}},
  doi = {10.1145/3276604.3276607},
  url = {http://udesou.info/wp-content/uploads/2018/10/layout-pp.pdf},
  urldate = {2020-06-15},
  abstract = {In layout-sensitive languages, the indentation of an expression or statement can influence how a program is parsed. While some of these languages (e.g., Haskell and Python) have been widely adopted, there is little support for software language engineers in building tools for layout-sensitive languages. As a result, parsers, pretty-printers, program analyses, and refactoring tools often need to be handwritten, which decreases the maintainability and extensibility of these tools. Even state-of-the-art language workbenches have little support for layout-sensitive languages, restricting the development and prototyping of such languages. In this paper, we introduce a novel approach to declarative specification of layout-sensitive languages using layout declarations. Layout declarations are high-level specifications of indentation rules that abstract from low-level technicalities. We show how to derive an efficient layout-sensitive generalized parser and a corresponding pretty-printer automatically from a language specification with layout declarations. We validate our approach in a case-study using a syntax definition for the Haskell programming language, investigating the performance of the generated parser and the correctness of the generated pretty-printer against 22191 Haskell files.},
  isbn = {978-1-4503-6029-6},
  language = {en}
}

@inproceedings{ananthanarayananKeepingMasterGreen2019,
  title = {Keeping {{Master Green}} at {{Scale}}},
  booktitle = {Proceedings of the {{Fourteenth EuroSys Conference}} 2019},
  author = {Ananthanarayanan, Sundaram and Ardekani, Masoud Saeida and Haenikel, Denis and Varadarajan, Balaji and Soriano, Simon and Patel, Dhaval and {Adl-Tabatabai}, Ali-Reza},
  year = {2019},
  month = mar,
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{Dresden, Germany}},
  doi = {10.1145/3302424.3303970},
  url = {https://doi.org/10.1145/3302424.3303970},
  urldate = {2020-07-06},
  abstract = {Giant monolithic source-code repositories are one of the fundamental pillars of the back end infrastructure in large and fast-paced software companies. The sheer volume of everyday code changes demands a reliable and efficient change management system with three uncompromisable key requirements --- always green master, high throughput, and low commit turnaround time. Green refers to a master branch that always successfully compiles and passes all build steps, the opposite being red. A broken master (red) leads to delayed feature rollouts because a faulty code commit needs to be detected and rolled backed. Additionally, a red master has a cascading effect that hampers developer productivity--- developers might face local test/build failures, or might end up working on a codebase that will eventually be rolled back. This paper presents the design and implementation of SubmitQueue. It guarantees an always green master branch at scale: all build steps (e.g., compilation, unit tests, UI tests) successfully execute for every commit point. SubmitQueue has been in production for over a year, and can scale to thousands of daily commits to giant monolithic repositories.},
  isbn = {978-1-4503-6281-8},
  series = {{{EuroSys}} '19}
}
% == BibTeX quality report for ananthanarayananKeepingMasterGreen2019:
% ? Title looks like it was stored in title-case in Zotero

@article{appelGarbageCollectionCan1987,
  title = {Garbage Collection Can Be Faster than Stack Allocation},
  author = {Appel, Andrew W.},
  year = {1987},
  month = jun,
  volume = {25},
  pages = {275--279},
  issn = {00200190},
  doi = {10.1016/0020-0190(87)90175-X},
  url = {https://www.cs.princeton.edu/~appel/papers/45.pdf},
  urldate = {2020-07-24},
  abstract = {A very old and simple algorithm for garbage collection gives very good results when the physical memory is much larger than the number of reachable cells. In fact, the overhead associated with allocating and collecting cells from the heap can be reduced to less than one instruction per cell by increasing the size of physical memory. Special hardware, intricate garbage-collection algorithms, and fancy compiler analysis become unnecessary.},
  journal = {Information Processing Letters},
  language = {en},
  number = {4}
}

@book{aspertiOptimalImplementationFunctional1999,
  title = {The Optimal Implementation of Functional Programming Languages},
  author = {Asperti, Andrea and Guerrini, Stefano},
  year = {1999},
  month = jan,
  edition = {1st},
  publisher = {{Cambridge University Press}},
  address = {{USA}},
  abstract = {All traditional implementation techniques for functional languages fail to avoid useless repetition of work. They are not "optimal" in their implementation of sharing, often causing a catastrophic, exponential explosion in reduction time. Optimal reduction is an innovative graph reduction technique for functional expressions, introduced by Lamping in 1990, that solves the sharing problem. This work, the first on the subject, is a comprehensive account by two of its leading exponents. Practical implementation aspects are fully covered as are the mathematical underpinnings of the subject. The relationship to the pioneering work of L\&\#233;vy and to Girard's more recent "Geometry of Interaction" are explored; optimal reduction is thereby revealed as a prime example of how a beautiful mathematical theory can lead to practical benefit. The book is essentially self-contained, requiring no more than basic familiarity with functional languages. It will be welcomed by graduate students and research workers in lambda calculus, functional programming or linear logic.},
  isbn = {978-0-521-62112-0},
  number = {45},
  series = {Cambridge {{Tracts}} in {{Theoretical Computer Science}}}
}

@inproceedings{bhatotiaIThreadsThreadingLibrary2015,
  title = {{{iThreads}}: {{A Threading Library}} for {{Parallel Incremental Computation}}},
  shorttitle = {{{iThreads}}},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}} - {{ASPLOS}} '15},
  author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Björn B. and Rodrigues, Rodrigo},
  year = {2015},
  pages = {645--659},
  publisher = {{ACM Press}},
  address = {{Istanbul, Turkey}},
  doi = {10.1145/2694344.2694371},
  url = {https://www.cs.purdue.edu/homes/pfonseca/papers/asplos2015-ithreads.pdf},
  urldate = {2020-10-25},
  isbn = {978-1-4503-2835-7},
  language = {en}
}

@article{brausseCDCLstyleCalculusSolving2019,
  title = {A {{CDCL}}-Style Calculus for Solving Non-Linear Constraints},
  author = {Brauße, Franz and Korovin, Konstantin and Korovina, Margarita and Müller, Norbert Th},
  year = {2019},
  month = jul,
  url = {http://arxiv.org/abs/1905.09227},
  urldate = {2020-07-25},
  abstract = {In this paper we propose a novel approach for checking satisfiability of non-linear constraints over the reals, called ksmt. The procedure is based on conflict resolution in CDCL-style calculus, using a composition of symbolical and numerical methods. To deal with the nonlinear components in case of conflicts we use numerically constructed restricted linearisations. This approach covers a large number of computable non-linear real functions such as polynomials, rational or trigonometrical functions and beyond. A prototypical implementation has been evaluated on several non-linear SMT-LIB examples and the results have been compared with state-of-the-art SMT solvers.},
  archivePrefix = {arXiv},
  eprint = {1905.09227},
  eprinttype = {arxiv},
  journal = {arXiv:1905.09227 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  language = {en},
  primaryClass = {cs}
}
% == BibTeX quality report for brausseCDCLstyleCalculusSolving2019:
% ? Possibly abbreviated journal title arXiv:1905.09227 [cs]

@article{castagnaCovarianceControvarianceFresh2020,
  title = {Covariance and {{Controvariance}}: A Fresh Look at an Old Issue (a Primer in Advanced Type Systems for Learning Functional Programmers)},
  shorttitle = {Covariance and {{Controvariance}}},
  author = {Castagna, Giuseppe},
  year = {2020},
  month = feb,
  volume = {16},
  doi = {10.23638/LMCS-16(1:15)2020},
  url = {http://arxiv.org/abs/1809.01427},
  urldate = {2020-06-22},
  abstract = {Twenty years ago, in an article titled "Covariance and contravariance: conflict without a cause", I argued that covariant and contravariant specialization of method parameters in object-oriented programming had different purposes and deduced that, not only they could, but actually they should both coexist in the same language. In this work I reexamine the result of that article in the light of recent advances in (sub-)typing theory and programming languages, taking a fresh look at this old issue. Actually, the revamping of this problem is just an excuse for writing an essay that aims at explaining sophisticated type-theoretic concepts, in simple terms and by examples, to undergraduate computer science students and/or willing functional programmers. Finally, I took advantage of this opportunity to describe some undocumented advanced techniques of type-systems implementation that are known only to few insiders that dug in the code of some compilers: therefore, even expert language designers and implementers may find this work worth of reading.},
  archivePrefix = {arXiv},
  eprint = {1809.01427},
  eprinttype = {arxiv},
  journal = {arXiv:1809.01427 [cs]},
  keywords = {Computer Science - Programming Languages},
  number = {1},
  primaryClass = {cs}
}
% == BibTeX quality report for castagnaCovarianceControvarianceFresh2020:
% ? Possibly abbreviated journal title arXiv:1809.01427 [cs]

@phdthesis{corbynPracticalStaticMemory2020,
  title = {Practical Static Memory Management},
  author = {Corbyn, Nathan},
  year = {2020},
  month = may,
  url = {http://nathancorbyn.com/nc513.pdf},
  language = {en},
  school = {King’s College},
  type = {Bachelor's Thesis}
}

@misc{coxVersionSAT2016,
  title = {Version {{SAT}}},
  author = {Cox, Russ},
  year = {2016},
  month = dec,
  url = {https://research.swtch.com/version-sat},
  urldate = {2021-01-26},
  journal = {research!rsc}
}
% == BibTeX quality report for coxVersionSAT2016:
% ? Title looks like it was stored in title-case in Zotero

@article{crolardFormulaeasTypesInterpretationSubtractive2004,
  title = {A {{Formulae}}-as-{{Types Interpretation}} of {{Subtractive Logic}}},
  author = {Crolard, Tristan},
  year = {2004},
  month = aug,
  volume = {14},
  pages = {529--570},
  publisher = {{Oxford Academic}},
  issn = {0955-792X},
  doi = {10.1093/logcom/14.4.529},
  url = {https://academic.oup.com/logcom/article/14/4/529/933555},
  urldate = {2020-06-18},
  abstract = {Abstract.  We present a formulae-as-types interpretation of Subtractive Logic (i.e. bi-intuitionistic logic). This presentation is two-fold: we first define a v},
  journal = {Journal of Logic and Computation},
  language = {en},
  number = {4}
}
% == BibTeX quality report for crolardFormulaeasTypesInterpretationSubtractive2004:
% ? Title looks like it was stored in title-case in Zotero

@article{delawareNarcissusCorrectbyconstructionDerivation2019,
  title = {Narcissus: Correct-by-Construction Derivation of Decoders and Encoders from Binary Formats},
  shorttitle = {Narcissus},
  author = {Delaware, Benjamin and Suriyakarn, Sorawit and {Pit-Claudel}, Clément and Ye, Qianchuan and Chlipala, Adam},
  year = {2019},
  month = jul,
  volume = {3},
  pages = {1--29},
  issn = {2475-1421, 2475-1421},
  doi = {10.1145/3341686},
  url = {https://www.cs.purdue.edu/homes/bendy/Narcissus/narcissus.pdf},
  urldate = {2020-07-26},
  journal = {Proceedings of the ACM on Programming Languages},
  language = {en},
  number = {ICFP}
}

@inproceedings{downenMakingFasterCurry2019,
  title = {Making a Faster {{Curry}} with Extensional Types},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Symposium}} on {{Haskell}}},
  author = {Downen, Paul and Sullivan, Zachary and Ariola, Zena M. and Peyton Jones, Simon},
  year = {2019},
  month = aug,
  pages = {58--70},
  publisher = {{Association for Computing Machinery}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3331545.3342594},
  url = {https://doi.org/10.1145/3331545.3342594},
  urldate = {2020-06-14},
  abstract = {Curried functions apparently take one argument at a time, which is slow. So optimizing compilers for higher-order languages invariably have some mechanism for working around currying by passing several arguments at once, as many as the function can handle, which is known as its arity. But such mechanisms are often ad-hoc, and do not work at all in higher-order functions. We show how extensional, call-by-name functions have the correct behavior for directly expressing the arity of curried functions. And these extensional functions can stand side-by-side with functions native to practical programming languages, which do not use call-by-name evaluation. Integrating call-by-name with other evaluation strategies in the same intermediate language expresses the arity of a function in its type and gives a principled and compositional account of multi-argument curried functions. An unexpected, but significant, bonus is that our approach is equally suitable for a call-by-value language and a call-by-need language, and it can be readily integrated into an existing compilation framework.},
  isbn = {978-1-4503-6813-1},
  keywords = {arity,extensionality,type systems},
  series = {Haskell 2019}
}

@inproceedings{downenSequentCalculusCompiler2016a,
  ids = {downenSequentCalculusCompiler,downenSequentCalculusCompiler2016},
  title = {Sequent Calculus as a Compiler Intermediate Language},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Downen, Paul and Maurer, Luke and Ariola, Zena M. and Peyton Jones, Simon},
  year = {2016},
  month = sep,
  pages = {74--88},
  publisher = {{Association for Computing Machinery}},
  address = {{Nara, Japan}},
  doi = {10.1145/2951913.2951931},
  url = {https://www.microsoft.com/en-us/research/publication/sequent-calculus-as-a-compiler-intermediate-language/},
  urldate = {2020-06-14},
  abstract = {The λ-calculus is popular as an intermediate language for practical compilers. But in the world of logic it has a lesser-known twin, born at the same time, called the sequent calculus. Perhaps that would make for a good intermediate language, too? To explore this question we designed Sequent Core, a practically-oriented core calculus based on the sequent calculus, and used it to re-implement a substantial chunk of the Glasgow Haskell Compiler.},
  isbn = {978-1-4503-4219-3},
  keywords = {Compiler optimizations,Continuations,Haskell,Intermediate representations,Natural deduction,Sequent calculus},
  series = {{{ICFP}} 2016}
}

@inproceedings{dsilvaConflictDrivenConditionalTermination2015,
  title = {Conflict-{{Driven Conditional Termination}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {D’Silva, Vijay and Urban, Caterina},
  editor = {Kroening, Daniel and Păsăreanu, Corina S.},
  year = {2015},
  pages = {271--286},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-21668-3_16},
  url = {http://homepages.inf.ed.ac.uk/wadler/papers/dual/dual.pdf},
  abstract = {Conflict-driven learning, which is essential to the performance of sat and smt solvers, consists of a procedure that searches for a model of a formula, and refutation procedure for proving that no model exists. This paper shows that conflict-driven learning can improve the precision of a termination analysis based on abstract interpretation. We encode non-termination as satisfiability in a monadic second-order logic and use abstract interpreters to reason about the satisfiability of this formula. Our search procedure combines decisions with reachability analysis to find potentially non-terminating executions and our refutation procedure uses a conditional termination analysis. Our implementation extends the set of conditional termination arguments discovered by an existing termination analyzer.},
  isbn = {978-3-319-21668-3},
  keywords = {Abstract Domain,Conflict Analysis,Ranking Function,Reachability Analysis,Trace Formula},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}
% == BibTeX quality report for dsilvaConflictDrivenConditionalTermination2015:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{dunfieldBidirectionalTyping2019,
  title = {Bidirectional {{Typing}}},
  author = {Dunfield, Joshua and Krishnaswami, Neel},
  year = {2019},
  month = aug,
  url = {http://arxiv.org/abs/1908.05839},
  urldate = {2020-06-22},
  abstract = {Bidirectional typing combines two modes of typing: type checking, which checks that a program satisfies a known type, and type synthesis, which determines a type from the program. Using checking enables bidirectional typing to break the decidability barrier of Damas-Milner approaches; using synthesis enables bidirectional typing to avoid the large annotation burden of explicitly typed languages. In addition, bidirectional typing improves error locality. We highlight the design principles that underlie bidirectional type systems, survey the development of bidirectional typing from the prehistoric period before Pierce and Turner's local type inference to the present day, and provide guidance for future investigations.},
  archivePrefix = {arXiv},
  eprint = {1908.05839},
  eprinttype = {arxiv},
  journal = {arXiv:1908.05839 [cs]},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  primaryClass = {cs}
}
% == BibTeX quality report for dunfieldBidirectionalTyping2019:
% ? Possibly abbreviated journal title arXiv:1908.05839 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{dyvbigMonadicFrameworkDelimited2007,
  ids = {dyvbigMonadicFrameworkDelimited2007a},
  title = {A Monadic Framework for Delimited Continuations},
  author = {Dyvbig, R. Kent and Peyton Jones, Simon and Sabry, Amr},
  year = {2007},
  month = nov,
  volume = {17},
  pages = {687--730},
  issn = {0956-7968},
  doi = {10.1017/S0956796807006259},
  url = {https://doi.org/10.1017/S0956796807006259},
  urldate = {2020-06-19},
  abstract = {Delimited continuations are more expressive than traditional abortive continuations and they apparently require a framework beyond traditional continuation-passing style (CPS). We show that this is not the case: standard CPS is sufficient to explain the common control operators for delimited continuations. We demonstrate this fact and present an implementation as a Scheme library. We then investigate a typed account of delimited continuations that makes explicit where control effects can occur. This results in a monadic framework for typed and encapsulated delimited continuations, which we design and implement as a Haskell library.},
  journal = {Journal of Functional Programming},
  number = {6}
}

@incollection{erdwegLayoutSensitiveGeneralizedParsing2013,
  title = {Layout-{{Sensitive Generalized Parsing}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Erdweg, Sebastian and Rendel, Tillmann and Kästner, Christian and Ostermann, Klaus},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Czarnecki, Krzysztof and Hedin, Görel},
  year = {2013},
  volume = {7745},
  pages = {244--263},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-36089-3_14},
  url = {http://link.springer.com/10.1007/978-3-642-36089-3_14},
  urldate = {2020-06-15},
  abstract = {The theory of context-free languages is well-understood and context-free parsers can be used as off-the-shelf tools in practice. In particular, to use a context-free parser framework, a user does not need to understand its internals but can specify a language declaratively as a grammar. However, many languages in practice are not context-free. One particularly important class of such languages is layout-sensitive languages, in which the structure of code depends on indentation and whitespace. For example, Python, Haskell, F\#, and Markdown use indentation instead of curly braces to determine the block structure of code. Their parsers (and lexers) are not declaratively specified but hand-tuned to account for layout-sensitivity.},
  isbn = {978-3-642-36088-6 978-3-642-36089-3},
  language = {en}
}
% == BibTeX quality report for erdwegLayoutSensitiveGeneralizedParsing2013:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{erdwegSoundOptimalIncremental2015,
  title = {A Sound and Optimal Incremental Build System with Dynamic Dependencies},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}} - {{OOPSLA}} 2015},
  author = {Erdweg, Sebastian and Lichter, Moritz and Weiel, Manuel},
  year = {2015},
  pages = {89--106},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, PA, USA}},
  doi = {10.1145/2814270.2814316},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.725.6063&rep=rep1&type=pdf},
  urldate = {2020-06-15},
  abstract = {Build systems are used in all but the smallest software projects to invoke the right build tools on the right files in the right order. A build system must be sound (after a build, generated files consistently reflect the latest source files) and efficient (recheck and rebuild as few build units as possible). Contemporary build systems provide limited efficiency because they lack support for expressing finegrained file dependencies. We present a build system called pluto that supports the definition of reusable, parameterized, interconnected builders. When run, a builder notifies the build system about dynamically required and produced files as well as about other builders whose results are needed. To support fine-grained file dependencies, we generalize the traditional notion of time stamps to allow builders to declare their actual requirements on a file’s content. pluto collects the requirements and products of a builder with their stamps in a build summary. This enables pluto to provides provably sound and optimal incremental rebuilding. To support dynamic dependencies, our rebuild algorithm interleaves dependency analysis and builder execution and enforces invariants on the dependency graph through a dynamic analysis. We have developed pluto as a Java API and used it to implement more than 25 builders. We describe our experience with migrating a larger Ant build script to pluto and compare the respective build times.},
  isbn = {978-1-4503-3689-5},
  language = {en}
}

@inproceedings{filinskiLinearContinuations1992,
  title = {Linear Continuations},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}}-{{SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Filinski, Andrzej},
  year = {1992},
  month = feb,
  pages = {27--38},
  publisher = {{Association for Computing Machinery}},
  address = {{Albuquerque, New Mexico, USA}},
  doi = {10.1145/143165.143174},
  url = {https://doi.org/10.1145/143165.143174},
  urldate = {2020-06-19},
  abstract = {We present a functional interpretation of classical linear logic based on the concept of linear continuations. Unlike their non-linear counterparts, such continuations lead to a model of control that does not inherently impose any particular evaluation strategy. Instead, such additional structure is expressed by admitting closely controlled copying and discarding of continuations. We also emphasize the importance of classicality in obtaining computationally appealing categorical models of linear logic and propose a simple “coreflective subcategory” interpretation of the modality “!”.},
  isbn = {978-0-89791-453-6},
  series = {{{POPL}} '92}
}
% == BibTeX quality report for filinskiLinearContinuations1992:
% ? Unsure about the formatting of the booktitle

@misc{gravgaardElasticTabstopsBetter,
  title = {Elastic Tabstops - a Better Way to Indent and Align Code},
  author = {Gravgaard, Nick},
  url = {https://nickgravgaard.com/elastic-tabstops/},
  urldate = {2021-02-13},
  abstract = {Elastic tabstops - a better way to indent and align code},
  language = {en}
}

@phdthesis{guerriniTheoreticalPracticalIssues1996,
  title = {Theoretical and Practical Issues of Optimal Implementations of Functional Languages},
  author = {Guerrini, Stefano},
  year = {1996},
  url = {https://www-lipn.univ-paris13.fr/~guerrini/mysite/sites/default/files/biblio/PhDThesis.pdf},
  school = {Università di Pisa. Dipartimento di Informatica}
}

@book{holmesElementarySetTheory1998,
  title = {Elementary {{Set Theory}} with a {{Universal Set}}},
  author = {Holmes, M. Randall},
  year = {1998},
  publisher = {{Bruylant-Academia}},
  url = {https://randall-holmes.github.io/head.pdf},
  googlebooks = {\_vjuAAAAMAAJ},
  isbn = {978-2-87209-488-2},
  language = {en}
}
% == BibTeX quality report for holmesElementarySetTheory1998:
% ? Title looks like it was stored in title-case in Zotero

@misc{jakobsDifferentialModularSoftware,
  title = {Differential {{Modular Software Verification}}},
  author = {Jakobs, Marie-Christine},
  url = {https://www.sosy-lab.org/research/prs/2019-10-01-CPA19-Differntial-Verification.pdf},
  urldate = {2020-07-25}
}
% == BibTeX quality report for jakobsDifferentialModularSoftware:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{jonesTacklingAwkwardSquad2001,
  ids = {jonesTacklingAwkwardSquad2001a},
  title = {Tackling the Awkward Squad: Monadic Input/Output, Concurrency, Exceptions, and Foreign-Language Calls in {{Haskell}}},
  booktitle = {Engineering Theories of Software Construction},
  author = {Jones, Simon Peyton},
  year = {2001},
  pages = {47--96},
  url = {https://www.microsoft.com/en-us/research/publication/tackling-awkward-squad-monadic-inputoutput-concurrency-exceptions-foreign-language-calls-haskell/},
  abstract = {I’ve revised the notes significantly, with the help of feedback from many people. Last update: 21 Feb 2001. PowerPoint slides Writing High-Performance Server Applications in Haskell, Case Study: A Haskell Web Server, Simon Marlow, Haskell Workshop, Montreal, Canada, Sept 2000. This paper describes the running example in the notes. ~ This tutorial focuses on explaining […]},
  language = {en-US}
}
% == BibTeX quality report for jonesTacklingAwkwardSquad2001:
% ? Unsure about the formatting of the booktitle

@misc{kiselyovManyFacesFixedpoint2013,
  title = {Many Faces of the Fixed-Point Combinator},
  author = {Kiselyov, Oleg},
  year = {2013},
  month = aug,
  url = {http://okmij.org/ftp/Computation/fixed-point-combinators.html},
  urldate = {2020-07-31},
  journal = {okmij.org}
}
% == BibTeX quality report for kiselyovManyFacesFixedpoint2013:
% ? Possibly abbreviated journal title okmij.org

@misc{lafontLinearLogicPages,
  title = {Linear {{Logic Pages}}},
  author = {Lafont, Yves},
  url = {http://iml.univ-mrs.fr/~lafont/pub/llpages.pdf},
  language = {en}
}
% == BibTeX quality report for lafontLinearLogicPages:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{levyJumboLCalculus2006,
  title = {Jumbo λ-{{Calculus}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Levy, Paul Blain},
  editor = {Bugliesi, Michele and Preneel, Bart and Sassone, Vladimiro and Wegener, Ingo},
  year = {2006},
  pages = {444--455},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11787006_38},
  url = {https://www.cs.bham.ac.uk/~pbl/papers/jumboicalp.pdf},
  abstract = {We make an argument that, for any study involving computational effects such as divergence or continuations, the traditional syntax of simply typed lambda-calculus cannot be regarded as canonical, because standard arguments for canonicity rely on isomorphisms that may not exist in an effectful setting. To remedy this, we define a “jumbo lambda-calculus” that fuses the traditional connectives together into more general ones, so-called “jumbo connectives”. We provide two pieces of evidence for our thesis that the jumbo formulation is advantageous.Firstly, we show that the jumbo lambda-calculus provides a “complete” range of connectives, in the sense of including every possible connective that, within the beta-eta theory, possesses a reversible rule.Secondly, in the presence of effects, we show that there is no decomposition of jumbo connectives into non-jumbo ones that is valid in both call-by-value and call-by-name.},
  isbn = {978-3-540-35908-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}
% == BibTeX quality report for levyJumboLCalculus2006:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@incollection{martiniFineStructureExponential1995,
  ids = {martiniFineStructureExponential1995a},
  title = {On the Fine Structure of the Exponential Rule},
  booktitle = {Advances in {{Linear Logic}}},
  author = {Martini, S. and Masini, A.},
  editor = {Girard, Jean-Yves and Lafont, Yves and Regnier, Laurent},
  year = {1995},
  pages = {197--210},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511629150.010},
  url = {https://pdfs.semanticscholar.org/b2cb/538c8ef21af42e48134a17a3c62ce5167837.pdf},
  urldate = {2020-08-01},
  abstract = {We present natural deduction systems for fragments of intuitionistic linear logic obtained by dropping weakening and contractions also on !-pre xed formulas. The systems are based on a twodimensional generalization of the notion of sequent, which accounts for a clean formulation of the introduction/elimination rules of the modality. Moreover, the di erent subsystems are obtained in a modular way, by simple conditions on the elimination rule for !. For the proposed systems we introduce a notion of reduction and we prove a normalization theorem.},
  isbn = {978-0-511-62915-0},
  language = {en}
}

@phdthesis{mauborgneRepresentationSetsTrees1999,
  title = {Representation of Sets of Trees for Abstract Interpretation},
  author = {Mauborgne, Laurent},
  year = {1999},
  month = nov,
  url = {http://software.imdea.org/~mauborgn/publi/t.pdf},
  journal = {PhD Thesis},
  school = {Ecole Polytechnique}
}

@article{mokhovBuildSystemsCarte2020,
  title = {Build Systems à La Carte: {{Theory}} and Practice},
  shorttitle = {Build Systems à La Carte},
  author = {Mokhov, Andrey and Mitchell, Neil and Peyton Jones, Simon},
  year = {2020},
  volume = {30},
  pages = {e11},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796820000088},
  url = {https://ndmitchell.com/downloads/paper-build_systems_a_la_carte_theory_and_practice-21_apr_2020.pdf},
  urldate = {2020-06-11},
  abstract = {Build systems are awesome, terrifying – and unloved. They are used by every developer around the world, but are rarely the object of study. In this paper, we offer a systematic, and executable, framework for developing and comparing build systems, viewing them as related points in a landscape rather than as isolated phenomena. By teasing apart existing build systems, we can recombine their components, allowing us to prototype new build systems with desired properties.},
  journal = {Journal of Functional Programming},
  language = {en}
}

@inproceedings{najafiBisectingCommitsModeling2019,
  title = {Bisecting Commits and Modeling Commit Risk during Testing},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}  - {{ESEC}}/{{FSE}} 2019},
  author = {Najafi, Armin and Rigby, Peter C. and Shang, Weiyi},
  year = {2019},
  pages = {279--289},
  publisher = {{ACM Press}},
  address = {{Tallinn, Estonia}},
  doi = {10.1145/3338906.3338944},
  url = {https://users.encs.concordia.ca/~shang/pubs/Armin_FSE_2019.pdf},
  urldate = {2020-07-06},
  abstract = {Software testing is one of the costliest stages in the software development life cycle. One approach to reducing the test execution cost is to group changes and test them as a batch (i.e. batch testing). However, when tests fail in a batch, commits in the batch need to be re-tested to identify the cause of the failure, i.e. the culprit commit. The re-testing is typically done through bisection (i.e. a binary search through the commits in a batch). Intuitively, the effectiveness of batch testing highly depends on the size of the batch. Larger batches require fewer initial test runs, but have a higher chance of a test failure that can lead to expensive test re-runs to find the culprit. We are unaware of research that investigates and simulates the impact of batch sizes on the cost of testing in industry. In this work, we first conduct empirical studies on the effectiveness of batch testing in three large-scale industrial software systems at Ericsson. Using 9 months of testing data, we simulate batch sizes from 1 to 20 and find the most cost-effective BatchSize for each project. Our results show that batch testing saves 72\% of test executions compared to testing each commit individually. In a second simulation, we incorporate flaky tests that pass and fail on the same commit as they are a significant source of additional test executions on large projects. We model the degree of flakiness for each project and find that test flakiness reduces the cost savings to 42\%. In a third simulation, we guide bisection to reduce the likelihood of batch-testing failures. We model the riskiness of each commit in a batch using a bug model and a test execution history model. The risky commits are tested individually, while the less risky commits are tested in a single larger batch. Culprit predictions with our approach reduce test executions up to 9\% compared to Ericsson’s current bisection approach.},
  isbn = {978-1-4503-5572-8},
  language = {en}
}

@article{peytonjonesSecretsGlasgowHaskell2002,
  ids = {jonesSecretsGlasgowHaskell2002},
  title = {Secrets of the {{Glasgow Haskell Compiler}} Inliner},
  author = {Peyton Jones, Simon and Marlow, Simon},
  year = {2002},
  month = jul,
  volume = {12},
  pages = {393--434},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796802004331},
  url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2002/07/inline.pdf},
  urldate = {2020-07-01},
  abstract = {Higher-order languages, such as Haskell, encourage the programmer to build abstractions by composing functions. A good compiler must inline many of these calls to recover an e ciently executable program.},
  journal = {Journal of Functional Programming},
  language = {en},
  number = {4}
}

@techreport{proustASAPStaticPossible2017,
  title = {{{ASAP}}: {{As Static As Possible}} Memory Management},
  author = {Proust, Raphaël L},
  year = {2017},
  month = jul,
  pages = {145},
  institution = {{University of Cambridge Computer Laboratory}},
  url = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.pdf},
  language = {en},
  number = {UCAM-CL-TR-908}
}

@misc{shalBuildSystemRules2009,
  title = {Build {{System Rules}} and {{Algorithms}}},
  author = {Shal, Mike},
  year = {2009},
  publisher = {{gittup.org}},
  url = {http://gittup.org/tup/build_system_rules_and_algorithms.pdf},
  urldate = {2021-01-22}
}
% == BibTeX quality report for shalBuildSystemRules2009:
% ? Title looks like it was stored in title-case in Zotero

@article{sperberGenerationLRParsers2000,
  title = {Generation of {{LR}} Parsers by Partial Evaluation},
  author = {Sperber, Michael and Thiemann, Peter},
  year = {2000},
  month = mar,
  volume = {22},
  pages = {224--264},
  issn = {0164-0925, 1558-4593},
  doi = {10.1145/349214.349219},
  url = {http://dl.acm.org/doi/10.1145/349214.349219},
  urldate = {2020-06-15},
  journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
  language = {en},
  number = {2}
}

@inproceedings{wadlerCallbyvalueDualCallbyname2003,
  ids = {wadlerCallbyValueDualCallbyName},
  title = {Call-by-Value Is Dual to Call-by-Name},
  booktitle = {Proceedings of the Eighth {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  author = {Wadler, Philip},
  year = {2003},
  month = aug,
  pages = {189--201},
  publisher = {{Association for Computing Machinery}},
  address = {{Uppsala, Sweden}},
  doi = {10.1145/944705.944723},
  url = {http://homepages.inf.ed.ac.uk/wadler/papers/dual/dual.pdf},
  urldate = {2020-06-17},
  abstract = {The rules of classical logic may be formulated in pairs corresponding to De Morgan duals: rules about \& are dual to rules about V. A line of work, including that of Filinski (1989), Griffin (1990), Parigot (1992), Danos, Joinet, and Schellinx (1995), Selinger (1998,2001), and Curien and Herbelin (2000), has led to the startling conclusion that call-by-value is the de Morgan dual of call-by-name.This paper presents a dual calculus that corresponds to the classical sequent calculus of Gentzen (1935) in the same way that the lambda calculus of Church (1932,1940) corresponds to the intuitionistic natural deduction of Gentzen (1935). The paper includes crisp formulations of call-by-value and call-by-name that are obviously dual; no similar formulations appear in the literature. The paper gives a CPS translation and its inverse, and shows that the translation is both sound and complete, strengthening a result in Curien and Herbelin (2000).},
  isbn = {978-1-58113-756-9},
  keywords = {Curry-Howard correspondence,De Morgan dual,lambda calculus,lambda mu calculus,logic,natural deduction,sequent calculus},
  series = {{{ICFP}} '03}
}
% == BibTeX quality report for wadlerCallbyvalueDualCallbyname2003:
% ? Unsure about the formatting of the booktitle

@article{wheelerFullyCounteringTrusting2010,
  title = {Fully {{Countering Trusting Trust}} through {{Diverse Double}}-{{Compiling}}},
  author = {Wheeler, David A.},
  year = {2010},
  month = apr,
  url = {http://arxiv.org/abs/1004.5534},
  urldate = {2020-09-13},
  abstract = {An Air Force evaluation of Multics, and Ken Thompson's Turing award lecture ("Reflections on Trusting Trust"), showed that compilers can be subverted to insert malicious Trojan horses into critical software, including themselves. If this "trusting trust" attack goes undetected, even complete analysis of a system's source code will not find the malicious code that is running. Previously-known countermeasures have been grossly inadequate. If this attack cannot be countered, attackers can quietly subvert entire classes of computer systems, gaining complete control over financial, infrastructure, military, and/or business systems worldwide. This dissertation's thesis is that the trusting trust attack can be detected and effectively countered using the "Diverse Double-Compiling" (DDC) technique, as demonstrated by (1) a formal proof that DDC can determine if source code and generated executable code correspond, (2) a demonstration of DDC with four compilers (a small C compiler, a small Lisp compiler, a small maliciously corrupted Lisp compiler, and a large industrial-strength C compiler, GCC), and (3) a description of approaches for applying DDC in various real-world scenarios. In the DDC technique, source code is compiled twice: the source code of the compiler's parent is compiled using a trusted compiler, and then the putative compiler source code is compiled using the result of the first compilation. If the DDC result is bit-for-bit identical with the original compiler-under-test's executable, and certain other assumptions hold, then the compiler-under-test's executable corresponds with its putative source code.},
  archivePrefix = {arXiv},
  eprint = {1004.5534},
  eprinttype = {arxiv},
  journal = {arXiv:1004.5534 [cs]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Programming Languages},
  primaryClass = {cs}
}
% == BibTeX quality report for wheelerFullyCounteringTrusting2010:
% ? Possibly abbreviated journal title arXiv:1004.5534 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{wikipediaForwardBackwardAlgorithm2020,
  title = {Forward–Backward Algorithm},
  author = {{Wikipedia}},
  year = {2020},
  month = jul,
  url = {https://en.wikipedia.org/w/index.php?title=Forward%E2%80%93backward_algorithm&oldid=966683884},
  urldate = {2020-07-08},
  abstract = {The forward–backward algorithm is an  inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions                                    o                        1             :             T                             :=                    o                        1                             ,         …         ,                    o                        T                                     \{\textbackslash displaystyle o\_\{1:T\}:=o\_\{1\},\textbackslash dots ,o\_\{T\}\}   , i.e. it computes, for all hidden state variables                                    X                        t                             ∈         \{                    X                        1                             ,         …         ,                    X                        T                             \}                 \{\textbackslash displaystyle X\_\{t\}\textbackslash in \textbackslash\{X\_\{1\},\textbackslash dots ,X\_\{T\}\textbackslash\}\}   , the distribution                         P         (                    X                        t                                                  |                                       o                        1             :             T                             )                 \{\textbackslash displaystyle P(X\_\{t\}\textbackslash{} |\textbackslash{} o\_\{1:T\})\}   . This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to efficiently compute the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm. The term forward–backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class.},
  annotation = {Page Version ID: 966683884},
  copyright = {Creative Commons Attribution-ShareAlike License},
  journal = {Wikipedia},
  language = {en}
}

@article{wikipediaLogit2020,
  title = {Logit},
  author = {{Wikipedia}},
  year = {2020},
  month = jun,
  url = {https://en.wikipedia.org/w/index.php?title=Logit&oldid=964387041},
  urldate = {2020-07-07},
  abstract = {In statistics, the logit ( LOH-jit) function or the log-odds is the logarithm of the odds                                                 p                            1               −               p                                                  \{\textbackslash displaystyle \{\textbackslash frac \{p\}\{1-p\}\}\}    where p is probability. It is a type of function that creates a map of probability values from                         (         0         ,         1         )                 \{\textbackslash displaystyle (0,1)\}    to                         (         −         ∞         ,         +         ∞         )                 \{\textbackslash displaystyle (-\textbackslash infty ,+\textbackslash infty )\}   . It is the inverse of the sigmoidal "logistic" function or logistic transform used in mathematics, especially in statistics. In deep learning, the term logits layer is popularly used for the last neuron layer of neural networks used for classification tasks, which produce raw prediction values as real numbers ranging from                         (         −         ∞         ,         +         ∞         )                 \{\textbackslash displaystyle (-\textbackslash infty ,+\textbackslash infty )\}   .},
  annotation = {Page Version ID: 964387041},
  copyright = {Creative Commons Attribution-ShareAlike License},
  journal = {Wikipedia},
  language = {en}
}

@inproceedings{ziftciWhoBrokeBuild2017,
  title = {Who Broke the Build? {{Automatically}} Identifying Changes That Induce Test Failures in Continuous Integration at {{Google Scale}}},
  shorttitle = {Who Broke the Build?},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice Track}} ({{ICSE}}-{{SEIP}})},
  author = {Ziftci, Celal and Reardon, Jim},
  year = {2017},
  month = may,
  pages = {113--122},
  publisher = {{IEEE}},
  address = {{Buenos Aires}},
  doi = {10.1109/ICSE-SEIP.2017.13},
  url = {https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45794.pdf},
  urldate = {2020-07-06},
  abstract = {Quickly identifying and fixing code changes that introduce regressions is critical to keep the momentum on software development, especially in very large scale software repositories with rapid development cycles, such as at Google. Identifying and fixing such regressions is one of the most expensive, tedious, and time consuming tasks in the software development life-cycle. Therefore, there is a high demand for automated techniques that can help developers identify such changes while minimizing manual human intervention. Various techniques have recently been proposed to identify such code changes. However, these techniques have shortcomings that make them unsuitable for rapid development cycles as at Google. In this paper, we propose a novel algorithm to identify code changes that introduce regressions, and discuss case studies performed at Google on 140 projects. Based on our case studies, our algorithm automatically identifies the change that introduced the regression in the top-5 among thousands of candidates 82\% of the time, and provides considerable savings on manual work developers need to perform.},
  isbn = {978-1-5386-2717-4},
  language = {en}
}


