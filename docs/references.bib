@misc{abdelgawadNOOPDomaintheoreticModel2018,
  title = {{{NOOP}}: A Domain-Theoretic Model of Nominally-Typed {{OOP}}},
  shorttitle = {{{NOOP}}},
  author = {AbdelGawad, Moez and Cartwright, Robert},
  year = {2018},
  month = jan,
  number = {arXiv:1801.06793},
  eprint = {1801.06793},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1801.06793},
  urldate = {2022-12-27},
  abstract = {The majority of industrial-strength object-oriented (OO) software is written using nominally-typed OO programming languages. Extant domaintheoretic models of OOP developed to analyze OO type systems miss, however, a crucial feature of these mainstream OO languages: nominality. This paper presents the construction of NOOP as the first domain-theoretic model of OOP that includes full class/type names information found in nominallytyped OOP. Inclusion of nominal information in objects of NOOP and asserting that type inheritance in statically-typed OO programming languages is an inherently nominal notion allow readily proving that type inheritance and subtyping are completely identified in these languages. This conclusion is in full agreement with intuitions of developers and language designers of these OO languages, and contrary to the belief that “inheritance is not subtyping,” which came from assuming non-nominal (a.k.a., structural) models of OOP.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  note = {Comment: 23 pages}
}

@article{abelUiCAAccurateThroughput2021,
  ids = {abelAccurateThroughputPrediction2021a},
  title = {{{uiCA}}: Accurate Throughput Prediction of Basic Blocks on Recent {{Intel}} Microarchitectures},
  author = {Abel, Andreas and Reineke, Jan},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.14210 [cs]},
  eprint = {2107.14210},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.14210},
  urldate = {2021-07-30},
  abstract = {Tools to predict the throughput of basic blocks on a specific microarchitecture are useful to optimize software performance and to build optimizing compilers. In recent work, several such tools have been proposed. However, the accuracy of their predictions has been shown to be relatively low. In this paper, we identify the most important factors for these inaccuracies. To a significant degree these inaccuracies are due to elements and parameters of the pipelines of recent CPUs that are not taken into account by previous tools. A primary reason for this is that the necessary details are often undocumented. In this paper, we build more precise models of relevant components by reverse engineering using microbenchmarks. Based on these models, we develop a simulator for predicting the throughput of basic blocks. In addition to predicting the throughput, our simulator also provides insights into how the code is executed. Our tool supports all Intel Core microarchitecture generations released in the last decade. We evaluate it on an improved version of the BHive benchmark suite. On many recent microarchitectures, its predictions are more accurate than the predictions of state-of-the-art tools by more than an order of magnitude.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Performance}
}
% == BibTeX quality report for abelUiCAAccurateThroughput2021:
% ? Possibly abbreviated journal title arXiv:2107.14210 [cs]

@article{abramskyGeometryInteractionLinear2002,
  title = {Geometry of Interaction and Linear Combinatory Algebras.},
  author = {Abramsky, Samson and Haghverdi, Esfandiar and Scott, Philip},
  year = {2002},
  month = oct,
  journal = {Mathematical Structures in Computer Science},
  volume = {12},
  pages = {625--665},
  doi = {10.1017/S0960129502003730},
  url = {https://www.researchgate.net/profile/Samson_Abramsky/publication/220173613_Geometry_of_Interaction_and_Linear_Combinatory_Algebras/links/0c96052560eec33e21000000/Geometry-of-Interaction-and-Linear-Combinatory-Algebras.pdf},
  abstract = {this paper was quite di\#erent, stemming from the axiomatics of categories of tangles (although the authors were aware of possible connections to iteration theories. In fact, similar axiomatics in the symmetric case, motivated by flowcharts and "flownomials" had been developed some years earlier by Stefanescu (Stefanescu 2000).) However, the first author realized, following a stimulating discussion with Gordon Plotkin, that traced monoidal categories provided a common denominator for the axiomatics of both the Girard-style and Abramsky-Jagadeesan-style versions of the Geometry of Interaction, at the basic level of the multiplicatives. This insight was presented in (Abramsky 1996), in which Girard-style GoI was dubbed "particle-style", since it concerns information particles or tokens flowing around a network, while the Abramsky-Jagadeesan style GoI was dubbed "wave-style", since it concerns the evolution of a global information state or "wave". Formally, this distinction is based on whether the tensor product (i.e. the symmetric monoidal structure) in the underlying category is interpreted as a coproduct (particle style) or as a product (wave style). This computational distinction between coproduct and product interpretations of the same underlying network geometry turned out to have been partially anticipated, in a rather di\#erent context, in a pioneering paper by E. S. Bainbridge (Bainbridge 1976), as observed by Dusko Pavlovic. These two forms of interpretation, and ways of combining them, have also been studied recently in (Stefanescu 2000). He uses the terminology "additive" for coproduct-based (i.e. our "particle-style") and "multiplicative" for product-based (i.e. our "wave-style"); this is not suitable for our purposes, because of the clash with Linear Logic term...}
}
% == BibTeX quality report for abramskyGeometryInteractionLinear2002:
% ? unused Library catalog ("ResearchGate")

@incollection{abrusciNoncommutativeProofNets1995,
  title = {Noncommutative Proof Nets},
  booktitle = {Advances in {{Linear Logic}}},
  author = {Abrusci, V. M.},
  editor = {Girard, Jean-Yves and Regnier, Laurent and Lafont, Yves},
  year = {1995},
  series = {London {{Mathematical Society Lecture Note Series}}},
  pages = {271--296},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511629150.014},
  url = {https://www.cambridge.org/core/books/advances-in-linear-logic/noncommutative-proof-nets/D0811717B2F7F378AF56856575ECF891},
  urldate = {2021-03-03},
  abstract = {IntroductionThe aim of this paper is to give a purely graph-theoretical definition of noncommutative proof nets, i.e. graphs coming from proofs in MNLL (multiplicative noncommutative linear logic, the (⊗, ℘)-fragment of the one-sided sequent calculus for classical noncommutative linear logic, introduced in [Abr91]). Analogously, one of the aims of [Gir87] was to give a purely graph-theoretical definition of proof nets, i.e. graphs coming from the proofs in MLL (multiplicative linear logic, the (⊗, ℘)-fragment of the one-sided sequent calculus for classical linear logic - better, for classical commutative linear logic). - The relevance of the purely graph-theoretical definition of proof nets for the development of commutative linear logic is well-know; thus we hope the results of this paper will be useful for a similar development of noncommutative linear logic.The language for MNLL is an extension of the language for MLL, obtained simply adding, as atomic formulas, propositional letters with an arbitrary finite number of negations written after the propositional letter (linear post-negation) or before the propositional letter (linear retronegation). Every formula A of MNLL may be translated into a formula Tv(A) of MLL (simply by replacing each propositional letter with an even number of negations by the propositional letter without negations, and each propositional letter with an odd number of negations by the propositional letter with only one negation after the propositional letter).},
  isbn = {978-0-521-55961-4},
  note = {Proceedings of the Workshop on Linear Logic, Ithaca, New York, June 1993}
}

@article{adamsPrincipledParsingIndentationsensitive2013,
  ids = {adamsPrincipledParsingIndentationSensitive,adamsPrincipledParsingIndentationSensitivea},
  title = {Principled Parsing for Indentation-Sensitive Languages: Revisiting {{Landin}}'s Offside Rule},
  shorttitle = {Principled Parsing for Indentation-Sensitive Languages},
  author = {Adams, Michael D.},
  year = {2013},
  month = jan,
  journal = {ACM SIGPLAN Notices},
  volume = {48},
  number = {1},
  pages = {511--522},
  issn = {0362-1340},
  doi = {10.1145/2480359.2429129},
  url = {https://doi.org/10.1145/2480359.2429129},
  urldate = {2022-05-18},
  abstract = {Several popular languages, such as Haskell, Python, and F\#, use the indentation and layout of code as part of their syntax. Because context-free grammars cannot express the rules of indentation, parsers for these languages currently use ad hoc techniques to handle layout. These techniques tend to be low-level and operational in nature and forgo the advantages of more declarative specifications like context-free grammars. For example, they are often coded by hand instead of being generated by a parser generator. This paper presents a simple extension to context-free grammars that can express these layout rules, and derives GLR and LR(k) algorithms for parsing these grammars. These grammars are easy to write and can be parsed efficiently. Examples for several languages are presented, as are benchmarks showing the practical efficiency of these algorithms.},
  keywords = {indentation,offside rule,parsing}
}
% == BibTeX quality report for adamsPrincipledParsingIndentationsensitive2013:
% ? unused Journal abbreviation ("SIGPLAN Not.")
% ? unused Library catalog ("January 2013")

@article{albertResourceAnalysisDriven2019,
  title = {Resource Analysis Driven by (Conditional) Termination Proofs},
  author = {Albert, Elvira and Bofill, Miquel and Borralleras, Cristina and {Martin-Martin}, Enrique and Rubio, Albert},
  year = {2019},
  month = sep,
  journal = {Theory and Practice of Logic Programming},
  volume = {19},
  number = {5-6},
  eprint = {1907.10096},
  pages = {722--739},
  issn = {1471-0684, 1475-3081},
  doi = {10.1017/S1471068419000152},
  url = {http://arxiv.org/abs/1907.10096},
  urldate = {2020-06-22},
  abstract = {When programs feature a complex control flow, existing techniques for resource analysis produce cost relation systems (CRS) whose cost functions retain the complex flow of the program and, consequently, might not be solvable into closed-form upper bounds. This paper presents a novel approach to resource analysis that is driven by the result of a termination analysis. The fundamental idea is that the termination proof encapsulates the flows of the program which are relevant for the cost computation so that, by driving the generation of the CRS using the termination proof, we produce a linearly-bounded CRS (LB-CRS). A LB-CRS is composed of cost functions that are guaranteed to be locally bounded by linear ranking functions and thus greatly simplify the process of CRS solving. We have built a new resource analysis tool, named MaxCore, that is guided by the VeryMax termination analyzer and uses CoFloCo and PUBS as CRS solvers. Our experimental results on the set of benchmarks from the Complexity and Termination Competition 2019 for C Integer programs show that MaxCore outperforms all other resource analysis tools. Under consideration for acceptance in TPLP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  note = {Comment: Paper presented at the 35th International Conference on Logic Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019, 16 pages}
}

@article{albrechtSoftwareFunctionSource1983,
  title = {Software Function, Source Lines of Code, and Development Effort Prediction: A Software Science Validation},
  shorttitle = {Software Function, Source Lines of Code, and Development Effort Prediction},
  author = {Albrecht, A.J. and Gaffney, J.E.},
  year = {1983},
  month = nov,
  journal = {IEEE Transactions on Software Engineering},
  volume = {SE-9},
  number = {6},
  pages = {639--648},
  issn = {0098-5589},
  doi = {10.1109/TSE.1983.235271},
  url = {http://ieeexplore.ieee.org/document/1703110/},
  urldate = {2023-12-28},
  abstract = {One of the most important problems faced by software developers and users is the prediction of the size of a programming system and its development effort. As an alternative to "size," one might deal with a measure of the "function" that the software is to perform. Albrecht [1] has developed a methodology to estimate the amount of the "function" the software is to perform, in terms of the data it is to use (absorb) and to generate (produce). The "function" is quantified as "function points," essentially, a weighted sum of the numbers of "inputs," "outputs," master files," and "inquiries" provided to, or generated by, the software. This paper demonstrates the equivalence between Albrecht's external input/output data flow representative of a program (the "function points" metric) and Halstead's [2] "software science" or "software linguistics" model of a program as well as the "soft content" variation of Halstead's model suggested by Gaffney [7].},
  langid = {english}
}
% == BibTeX quality report for albrechtSoftwareFunctionSource1983:
% ? unused Journal abbreviation ("IIEEE Trans. Software Eng.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{allenCatalogueOptimizingTransformations1971,
  title = {A Catalogue of Optimizing Transformations},
  author = {Allen, Frances E and Cocke, John},
  year = {1971},
  journal = {IBM Research Center},
  pages = {30},
  url = {https://www.clear.rice.edu/comp512/Lectures/Papers/1971-allen-catalog.pdf},
  langid = {english}
}
% == BibTeX quality report for allenCatalogueOptimizingTransformations1971:
% ? unused Library catalog ("Zotero")

@inproceedings{amorimDeclarativeSpecificationIndentation2018,
  ids = {amorimDeclarativeSpecificationIndentation2018a},
  title = {Declarative Specification of Indentation Rules: A Tooling Perspective on Parsing and Pretty-Printing Layout-Sensitive Languages},
  shorttitle = {Declarative Specification of Indentation Rules},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN International Conference}} on {{Software Language Engineering}}  - {{SLE}} 2018},
  author = {Amorim, Luís Eduardo de Souza and Steindorfer, Michael J. and Erdweg, Sebastian and Visser, Eelco},
  year = {2018},
  pages = {3--15},
  publisher = {{ACM Press}},
  address = {{Boston, MA, USA}},
  doi = {10.1145/3276604.3276607},
  url = {http://udesou.info/wp-content/uploads/2018/10/layout-pp.pdf},
  urldate = {2020-06-15},
  abstract = {In layout-sensitive languages, the indentation of an expression or statement can influence how a program is parsed. While some of these languages (e.g., Haskell and Python) have been widely adopted, there is little support for software language engineers in building tools for layout-sensitive languages. As a result, parsers, pretty-printers, program analyses, and refactoring tools often need to be handwritten, which decreases the maintainability and extensibility of these tools. Even state-of-the-art language workbenches have little support for layout-sensitive languages, restricting the development and prototyping of such languages. In this paper, we introduce a novel approach to declarative specification of layout-sensitive languages using layout declarations. Layout declarations are high-level specifications of indentation rules that abstract from low-level technicalities. We show how to derive an efficient layout-sensitive generalized parser and a corresponding pretty-printer automatically from a language specification with layout declarations. We validate our approach in a case-study using a syntax definition for the Haskell programming language, investigating the performance of the generated parser and the correctness of the generated pretty-printer against 22191 Haskell files.},
  isbn = {978-1-4503-6029-6},
  langid = {english}
}
% == BibTeX quality report for amorimDeclarativeSpecificationIndentation2018:
% ? unused Conference name ("the 11th ACM SIGPLAN International Conference")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{ananthanarayananKeepingMasterGreen2019,
  title = {Keeping Master Green at Scale},
  booktitle = {Proceedings of the {{Fourteenth EuroSys Conference}} 2019},
  author = {Ananthanarayanan, Sundaram and Ardekani, Masoud Saeida and Haenikel, Denis and Varadarajan, Balaji and Soriano, Simon and Patel, Dhaval and {Adl-Tabatabai}, Ali-Reza},
  year = {2019},
  month = mar,
  series = {{{EuroSys}} '19},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{Dresden, Germany}},
  doi = {10.1145/3302424.3303970},
  url = {https://doi.org/10.1145/3302424.3303970},
  urldate = {2020-07-06},
  abstract = {Giant monolithic source-code repositories are one of the fundamental pillars of the back end infrastructure in large and fast-paced software companies. The sheer volume of everyday code changes demands a reliable and efficient change management system with three uncompromisable key requirements --- always green master, high throughput, and low commit turnaround time. Green refers to a master branch that always successfully compiles and passes all build steps, the opposite being red. A broken master (red) leads to delayed feature rollouts because a faulty code commit needs to be detected and rolled backed. Additionally, a red master has a cascading effect that hampers developer productivity--- developers might face local test/build failures, or might end up working on a codebase that will eventually be rolled back. This paper presents the design and implementation of SubmitQueue. It guarantees an always green master branch at scale: all build steps (e.g., compilation, unit tests, UI tests) successfully execute for every commit point. SubmitQueue has been in production for over a year, and can scale to thousands of daily commits to giant monolithic repositories.},
  isbn = {978-1-4503-6281-8}
}
% == BibTeX quality report for ananthanarayananKeepingMasterGreen2019:
% ? unused Library catalog ("ACM Digital Library")

@article{appelGarbageCollectionCan1987,
  title = {Garbage Collection Can Be Faster than Stack Allocation},
  author = {Appel, Andrew W.},
  year = {1987},
  month = jun,
  journal = {Information Processing Letters},
  volume = {25},
  number = {4},
  pages = {275--279},
  issn = {00200190},
  doi = {10.1016/0020-0190(87)90175-X},
  url = {https://www.cs.princeton.edu/~appel/papers/45.pdf},
  urldate = {2020-07-24},
  abstract = {A very old and simple algorithm for garbage collection gives very good results when the physical memory is much larger than the number of reachable cells. In fact, the overhead associated with allocating and collecting cells from the heap can be reduced to less than one instruction per cell by increasing the size of physical memory. Special hardware, intricate garbage-collection algorithms, and fancy compiler analysis become unnecessary.},
  langid = {english}
}
% == BibTeX quality report for appelGarbageCollectionCan1987:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{appelSSAFunctionalProgramming1998,
  title = {{{SSA}} Is Functional Programming},
  author = {Appel, Andrew W.},
  year = {1998},
  month = apr,
  journal = {ACM SIGPLAN Notices},
  volume = {33},
  number = {4},
  pages = {17--20},
  issn = {0362-1340},
  doi = {10.1145/278283.278285},
  url = {https://dl.acm.org/doi/10.1145/278283.278285},
  urldate = {2023-05-03}
}
% == BibTeX quality report for appelSSAFunctionalProgramming1998:
% ? unused Journal abbreviation ("SIGPLAN Not.")
% ? unused Library catalog ("ACM Digital Library")

@incollection{aptWhyOccurcheckNot1992,
  title = {Why the Occur-Check Is Not a Problem},
  booktitle = {Programming {{Language Implementation}} and {{Logic Programming}}},
  author = {Apt, Krzysztof R. and Pellegrini, Alessandro},
  editor = {Bruynooghe, Maurice and Wirsing, Martin},
  year = {1992},
  volume = {631},
  pages = {69--86},
  publisher = {{Springer-Verlag}},
  address = {{Berlin/Heidelberg}},
  doi = {10.1007/3-540-55844-6_128},
  url = {http://link.springer.com/10.1007/3-540-55844-6_128},
  urldate = {2022-07-14},
  abstract = {In most Prolog implementations for the efficiency reasons so-called occur-check is omitted from the unification algorithm. We provide here natural syntactic conditions which allow the occw-check to be safely omitted. The established results apply to most well-known Prolog programs and seem to explain why this omission does not lead in practice to any complications.},
  isbn = {978-3-540-55844-6},
  langid = {english}
}
% == BibTeX quality report for aptWhyOccurcheckNot1992:
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{arditiReadingFixedVariable1990,
  title = {Reading with Fixed and Variable Character Pitch},
  author = {Arditi, Aries and Knoblauch, Kenneth and Grunwald, Ilana},
  year = {1990},
  month = oct,
  journal = {Journal of the Optical Society of America A},
  volume = {7},
  number = {10},
  pages = {2011},
  issn = {1084-7529, 1520-8532},
  doi = {10.1364/JOSAA.7.002011},
  url = {https://opg.optica.org/abstract.cfm?URI=josaa-7-10-2011},
  urldate = {2022-05-25},
  langid = {english}
}
% == BibTeX quality report for arditiReadingFixedVariable1990:
% ? unused Journal abbreviation ("J. Opt. Soc. Am. A")
% ? unused Library catalog ("DOI.org (Crossref)")

@misc{ArtimaJavaDesign,
  title = {Artima - {{Java Design Issues}}},
  url = {https://www.artima.com/articles/java-design-issues},
  urldate = {2022-12-13}
}

@article{aspertiBolognaOptimalHigherorder1996,
  title = {The Bologna Optimal Higher-Order Machine},
  author = {Asperti, Andrea and Giovannetti, Cecilia and Naletto, Andrea},
  year = {1996},
  month = nov,
  journal = {Journal of Functional Programming},
  volume = {6},
  number = {6},
  pages = {763--810},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796800001994},
  url = {https://www.cambridge.org/core/product/identifier/S0956796800001994/type/journal_article},
  urldate = {2020-06-15},
  abstract = {The Bologna Optimal Higher-order Machine (BOHM) is a prototype implementation of the core of a functional language based on (a variant of) Lamping’s optimal graph reduction technique [Lam90, GAL92a, As94]. The source language is a sugared -calculus enriched with booleans, integers, lists and basic operations on these data types (following the guidelines of Interaction Systems [AL94, AL93b, Lan93]). In this paper, we shall describe BOHM’s general architecture (comprising the garbage collector), and we shall give a large set of benchmarks and experimental results.},
  langid = {english}
}
% == BibTeX quality report for aspertiBolognaOptimalHigherorder1996:
% ? unused Journal abbreviation ("J. Funct. Prog.")
% ? unused Library catalog ("DOI.org (Crossref)")

@book{aspertiOptimalImplementationFunctional1999,
  title = {The Optimal Implementation of Functional Programming Languages},
  author = {Asperti, Andrea and Guerrini, Stefano},
  year = {1999},
  month = jan,
  series = {Cambridge {{Tracts}} in {{Theoretical Computer Science}}},
  edition = {1st},
  number = {45},
  publisher = {{Cambridge University Press}},
  address = {{USA}},
  abstract = {All traditional implementation techniques for functional languages fail to avoid useless repetition of work. They are not "optimal" in their implementation of sharing, often causing a catastrophic, exponential explosion in reduction time. Optimal reduction is an innovative graph reduction technique for functional expressions, introduced by Lamping in 1990, that solves the sharing problem. This work, the first on the subject, is a comprehensive account by two of its leading exponents. Practical implementation aspects are fully covered as are the mathematical underpinnings of the subject. The relationship to the pioneering work of L\&\#233;vy and to Girard's more recent "Geometry of Interaction" are explored; optimal reduction is thereby revealed as a prime example of how a beautiful mathematical theory can lead to practical benefit. The book is essentially self-contained, requiring no more than basic familiarity with functional languages. It will be welcomed by graduate students and research workers in lambda calculus, functional programming or linear logic.},
  isbn = {978-0-521-62112-0}
}
% == BibTeX quality report for aspertiOptimalImplementationFunctional1999:
% ? unused Library catalog ("ACM Digital Library")
% ? unused Number of pages ("408")

@article{aspertiParallelBetaReduction2001,
  title = {Parallel Beta Reduction Is Not Elementary Recursive},
  author = {Asperti, Andrea and Mairson, Harry G.},
  year = {2001},
  month = oct,
  journal = {Information and Computation},
  volume = {170},
  number = {1},
  pages = {49--80},
  issn = {08905401},
  doi = {10.1006/inco.2001.2869},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S089054010192869X},
  urldate = {2020-06-15},
  langid = {english}
}
% == BibTeX quality report for aspertiParallelBetaReduction2001:
% ? unused Library catalog ("DOI.org (Crossref)")

@techreport{baikCOCOMOIIModel2000,
  title = {{{COCOMO II}} Model Manual 2000.0},
  author = {Baik, Jongmoon and Horowitz, Ellis},
  year = {2000},
  institution = {{University of Southern California}},
  url = {http://web.archive.org/web/20181123110403/http://csse.usc.edu/csse/research/COCOMOII/cocomo2000.0/CII_modelman2000.0.pdf},
  urldate = {2023-12-26}
}

@article{balabonskiUnifiedApproachFully2011,
  title = {A Unified Approach to Fully Lazy Sharing},
  author = {Balabonski, Thibaut},
  year = {2011},
  month = oct,
  abstract = {We give an axiomatic presentation of sharing-via-labelling for weak λ-calculi, that allows to formally compare many different approaches to fully lazy sharing, and obtain two important results. We prove that the known implementations of full laziness are all equivalent in terms of the number of β-reductions performed, although they behave differently regarding the duplication of terms. We establish a link between the optimality theories of weak λ-calculi and first-order rewriting systems by expressing fully lazy λ-lifting in our framework, thus emphasizing the first-order essence of weak reduction.},
  langid = {english}
}
% == BibTeX quality report for balabonskiUnifiedApproachFully2011:
% Missing required field 'journal'
% ? unused Library catalog ("Zotero")

@phdthesis{barikErrorMessagesRational2018,
  title = {Error {{Messages}} as {{Rational Reconstructions}}},
  author = {Barik, Titus},
  year = {2018},
  langid = {english},
  school = {North Carolina State University}
}
% == BibTeX quality report for barikErrorMessagesRational2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Zotero")

@inproceedings{barikHowShouldCompilers2018,
  title = {How Should Compilers Explain Problems to Developers?},
  booktitle = {Proceedings of the 2018 26th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Barik, Titus and Ford, Denae and {Murphy-Hill}, Emerson and Parnin, Chris},
  year = {2018},
  month = oct,
  pages = {633--643},
  publisher = {{ACM}},
  address = {{Lake Buena Vista FL USA}},
  doi = {10.1145/3236024.3236040},
  url = {https://dl.acm.org/doi/10.1145/3236024.3236040},
  urldate = {2022-10-15},
  abstract = {Compilers primarily give feedback about problems to developers through the use of error messages. Unfortunately, developers routinely find these messages to be confusing and unhelpful. In this paper, we postulate that because error messages present poor explanations, theories of explanation—such as Toulmin’s model of argument—can be applied to improve their quality. To understand how compilers should present explanations to developers, we conducted a comparative evaluation with 68 professional software developers and an empirical study of compiler error messages found in Stack Overflow questions across seven different programming languages.},
  isbn = {978-1-4503-5573-5},
  langid = {english}
}
% == BibTeX quality report for barikHowShouldCompilers2018:
% ? unused Conference name ("ESEC/FSE '18: 26th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{bauerIndentationSimplyMatter2019,
  title = {Indentation: Simply a Matter of Style or Support for Program Comprehension?},
  shorttitle = {Indentation},
  booktitle = {2019 {{IEEE}}/{{ACM}} 27th {{International Conference}} on {{Program Comprehension}} ({{ICPC}})},
  author = {Bauer, Jennifer and Siegmund, Janet and Peitek, Norman and Hofmeister, Johannes C. and Apel, Sven},
  year = {2019},
  month = may,
  pages = {154--164},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICPC.2019.00033},
  url = {https://ieeexplore.ieee.org/document/8813302/},
  urldate = {2022-05-19},
  abstract = {An early study showed that indentation is not a matter of style, but provides actual support for program comprehension. In this paper, we present a non-exact replication of this study. Our aim is to provide empirical evidence for the suggested level of indentation made by many style guides. Following Miara and others, we also included the perceived difficulty, and we extended the original design to gain additional insights into the influence of indentation on visual effort by employing an eyetracker. In the course of our study, we asked 22 participants to calculate the output of Java code snippets with different levels of indentation, while we recorded their gaze behavior. We did not find any indication that the indentation levels affect program comprehension or visual effort, so we could not replicate the findings of Miara and others. Nevertheless, our modernization of the original experiment design is a promising starting point for future studies in this field.},
  isbn = {978-1-72811-519-1},
  langid = {english}
}
% == BibTeX quality report for bauerIndentationSimplyMatter2019:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{beckerCompilerErrorMessages2019,
  title = {Compiler Error Messages Considered Unhelpful: {{The}} Landscape of Text-Based Programming Error Message Research},
  shorttitle = {Compiler Error Messages Considered Unhelpful},
  booktitle = {Proceedings of the {{Working Group Reports}} on {{Innovation}} and {{Technology}} in {{Computer Science Education}}},
  author = {Becker, Brett A. and Denny, Paul and Pettit, Raymond and Bouchard, Durell and Bouvier, Dennis J. and Harrington, Brian and Kamil, Amir and Karkare, Amey and McDonald, Chris and Osera, Peter-Michael and Pearce, Janice L. and Prather, James},
  year = {2019},
  month = dec,
  pages = {177--210},
  publisher = {{ACM}},
  address = {{Aberdeen Scotland Uk}},
  doi = {10.1145/3344429.3372508},
  url = {https://dl.acm.org/doi/10.1145/3344429.3372508},
  urldate = {2021-03-30},
  abstract = {Diagnostic messages generated by compilers and interpreters such as syntax error messages have been researched for over half of a century. Unfortunately, these messages which include error, warning, and run-time messages, present substantial difficulty and could be more effective, particularly for novices. Recent years have seen an increased number of papers in the area including studies on the effectiveness of these messages, improving or enhancing them, and their usefulness as a part of programming process data that can be used to predict student performance, track student progress, and tailor learning plans. Despite this increased interest, the long history of literature is quite scattered and has not been brought together in any digestible form.},
  isbn = {978-1-4503-7567-2},
  langid = {english}
}
% == BibTeX quality report for beckerCompilerErrorMessages2019:
% ? unused Conference name ("ITiCSE '19: Innovation and Technology in Computer Science Education")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{beckerWhatDoesSaying2021,
  title = {What Does Saying That 'programming Is Hard' Really Say, and about Whom?},
  author = {Becker, Brett A.},
  year = {2021},
  month = aug,
  journal = {Communications of the ACM},
  volume = {64},
  number = {8},
  pages = {27--29},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3469115},
  url = {https://cacm.acm.org/magazines/2021/8/254304-what-does-saying-that-programming-is-hard-really-say-and-about-whom/fulltext},
  urldate = {2021-08-28},
  abstract = {Shifting the focus from the perceived difficulty of learning programming to making programming more universally accessible.},
  langid = {english}
}
% == BibTeX quality report for beckerWhatDoesSaying2021:
% ? unused Journal abbreviation ("Commun. ACM")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{beierHowDoesTypeface2013,
  title = {How Does Typeface Familiarity Affect Reading Performance and Reader Preference?},
  author = {Beier, Sofie and Larson, Kevin},
  year = {2013},
  month = oct,
  journal = {Information Design Journal},
  volume = {20},
  number = {1},
  pages = {16--31},
  issn = {0142-5471, 1569-979X},
  doi = {10.1075/idj.20.1.02bei},
  url = {http://www.jbe-platform.com/content/journals/10.1075/idj.20.1.02bei},
  urldate = {2023-01-13},
  abstract = {Some typographers have proposed that typeface familiarity is defined by the amount of time that a reader has been exposed to a typeface design, while other typographers have proposed that familiarity is defined by the commonalities in letter shapes. These two hypotheses were tested by measuring the reading speed and preferences of participants. Participants were tested twice with common and uncommon letter shapes, once before and once after spending 20 minutes reading a story with the font. The results indicate that the exposure period has an effect on the speed of reading, but the uncommon letter shapes did not. Readers did not like the uncommon letter shapes. This has implications for the selection of type and the design of future typefaces.},
  langid = {english}
}
% == BibTeX quality report for beierHowDoesTypeface2013:
% ? unused Journal abbreviation ("IDJ")
% ? unused Library catalog ("DOI.org (Crossref)")

@phdthesis{beierTypefaceLegibilityDefining2009,
  type = {Thesis},
  title = {Typeface Legibility: {{Towards}} Defining Familiarity},
  shorttitle = {Typeface {{Legibility}}},
  author = {Beier, Sofie},
  year = {2009},
  month = may,
  url = {https://researchonline.rca.ac.uk/957/},
  urldate = {2023-02-11},
  abstract = {The aim of the project is to investigate the influence of fa- miliarity on reading. Three new fonts were created in order to examine the familiarity of fonts that readers could not have seen before. Each of the new fonts contains lowercase letters with fa- miliar and unfamiliar skeleton variations. The different skeleton variations were tested with distance threshold and time thresh- old methods in order to account for differences in visibility. This investigation helped create final typeface designs where the fa- miliar and unfamiliar skeleton variations have roughly similar and good performance. The typefaces were later applied as the test material in the familiarity investigation. Some typographers have proposed that familiarity means the amount of time that a reader has been exposed to a typeface design, while other typographers have proposed that familiarity is the commonalities in letterforms. These two hypotheses were tested by measuring the reading speed and preference of partici- pants, as they read fonts that had either common or uncommon letterforms, the fonts were then re-measured after an exposure period. The results indicate that exposure has an immediate ef- fect on the speed of reading, but that unfamiliar letter features only have an effect of preference and not on reading speed. By combining the craftsmen’s knowledge of designing with the methods of experimental research, the project takes a new step forward towards a better understanding of how different type- faces can influence the reading process.},
  langid = {english},
  school = {Royal College of Art}
}
% == BibTeX quality report for beierTypefaceLegibilityDefining2009:
% ? unused Library catalog ("researchonline.rca.ac.uk")
% ? unused Number of pages ("268")

@misc{ben-amramNotesPippengerComparison1996,
  title = {Notes on {{Pippenger}}'s Comparison of Pure and Impure {{LISP}}},
  author = {{Ben-amram}, Amir M.},
  year = {1996},
  url = {citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.3024&rep=rep1&type=pdf},
  abstract = {any impure-LISP program running in time t can be compiled into a pure-LISP program running in time O(t log t): first implement the impure-LISP operations using an array of size at most t. Then represent the array as a balanced binary tree, which can be done in pure LISP. The main result of the paper is a lower-bound theorem. It can roughly be described as follows. A problem P is presented, that can be solved in linear time, t = O(n), in impure  LISP. It is proved that for any pure-LISP program p for P , the worst-case time complexity is \textbackslash Omega\textbackslash Gamma n log n). 2 Restrictions of the Proof and Open Problems  The lower-bound result requires two restrictive assumptions. We first describe the restrictions and their technical implications. Next, we discuss the two questions that}
}
% == BibTeX quality report for ben-amramNotesPippengerComparison1996:
% ? unused Library catalog ("CiteSeer")

@article{bergerImpactProgrammingLanguages2019,
  title = {On the {{Impact}} of {{Programming Languages}} on {{Code Quality}}: {{A Reproduction Study}}},
  shorttitle = {On the {{Impact}} of {{Programming Languages}} on {{Code Quality}}},
  author = {Berger, Emery D. and Hollenbeck, Celeste and Maj, Petr and Vitek, Olga and Vitek, Jan},
  year = {2019},
  month = dec,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {41},
  number = {4},
  pages = {1--24},
  issn = {0164-0925, 1558-4593},
  doi = {10.1145/3340571},
  url = {https://dl.acm.org/doi/10.1145/3340571},
  urldate = {2023-12-27},
  abstract = {In a 2014 article, Ray, Posnett, Devanbu, and Filkov claimed to have uncovered a statistically significant association between 11 programming languages and software defects in 729 projects hosted on GitHub. Specifically, their work answered four research questions relating to software defects and programming languages. With data and code provided by the authors, the present article first attempts to conduct an experimental repetition of the original study. The repetition is only partially successful, due to missing code and issues with the classification of languages. The second part of this work focuses on their main claim, the association between bugs and languages, and performs a complete, independent reanalysis of the data and of the statistical modeling steps undertaken by Ray et al. in 2014. This reanalysis uncovers a number of serious flaws that reduce the number of languages with an association with defects down from 11 to only 4. Moreover, the practical effect size is exceedingly small. These results thus undermine the conclusions of the original study. Correcting the record is important, as many subsequent works have cited the 2014 article and have asserted, without evidence, a causal link between the choice of programming language for a given task and the number of software defects. Causation is not supported by the data at hand; and, in our opinion, even after fixing the methodological flaws we uncovered, too many unaccounted sources of bias remain to hope for a meaningful comparison of bug rates across languages.},
  langid = {english}
}
% == BibTeX quality report for bergerImpactProgrammingLanguages2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("ACM Trans. Program. Lang. Syst.")
% ? unused Library catalog ("DOI.org (Crossref)")

@incollection{bergeronSystemsProgrammingLanguages1972,
  title = {Systems {{Programming Languages}}},
  booktitle = {Advances in {{Computers}}},
  author = {Bergeron, R.D. and Gannon, J.D. and Shecter, D.P. and Tompa, F.W. and Dam, A. Van},
  year = {1972},
  volume = {12},
  pages = {175--284},
  publisher = {{Elsevier}},
  doi = {10.1016/S0065-2458(08)60510-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0065245808605100},
  urldate = {2023-12-23},
  isbn = {978-0-12-012112-0},
  langid = {english}
}
% == BibTeX quality report for bergeronSystemsProgrammingLanguages1972:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{bhatotiaIThreadsThreadingLibrary2015,
  title = {{{iThreads}}: A Threading Library for Parallel Incremental Computation},
  shorttitle = {{{iThreads}}},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}} - {{ASPLOS}} '15},
  author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Björn B. and Rodrigues, Rodrigo},
  year = {2015},
  pages = {645--659},
  publisher = {{ACM Press}},
  address = {{Istanbul, Turkey}},
  doi = {10.1145/2694344.2694371},
  url = {https://www.cs.purdue.edu/homes/pfonseca/papers/asplos2015-ithreads.pdf},
  urldate = {2020-10-25},
  isbn = {978-1-4503-2835-7},
  langid = {english}
}
% == BibTeX quality report for bhatotiaIThreadsThreadingLibrary2015:
% ? unused Conference name ("the Twentieth International Conference")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{billotStructureSharedForests1989,
  title = {The Structure of Shared Forests in Ambiguous Parsing},
  booktitle = {Proceedings of the 27th Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Billot, Sylvie and Lang, Bernard},
  year = {1989},
  month = jun,
  series = {{{ACL}} '89},
  pages = {143--151},
  publisher = {{Association for Computational Linguistics}},
  address = {{USA}},
  doi = {10.3115/981623.981641},
  url = {https://dl.acm.org/doi/10.3115/981623.981641},
  urldate = {2023-05-22},
  abstract = {The Context-Free backbone of some natural language analyzers produces all possible CF parses as some kind of shared forest, from which a single tree is to be chosen by a disambiguation process that may be based on the finer features of the language. We study the structure of these forests with respect to optimality of sharing, and in relation with the parsing schema used to produce them. In addition to a theoretical and experimental framework for studying these issues, the main results presented are:- sophistication in chart parsing schemata (e.g. use of look-ahed) may reduce time and space efficiency instead of improving it,- there is a shared forest structure with at most cubic size for any CF grammar,- when O(n3) complexity is required, the shape of a shared forest is dependent on the parsing schema used.Though analyzed on CF grammars for simplicity, these results extend to more complex formalisms such as unification based grammars.},
  keywords = {Ambiguity,Chart Parsing,Context-Free Parsing,Dynamic Programming,Earley Parsing,Parse Forest,Parse Tree,Parsing Schemata,Parsing Strategies}
}
% == BibTeX quality report for billotStructureSharedForests1989:
% ? unused Library catalog ("ACM Digital Library")

@article{birdMoreHasteLess1997,
  title = {More Haste, Less Speed: Lazy versus Eager Evaluation},
  shorttitle = {More Haste, Less Speed},
  author = {Bird, Richard and Jones, Geraint and Moor, Oege De},
  year = {1997},
  month = sep,
  journal = {Journal of Functional Programming},
  volume = {7},
  number = {5},
  pages = {541--547},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796897002827},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/more-haste-less-speed-lazy-versus-eager-evaluation/162B391CBCD864794C766CA2A2EC7CBE},
  urldate = {2022-01-06},
  abstract = {Nicholas Pippenger has recently given a problem that, under two simple restrictions, can be solved in linear time by an impure Lisp program, but requires Ω(n log n) steps to be solved by any eager pure Lisp program. By showing how to solve the problem in linear time with a lazy functional program, we demonstrate that – for some problems at least – lazy evaluators are strictly more powerful than eager ones.},
  langid = {english}
}

@inproceedings{blaisdellNonassociativeNoncommutativeMultimodal2022,
  title = {Non-Associative, Non-Commutative Multi-Modal Linear Logic},
  booktitle = {Automated {{Reasoning}}},
  author = {Blaisdell, Eben and Kanovich, Max and Kuznetsov, Stepan L. and Pimentel, Elaine and Scedrov, Andre},
  editor = {Blanchette, Jasmin and Kovács, Laura and Pattinson, Dirk},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {449--467},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-10769-6_27},
  abstract = {Adding multi-modalities (called subexponentials) to linear logic enhances its power as a logical framework, which has been extensively used in the specification of e.g. proof systems, programming languages and bigraphs. Initially, subexponentials allowed for classical, linear, affine or relevant behaviors. Recently, this framework was enhanced so to allow for commutativity as well. In this work, we close the cycle by considering associativity. We show that the resulting system (\$\$\textbackslash mathsf \{acLL\}\_\textbackslash varSigma \$\$acLLΣ) admits the (multi)cut rule, and we prove two undecidability results for fragments/variations of \$\$\textbackslash mathsf \{acLL\}\_\textbackslash varSigma \$\$acLLΣ.},
  isbn = {978-3-031-10769-6},
  langid = {english}
}
% == BibTeX quality report for blaisdellNonassociativeNoncommutativeMultimodal2022:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Springer Link")

@book{boehmSoftwareEngineeringEconomics1981,
  title = {Software Engineering Economics},
  author = {Boehm, Barry W.},
  year = {1981},
  publisher = {{Englewood Cliffs, N.J. : Prentice-Hall}},
  url = {http://archive.org/details/softwareengineer0000boeh},
  urldate = {2023-12-29},
  abstract = {xxvii, 767 p. : 25 cm. --; Includes indexes; Bibliography: p. 733-749},
  collaborator = {{Internet Archive}},
  isbn = {978-0-13-822122-5},
  langid = {english},
  keywords = {Software engineering -- Economic aspects}
}
% == BibTeX quality report for boehmSoftwareEngineeringEconomics1981:
% ? unused Library catalog ("Internet Archive")
% ? unused Number of pages ("812")

@inproceedings{bolingbrokeSupercompilationEvaluation2010,
  title = {Supercompilation by Evaluation},
  booktitle = {Proceedings of the Third {{ACM Haskell}} Symposium on {{Haskell}}},
  author = {Bolingbroke, Maximilian and Peyton Jones, Simon},
  year = {2010},
  month = sep,
  series = {Haskell '10},
  pages = {135--146},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863523.1863540},
  url = {https://www.microsoft.com/en-us/research/publication/supercompilation-by-evaluation/},
  urldate = {2021-03-24},
  abstract = {This paper shows how call-by-need supercompilation can be recast to be based explicitly on an evaluator, contrasting with standard presentations which are specified as algorithms that mix evaluation rules with reductions that are unique to supercompilation. Building on standard operational-semantics technology for call-by-need languages, we show how to extend the supercompilation algorithm to deal with recursive let expressions.},
  isbn = {978-1-4503-0252-4},
  langid = {american},
  keywords = {deforestation,haskell,optimisation,specialisation,supercompilation}
}
% == BibTeX quality report for bolingbrokeSupercompilationEvaluation2010:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@inproceedings{bolingbrokeTypesAreCalling2009,
  ids = {jonesTypesAreCalling2009},
  title = {Types Are Calling Conventions},
  booktitle = {Proceedings of the 2nd {{ACM SIGPLAN}} Symposium on {{Haskell}} - {{Haskell}} '09},
  author = {Bolingbroke, Maximilian C. and Peyton Jones, Simon L.},
  year = {2009},
  month = may,
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Edinburgh, Scotland}},
  doi = {10.1145/1596638.1596640},
  url = {https://www.microsoft.com/en-us/research/publication/types-are-calling-conventions/},
  urldate = {2020-06-15},
  abstract = {It is common for compilers to derive the calling convention of a function from its type. Doing so is simple and modular but misses many optimisation opportunities, particularly in lazy, higher-order functional languages with extensive use of currying. We restore the lost opportunities by defining Strict Core, a new intermediate language whose type system makes the missing distinctions: laziness is explicit, and functions take multiple arguments and return multiple results.},
  isbn = {978-1-60558-508-6},
  langid = {american}
}
% == BibTeX quality report for bolingbrokeTypesAreCalling2009:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 2nd ACM SIGPLAN symposium")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{borningClassesPrototypesObjectoriented1986,
  title = {Classes versus Prototypes in Object-Oriented Languages},
  booktitle = {Proceedings of 1986 {{ACM Fall}} Joint Computer Conference},
  author = {Borning, A. H.},
  year = {1986},
  month = nov,
  series = {{{ACM}} '86},
  pages = {36--40},
  publisher = {{IEEE Computer Society Press}},
  address = {{Washington, DC, USA}},
  urldate = {2023-03-02},
  isbn = {978-0-8186-4743-7}
}
% == BibTeX quality report for borningClassesPrototypesObjectoriented1986:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@article{boucherTrojanSourceInvisible,
  title = {Trojan Source: Invisible Vulnerabilities},
  author = {Boucher, Anderson},
  pages = {15},
  abstract = {We present a new type of attack in which source code is maliciously encoded so that it appears different to a compiler and to the human eye. This attack exploits subtleties in text-encoding standards such as Unicode to produce source code whose tokens are logically encoded in a different order from the one in which they are displayed, leading to vulnerabilities that cannot be perceived directly by human code reviewers. ‘Trojan Source’ attacks, as we call them, pose an immediate threat both to first-party software and of supply-chain compromise across the industry. We present working examples of Trojan-Source attacks in C, C++, C\#, JavaScript, Java, Rust, Go, and Python. We propose definitive compiler-level defenses, and describe other mitigating controls that can be deployed in editors, repositories, and build pipelines while compilers are upgraded to block this attack.},
  langid = {english}
}
% == BibTeX quality report for boucherTrojanSourceInvisible:
% Missing required field 'journal'
% Missing required field 'year'
% ? unused Library catalog ("Zotero")

@article{brachthauserParsingFirstclassDerivatives,
  ids = {brachthaeuserParsingFirstClassDerivatives},
  title = {Parsing with First-Class Derivatives},
  author = {Brachthauser, Jonathan Immanuel and Rendel, Tillmann and Ostermann, Klaus},
  pages = {19},
  abstract = {Brzozowski derivatives, well known in the context of regular expressions, have recently been rediscovered to give a simplified explanation to parsers of context-free languages. We add derivatives as a novel first-class feature to a standard parser combinator language. First-class derivatives enable an inversion of the control flow, allowing to implement modular parsers for languages that previously required separate preprocessing steps or cross-cutting modifications of the parsers. We show that our framework offers new opportunities for reuse and supports a modular definition of interesting use cases of layout-sensitive parsing.},
  langid = {english}
}
% == BibTeX quality report for brachthauserParsingFirstclassDerivatives:
% Missing required field 'journal'
% Missing required field 'year'
% ? unused Library catalog ("Zotero")

@article{brausseCDCLstyleCalculusSolving2019,
  title = {A {{CDCL-style}} Calculus for Solving Non-Linear Constraints},
  author = {Brauße, Franz and Korovin, Konstantin and Korovina, Margarita and Müller, Norbert Th},
  year = {2019},
  month = jul,
  journal = {arXiv:1905.09227 [cs]},
  eprint = {1905.09227},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1905.09227},
  urldate = {2020-07-25},
  abstract = {In this paper we propose a novel approach for checking satisfiability of non-linear constraints over the reals, called ksmt. The procedure is based on conflict resolution in CDCL-style calculus, using a composition of symbolical and numerical methods. To deal with the nonlinear components in case of conflicts we use numerically constructed restricted linearisations. This approach covers a large number of computable non-linear real functions such as polynomials, rational or trigonometrical functions and beyond. A prototypical implementation has been evaluated on several non-linear SMT-LIB examples and the results have been compared with state-of-the-art SMT solvers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Logic in Computer Science},
  note = {Comment: 17 pages, 3 figures; accepted at FroCoS 2019; software available at {$<$}http://informatik.uni-trier.de/\textasciitilde brausse/ksmt/{$>$}}
}
% == BibTeX quality report for brausseCDCLstyleCalculusSolving2019:
% ? Possibly abbreviated journal title arXiv:1905.09227 [cs]

@misc{brightProgrammingLanguageIdeas2022,
  title = {Programming Language Ideas That Work and Don't Work},
  year = {2022},
  month = jun,
  publisher = {{Code Europe}},
  url = {https://www.youtube.com/watch?v=y7KWGv_t-MU},
  urldate = {2023-07-19},
  abstract = {"Programming language ideas are a dime a dozen. I'm sure you have your favourite. "Why don't they implement X?". Some are obvious, some are obvious in retrospect, some obvious ideas turn out surprisingly to be very wrong, and some unexpected ideas turn out famously. I've designed two programming languages, ABEL and D. I've implemented compilers for them, along with C, C++, Javascript, and D. I've done the tech support for them. I hear a lot about what works and doesn't from the trenches. I'm sure you'll disagree with a number of my conclusions, but that's what makes it fun!"},
  collaborator = {Bright, Walter}
}
% == BibTeX quality report for brightProgrammingLanguageIdeas2022:
% ? unused Library catalog ("YouTube")
% ? unused Running time ("57:09")

@book{brooksMythicalManmonth1995,
  title = {The Mythical Man-Month},
  author = {Brooks, Jr., Frederick P.},
  year = {1995},
  month = aug,
  edition = {Anniversary},
  url = {http://archive.org/details/MythicalManMonth},
  urldate = {2023-12-30},
  abstract = {Few books on software project management have been as influential and timeless as~The Mythical Man-Month. With a blend of software engineering facts and thought-provoking opinions, Fred Brooks offers insight for anyone managing complex projects. These essays draw from his experience as project manager for the IBM System/360 computer family and then for OS/360, its massive software system. Now, 20 years after the initial publication of his book, Brooks has revisited his original ideas and added new thoughts and advice, both for readers already familiar with his work and for readers discovering it for the first time. ~ The added chapters contain (1) a crisp condensation of all the propositions asserted in the original book, including Brooks' central argument in~The Mythical Man-Month:~that large programming projects suffer management problems different from small ones due to the division of labor; that the conceptual integrity of the product is therefore critical; and that it is difficult but possible to achieve this unity; (2) Brooks' view of these propositions a generation later; (3) a reprint of his classic 1986 paper "No Silver Bullet"; and (4) today's thoughts on the 1986 assertion, "There will be no silver bullet within ten years."},
  langid = {english},
  keywords = {software}
}
% == BibTeX quality report for brooksMythicalManmonth1995:
% Missing required field 'publisher'
% ? unused Library catalog ("Internet Archive")

@article{bucciarelliGraphEasySets2016,
  title = {Graph Easy Sets of Mute Lambda Terms},
  author = {Bucciarelli, A. and Carraro, A. and Favro, G. and Salibra, A.},
  year = {2016},
  month = may,
  journal = {Theoretical Computer Science},
  series = {Theoretical {{Computer Science}} in {{Italy}}},
  volume = {629},
  pages = {51--63},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2015.12.024},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397515011858},
  urldate = {2022-07-01},
  abstract = {Among the unsolvable terms of the lambda calculus, the mute ones are those having the highest degree of undefinedness. In this paper, we define for each natural number n, an infinite and recursive set Mn of mute terms, and show that it is graph-easy: for any closed term t of the lambda calculus there exists a graph model equating all the terms of Mn to t. Alongside, we provide a brief survey of the notion of undefinedness in the lambda calculus.},
  langid = {english},
  keywords = {Forcing,Graph models,Lambda-calculus,Mute terms}
}
% == BibTeX quality report for bucciarelliGraphEasySets2016:
% ? unused Library catalog ("ScienceDirect")

@inproceedings{buseMetricSoftwareReadability2008,
  title = {A Metric for Software Readability},
  booktitle = {Proceedings of the 2008 International Symposium on {{Software}} Testing and Analysis - {{ISSTA}} '08},
  author = {Buse, Raymond P.L. and Weimer, Westley R.},
  year = {2008},
  pages = {121},
  publisher = {{ACM Press}},
  address = {{Seattle, WA, USA}},
  doi = {10.1145/1390630.1390647},
  url = {http://portal.acm.org/citation.cfm?doid=1390630.1390647},
  urldate = {2022-05-19},
  abstract = {In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80\% effective, and better than a human on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with two traditional measures of software quality, code changes and defect reports. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggests that comments, in of themselves, are less important than simple blank lines to local judgments of readability.},
  isbn = {978-1-60558-050-0},
  langid = {english}
}
% == BibTeX quality report for buseMetricSoftwareReadability2008:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 2008 international symposium")
% ? unused Library catalog ("DOI.org (Crossref)")

@phdthesis{byrdRelationalProgrammingMinikanren2009,
  title = {Relational Programming in Minikanren: {{Techniques}}, Applications, and Implementations},
  shorttitle = {Relational Programming in Minikanren},
  author = {Byrd, William E.},
  year = {2009},
  month = aug,
  url = {https://scholarworks.iu.edu/dspace/handle/2022/8777},
  urldate = {2022-07-14},
  abstract = {The promise of logic programming is that programs can be written
 								
 {$<$}italic{$>$}relationally{$<$}/italic{$>$}, without distinguishing between input
 								
 and output arguments.  Relational programs are remarkably
 								
 flexible\&mdash;for example, a relational type-inferencer also performs
 								
 type checking and type inhabitation, while a relational theorem prover
 								
 generates theorems as well as proofs and can even be used as a simple
 								
 proof assistant.
 								
 Unfortunately, writing relational programs is difficult, and requires
 								
 many interesting and unusual tools and techniques.  For example, a
 								
 relational interpreter for a subset of Scheme might use nominal
 								
 unification to support variable binding and scope, Constraint Logic
 								
 Programming over Finite Domains (CLP(FD)) to implement relational
 								
 arithmetic, and tabling to improve termination behavior.
 								
 In this dissertation I present {$<$}italic{$>$}miniKanren{$<$}/italic{$>$}, a family
 								
 of languages specifically designed for relational programming, and
 								
 which supports a variety of relational idioms and techniques.  I show
 								
 how miniKanren can be used to write interesting relational programs,
 								
 including an extremely flexible lean tableau theorem prover and a
 								
 novel constraint-free binary arithmetic system with strong termination
 								
 guarantees.  I also present interesting and practical techniques used
 								
 to implement miniKanren, including a nominal unifier that uses
 								
 triangular rather than idempotent substitutions and a novel
 								
 \&ldquo;walk\&rdquo;-based algorithm for variable lookup in triangular
 								
 substitutions.
 								
 The result of this research is a family of languages that supports a
 								
 variety of relational idioms and techniques, making it feasible and
 								
 useful to write interesting programs as relations.},
  copyright = {This work is licensed under the Creative Commons Attribution-By 3.0 Unported License},
  langid = {english},
  school = {Indiana University},
  annotation = {Accepted: 2010-06-16T17:43:40Z}
}
% == BibTeX quality report for byrdRelationalProgrammingMinikanren2009:
% ? unused Library catalog ("scholarworks.iu.edu")
% ? unused Type ("PhD")

@article{caiDistillingRealCost2022,
  title = {Distilling the Real Cost of Production Garbage Collectors},
  author = {Cai, Zixian and Blackburn, Stephen M and Bond, Michael D and Maas, Martin},
  year = {2022},
  journal = {IEEE International Symposium on Performance Analysis of Systems and Software},
  pages = {12},
  url = {https://users.cecs.anu.edu.au/~steveb/pubs/papers/lbo-ispass-2022.pdf},
  abstract = {Despite the long history of garbage collection (GC) and its prevalence in modern programming languages, there is surprisingly little clarity about its true cost. Without understanding their cost, crucial tradeoffs made by garbage collectors (GCs) go unnoticed. This can lead to misguided design constraints and evaluation criteria used by GC researchers and users, hindering the development of high-performance, low-cost GCs.},
  langid = {english}
}
% == BibTeX quality report for caiDistillingRealCost2022:
% ? unused Library catalog ("Zotero")

@article{castagnaCovarianceContravarianceFresh2020,
  title = {Covariance and Contravariance: A Fresh Look at an Old Issue (a Primer in Advanced Type Systems for Learning Functional Programmers)},
  shorttitle = {Covariance and {{Controvariance}}},
  author = {Castagna, Giuseppe},
  year = {2020},
  month = feb,
  journal = {arXiv:1809.01427 [cs]},
  volume = {16},
  number = {1},
  eprint = {1809.01427},
  primaryclass = {cs},
  doi = {10.23638/LMCS-16(1:15)2020},
  url = {http://arxiv.org/abs/1809.01427},
  urldate = {2020-06-22},
  abstract = {Twenty years ago, in an article titled "Covariance and contravariance: conflict without a cause", I argued that covariant and contravariant specialization of method parameters in object-oriented programming had different purposes and deduced that, not only they could, but actually they should both coexist in the same language. In this work I reexamine the result of that article in the light of recent advances in (sub-)typing theory and programming languages, taking a fresh look at this old issue. Actually, the revamping of this problem is just an excuse for writing an essay that aims at explaining sophisticated type-theoretic concepts, in simple terms and by examples, to undergraduate computer science students and/or willing functional programmers. Finally, I took advantage of this opportunity to describe some undocumented advanced techniques of type-systems implementation that are known only to few insiders that dug in the code of some compilers: therefore, even expert language designers and implementers may find this work worth of reading.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Programming Languages}
}
% == BibTeX quality report for castagnaCovarianceContravarianceFresh2020:
% ? Possibly abbreviated journal title arXiv:1809.01427 [cs]

@article{chenComputationalInterpretationCompact2021,
  title = {A Computational Interpretation of Compact Closed Categories: Reversible Programming with Negative and Fractional Types},
  shorttitle = {A Computational Interpretation of Compact Closed Categories},
  author = {Chen, Chao-Hong and Sabry, Amr},
  year = {2021},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {5},
  number = {POPL},
  pages = {1--29},
  issn = {2475-1421},
  doi = {10.1145/3434290},
  url = {https://dl.acm.org/doi/10.1145/3434290},
  urldate = {2021-08-04},
  abstract = {CHAO-HONG CHEN, Indiana University, USA AMR SABRY, Indiana University, USA Compact closed categories include objects representing higher-order functions and are well-established as models of linear logic, concurrency, and quantum computing. We show that it is possible to construct such compact closed categories for conventional sum and product types by defining a dual to sum types, a negative type, and a dual to product types, a fractional type. Inspired by the categorical semantics, we define a sound operational semantics for negative and fractional types in which a negative type represents a computational effect that “reverses execution flow” and a fractional type represents a computational effect that “garbage collects” particular values or throws exceptions. Specifically, we extend a first-order reversible language of type isomorphisms with negative and fractional types, specify an operational semantics for each extension, and prove that each extension forms a compact closed category. We furthermore show that both operational semantics can be merged using the standard combination of backtracking and exceptions resulting in a smooth interoperability of negative and fractional types. We illustrate the expressiveness of this combination by writing a reversible SAT solver that uses backtracking search along freshly allocated and de-allocated locations. The operational semantics, most of its meta-theoretic properties, and all examples are formalized in a supplementary Agda package. CCS Concepts: • Theory of computation → Type theory; Abstract machines; Operational semantics.},
  langid = {english}
}
% == BibTeX quality report for chenComputationalInterpretationCompact2021:
% ? unused Journal abbreviation ("Proc. ACM Program. Lang.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{clarkDepartmentDefenseSoftware2017,
  title = {Department of {{Defense}} Software Factbook},
  author = {Clark, Bradford and Miller, Christopher and {etc.}},
  year = {2017},
  month = apr,
  langid = {english}
}
% == BibTeX quality report for clarkDepartmentDefenseSoftware2017:
% Missing required field 'journal'
% ? unused Library catalog ("Zotero")

@article{clarkDoDSoftwareFactbook2015,
  title = {{{DoD}} Software Factbook},
  author = {Clark, Brad},
  year = {2015},
  langid = {english}
}
% == BibTeX quality report for clarkDoDSoftwareFactbook2015:
% Missing required field 'journal'
% ? unused Library catalog ("Zotero")

@incollection{clarkNegationFailure1978,
  title = {Negation as {{Failure}}},
  booktitle = {Logic and {{Data Bases}}},
  author = {Clark, Keith L.},
  editor = {Gallaire, Hervé and Minker, Jack},
  year = {1978},
  pages = {293--322},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4684-3384-5_11},
  url = {http://www.doc.ic.ac.uk/~klc/NegAsFailure.pdf},
  urldate = {2022-07-15},
  abstract = {A query evaluation process for a logic data base comprising a set of clauses is described. It is essentially a Horn clause theorem prover augmented with a special inference rule for dealing with negation. This is the negation as failure inference rule whereby \textasciitilde{} P can be inferred if every possible proof of P fails. The chief advantage of the query evaluator described is the effeciency with which it can be implemented. Moreover, we show that the negation as failure rule only allows us to conclude negated facts that could be inferred from the axioms of the completed data base, a data base of relation definitions and equality schemas that we consider is implicitly given by the data base of clauses. We also show that when the clause data base and the queries satisfy certain constraints, which still leaves us with a data base more general than a conventional relational data base, the query evaluation process will find every answer that is a logical consequence of the completed data base.},
  isbn = {978-1-4684-3384-5},
  langid = {english},
  keywords = {Data Base,Evaluation Tree,Inference Rule,Query Evaluation,Selection Rule}
}
% == BibTeX quality report for clarkNegationFailure1978:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Springer Link")

@inproceedings{clickPauselessGCAlgorithm2005,
  title = {The Pauseless {{GC}} Algorithm},
  booktitle = {Proceedings of the 1st {{ACM}}/{{USENIX}} International Conference on {{Virtual}} Execution Environments  - {{VEE}} '05},
  author = {Click, Cliff and Tene, Gil and Wolf, Michael},
  year = {2005},
  pages = {46},
  publisher = {{ACM Press}},
  address = {{Chicago, IL, USA}},
  doi = {10.1145/1064979.1064988},
  url = {https://static.usenix.org/events/vee05/full_papers/p46-click.pdf},
  urldate = {2021-04-25},
  abstract = {Modern transactional response-time sensitive applications have run into practical limits on the size of garbage collected heaps. The heap can only grow until GC pauses exceed the responsetime limits. Sustainable, scalable concurrent collection has become a feature worth paying for.},
  isbn = {978-1-59593-047-7},
  langid = {english}
}
% == BibTeX quality report for clickPauselessGCAlgorithm2005:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 1st ACM/USENIX international conference")
% ? unused Library catalog ("DOI.org (Crossref)")

@book{coburnChangeFunctionWhy2006,
  title = {The Change Function: Why Some Technologies Take off and Others Crash and Burn},
  shorttitle = {The Change Function},
  author = {Coburn, Pip},
  year = {2006},
  publisher = {{Penguin}},
  abstract = {The ultimate guide to predicting winners and losers in high technology  Pip Coburn became famous for writing some of the liveliest reports on Wall Street. He quoted everyone from Machiavelli to HAL, AnaÃ¯s Nin to Yoda, Einstein to Gandhi. But along with the quirky writing, he consistently delivered sharp insights into technology trends and helped investors pick stocks with long-term potential.  After years of studying countless winners and losers, Coburn has come up with a simple idea that explains why some technologies become huge hits (iPods, DVD players, Netflix), but others never reach more than a tiny audience (Segways, video phones, tablet PCs). He says that people are only willing to change when the pain of their current situation outweighs the perceived pain of trying something new.  In other words, technology demands a change in habits, and thatÂ\&\#39;s the leading cause of failure for countless cool inventions. Too many tech companies believe in Â“build it and they will comeÂ”Â— build something better and people will beat a path to your door. But, as Coburn shows, most potential users are afraid of new technologies, and they need a really great reason to change.  The Change Function is an irreverent look at how this pattern plays out in countless sectors, from computers to cell phones to digital TV recorders. It will be an invaluable book for people who create and invest in new technologies.},
  googlebooks = {0U1fx2OU6gYC},
  isbn = {978-1-59184-132-6},
  langid = {english},
  keywords = {Technology \& Engineering / Industrial Technology}
}
% == BibTeX quality report for coburnChangeFunctionWhy2006:
% ? unused Library catalog ("Google Books")
% ? unused Number of pages ("252")

@article{coniglioEqualityLinearLogic2002,
  ids = {coniglioEQUALITYLINEARLOGIC1996},
  title = {Equality in Linear Logic},
  author = {Coniglio, Marcelo and Miraglia, Francisco},
  year = {2002},
  month = jan,
  journal = {Logique et Analyse},
  volume = {39},
  number = {153/154},
  pages = {113--151},
  publisher = {{Peeters Publishers}},
  issn = {0024-5836},
  url = {https://www.researchgate.net/profile/Marcelo-Coniglio/publication/2387274_Equality_In_Linear_Logic/links/0deec5165a6babbd8a000000/Equality-In-Linear-Logic.pdf},
  abstract = {reference is [Ros]). Quantales were introduced by Mulvey ([Mul]) as an algebraic tool for studying representations of non-commutative C -algebras. Informally, a quantale is a complete lattice Q equipped with a product distributive over arbitrary sup's. The importance of quantales for Linear Logic is revealed in Yetter's work ([Yet]), who proved that semantics of classical linear logic is given by a class of quantales, named Girard quantales, which coincides with Girard's phase semantics. An analogous result is obtained for a sort of non-commutative linear logic, as well as intuitionistic linear logic without negation, which suggest that the utilisation of the theory of quantales (or even weaker structures, such that *-autonomous posets) might be fruitful in studying the semantic of several variants of linear logic. As usual, we denote the order in a lattice by , while W and V denote the operatio}
}
% == BibTeX quality report for coniglioEqualityLinearLogic2002:
% ? unused Library catalog ("JSTOR")

@phdthesis{cookDenotationalSemanticsInheritance1989,
  title = {A Denotational Semantics of Inheritance},
  author = {Cook, William R},
  year = {1989},
  month = may,
  url = {https://www.cs.utexas.edu/~wcook/papers/thesis/cook89.pdf},
  langid = {english},
  school = {Brown University}
}
% == BibTeX quality report for cookDenotationalSemanticsInheritance1989:
% ? unused Library catalog ("Zotero")
% ? unused Type ("PhD")

@inproceedings{cookInheritanceNotSubtyping1989,
  title = {Inheritance Is Not Subtyping},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Cook, William R. and Hill, Walter and Canning, Peter S.},
  year = {1989},
  month = dec,
  series = {{{POPL}} '90},
  pages = {125--135},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/96709.96721},
  url = {https://doi.org/10.1145/96709.96721},
  urldate = {2022-12-15},
  abstract = {In typed object-oriented languages the subtype relation is typically based on the inheritance hierarchy. This approach, however, leads either to insecure type-systems or to restrictions on inheritance that make it less flexible than untyped Smalltalk inheritance. We present a new typed model of inheritance that allows more of the flexibility of Smalltalk inheritance within a statically-typed system. Significant features of our analysis are the introduction of polymorphism into the typing of inheritance and the uniform application of inheritance to objects, classes and types. The resulting notion of type inheritance allows us to show that the type of an inherited object is an inherited type but not always a subtype.},
  isbn = {978-0-89791-343-0}
}
% == BibTeX quality report for cookInheritanceNotSubtyping1989:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@article{cookUnderstandingDataAbstraction2009,
  title = {On Understanding Data Abstraction, Revisited},
  author = {Cook, William R},
  year = {2009},
  abstract = {In 1985 Luca Cardelli and Peter Wegner, my advisor, published an ACM Computing Surveys paper called “On understanding types, data abstraction, and polymorphism”. Their work kicked off a flood of research on semantics and type theory for object-oriented programming, which continues to this day. Despite 25 years of research, there is still widespread confusion about the two forms of data abstraction, abstract data types and objects. This essay attempts to explain the differences and also why the differences matter.},
  langid = {english}
}
% == BibTeX quality report for cookUnderstandingDataAbstraction2009:
% Missing required field 'journal'
% ? unused Library catalog ("Zotero")

@phdthesis{coppolaComplexityOptimalReduction2002,
  title = {On the Complexity of Optimal Reduction of Functional Programming Languages},
  author = {Coppola, Paolo},
  year = {2002},
  month = feb,
  collaborator = {Martini, Simone},
  langid = {english},
  school = {Università degli Studi di Udine}
}
% == BibTeX quality report for coppolaComplexityOptimalReduction2002:
% ? unused Library catalog ("Zotero")

@phdthesis{corbynPracticalStaticMemory2020,
  type = {Bachelor's Thesis},
  title = {Practical Static Memory Management},
  author = {Corbyn, Nathan},
  year = {2020},
  month = may,
  url = {http://nathancorbyn.com/nc513.pdf},
  langid = {english},
  school = {King’s College}
}
% == BibTeX quality report for corbynPracticalStaticMemory2020:
% ? unused Library catalog ("Zotero")
% ? unused Number of pages ("57")

@misc{coxVersionSAT2016,
  title = {Version {{SAT}}},
  author = {Cox, Russ},
  year = {2016},
  month = dec,
  journal = {research!rsc},
  url = {https://research.swtch.com/version-sat},
  urldate = {2021-01-26}
}
% == BibTeX quality report for coxVersionSAT2016:
% ? Title looks like it was stored in title-case in Zotero

@article{crolardFormulaeastypesInterpretationSubtractive2004,
  title = {A Formulae-as-Types Interpretation of Subtractive Logic},
  author = {Crolard, Tristan},
  year = {2004},
  month = aug,
  journal = {Journal of Logic and Computation},
  volume = {14},
  number = {4},
  pages = {529--570},
  publisher = {{Oxford Academic}},
  issn = {0955-792X},
  doi = {10.1093/logcom/14.4.529},
  url = {https://academic.oup.com/logcom/article/14/4/529/933555},
  urldate = {2020-06-18},
  abstract = {Abstract.  We present a formulae-as-types interpretation of Subtractive Logic (i.e. bi-intuitionistic logic). This presentation is two-fold: we first define a v},
  langid = {english}
}
% == BibTeX quality report for crolardFormulaeastypesInterpretationSubtractive2004:
% ? unused Journal abbreviation ("J Logic Computation")
% ? unused Library catalog ("academic.oup.com")

@article{CrossTalkCostEstimation2005,
  title = {{{CrossTalk}}: Cost Estimation},
  year = {2005},
  month = apr,
  volume = {18},
  number = {4},
  url = {https://apps.dtic.mil/sti/pdfs/ADA487403.pdf},
  urldate = {2023-12-26}
}
% == BibTeX quality report for CrossTalkCostEstimation2005:
% Missing required field 'author'
% Missing required field 'journal'

@misc{dahlCommonBaseLanguage1970,
  title = {Common {{Base Language}}},
  author = {Dahl, Ole-Johan and Myhrhaug, Bjørn and Nygaard, Kristen},
  year = {1970},
  month = oct,
  publisher = {{Norwegian Computing Center}},
  url = {https://www.ics.uci.edu/~jajones/INF102-S18/readings/10_Simula.pdf},
  urldate = {2022-12-20}
}
% == BibTeX quality report for dahlCommonBaseLanguage1970:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{danosStructureExponentialsUncovering1993,
  title = {The Structure of Exponentials: Uncovering the Dynamics of Linear Logic Proofs},
  shorttitle = {The Structure of Exponentials},
  booktitle = {Proceedings of the {{Third Kurt Gödel Colloquium}} on {{Computational Logic}} and {{Proof Theory}}},
  author = {Danos, Vincent and Joinet, Jean-Baptiste and Schellinx, Harold},
  editor = {Gottlob, G. and Leitsch, A. and Mundici, D.},
  year = {1993},
  month = aug,
  pages = {159--171},
  publisher = {{Springer-Verlag LNCS 348}},
  address = {{Brno, Czech Republic}},
  url = {https://eprints.illc.uva.nl/1334/},
  langid = {english}
}
% == BibTeX quality report for danosStructureExponentialsUncovering1993:
% ? unused Library catalog ("eprints.illc.uva.nl")

@article{davidsonDesignApplicationRetargetable1980,
  title = {The Design and Application of a Retargetable Peephole Optimizer},
  author = {Davidson, Jack W. and Fraser, Christopher W.},
  year = {1980},
  month = apr,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {2},
  number = {2},
  pages = {191--202},
  issn = {0164-0925, 1558-4593},
  doi = {10.1145/357094.357098},
  url = {https://dl.acm.org/doi/10.1145/357094.357098},
  urldate = {2021-07-30},
  abstract = {Peephole optimizers improve object code by replacing certain sequences of instructions with better sequences. This paper describes PO, a peephole optimizer that uses a symbolic machine description to simulate pairs of adjacent instructions, replacing them, where possible, with an equivalent sing!e instruction. As a result of this organization, PO is machine independent and can be described formally and concisely: when PO is finished, no instruction, and no pair of adjacent instructions, can be replaced with a cheaper single instruction that has the same effect. This thoroughness allows PO to relieve code generators of much case analysis; for example, they might produce only load/add-register sequences and rely on PO to, where possible, discard them in favor of add-memory, add-immediate, or increment instructions. Experiments indicate that naive code generators can give good code if used with PO.},
  langid = {english}
}
% == BibTeX quality report for davidsonDesignApplicationRetargetable1980:
% ? unused Journal abbreviation ("ACM Trans. Program. Lang. Syst.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{delawareNarcissusCorrectbyconstructionDerivation2019,
  title = {Narcissus: Correct-by-Construction Derivation of Decoders and Encoders from Binary Formats},
  shorttitle = {Narcissus},
  author = {Delaware, Benjamin and Suriyakarn, Sorawit and {Pit-Claudel}, Clément and Ye, Qianchuan and Chlipala, Adam},
  year = {2019},
  month = jul,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {ICFP},
  pages = {1--29},
  issn = {2475-1421, 2475-1421},
  doi = {10.1145/3341686},
  url = {https://www.cs.purdue.edu/homes/bendy/Narcissus/narcissus.pdf},
  urldate = {2020-07-26},
  langid = {english}
}
% == BibTeX quality report for delawareNarcissusCorrectbyconstructionDerivation2019:
% ? unused Journal abbreviation ("Proc. ACM Program. Lang.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{deloreyProgrammingLanguagesAffect2007,
  title = {Do Programming Languages Affect Productivity? {{A}} Case Study Using Data from Open Source Projects},
  shorttitle = {Do Programming Languages Affect Productivity?},
  booktitle = {First {{International Workshop}} on {{Emerging Trends}} in {{FLOSS Research}} and {{Development}}, {{FLOSS}}'07},
  author = {Delorey, Daniel and Knutson, Charles and Chun, Scott},
  year = {2007},
  month = jun,
  pages = {8--8},
  doi = {10.1109/FLOSS.2007.5},
  abstract = {Brooks and others long ago suggested that on average computer programmers write the same number of lines of code in a given amount of time regardless of the programming language used. We examine data collected from the CVS repositories of 9,999 open source projects hosted on SourceForge.net to test this assumption for 10 of the most popular programming languages in use in the open source community. We find that for 24 of the 45 pairwise comparisons, the programming language is a significant factor in determining the rate at which source code is written, even after accounting for variations between programmers and projects.},
  isbn = {978-0-7695-2961-5}
}
% == BibTeX quality report for deloreyProgrammingLanguagesAffect2007:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ResearchGate")

@article{dershowitzRewriteRewriteRewrite1991,
  title = {Rewrite, Rewrite, Rewrite, Rewrite, Rewrite, …},
  author = {Dershowitz, Nachum and Kaplan, Stéphane and Plaisted, David A.},
  year = {1991},
  month = jun,
  journal = {Theoretical Computer Science},
  volume = {83},
  number = {1},
  pages = {71--96},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(91)90040-9},
  url = {https://www.sciencedirect.com/science/article/pii/0304397591900409},
  urldate = {2022-07-02},
  abstract = {We study properties of rewrite systems that are not necessarily terminating, but allow instead for transfinite derivations that have a limit. In particular, we give conditions for the existence of a limit and for its uniqueness and relate the operational and algebraic semantics of infinitary theories. We also consider sufficient completeness of hierarchical systems.},
  langid = {english}
}
% == BibTeX quality report for dershowitzRewriteRewriteRewrite1991:
% ? unused Library catalog ("ScienceDirect")

@incollection{dershowitzRewriteSystems1991,
  title = {Rewrite Systems},
  booktitle = {Handbook of Theoretical Computer Science (Vol. {{B}}): Formal Models and Semantics},
  author = {Dershowitz, Nachum and Jouannaud, Jean-Pierre},
  year = {1991},
  month = jan,
  pages = {243--320},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  url = {https://www.cs.tau.ac.il/~nachum/papers/survey-draft.pdf},
  urldate = {2022-02-25},
  isbn = {978-0-444-88074-1}
}
% == BibTeX quality report for dershowitzRewriteSystems1991:
% ? unused Library catalog ("ACM Digital Library")

@book{dicosmoIntroductionLinearLogic2015,
  title = {Introduction to Linear Logic},
  author = {Di Cosmo, Roberto},
  year = {2015},
  publisher = {{MPRI course notes}},
  url = {https://www.dicosmo.org/CourseNotes/LinLog/IntroductionLinearLogic.pdf},
  urldate = {2020-05-13}
}
% == BibTeX quality report for dicosmoIntroductionLinearLogic2015:
% ? unused Number of pages ("86")

@phdthesis{dolanAlgebraicSubtyping2016,
  title = {Algebraic Subtyping},
  author = {Dolan, Stephen},
  year = {2016},
  month = sep,
  url = {https://www.cs.tufts.edu/~nr/cs257/archive/stephen-dolan/thesis.pdf},
  abstract = {Type inference gives programmers the benefit of static, compile-time type checking without the cost of manually specifying types, and has long been a standard feature of functional programming languages. However, it has proven difficult to integrate type inference with subtyping, since the unification engine at the core of classical type inference accepts only equations, not subtyping constraints.},
  langid = {english},
  school = {University of Cambridge}
}
% == BibTeX quality report for dolanAlgebraicSubtyping2016:
% ? unused Library catalog ("Zotero")
% ? unused Number of pages ("157")

@inproceedings{dolanPolymorphismSubtypingType2017,
  title = {Polymorphism, Subtyping, and Type Inference in {{MLsub}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Dolan, Stephen and Mycroft, Alan},
  year = {2017},
  month = jan,
  series = {{{POPL}} 2017},
  pages = {60--72},
  publisher = {{Association for Computing Machinery}},
  address = {{Paris, France}},
  doi = {10.1145/3009837.3009882},
  url = {https://doi.org/10.1145/3009837.3009882},
  urldate = {2020-06-15},
  abstract = {We present a type system combining subtyping and ML-style parametric polymorphism. Unlike previous work, our system supports type inference and has compact principal types. We demonstrate this system in the minimal language MLsub, which types a strict superset of core ML programs. This is made possible by keeping a strict separation between the types used to describe inputs and those used to describe outputs, and extending the classical unification algorithm to handle subtyping constraints between these input and output types. Principal types are kept compact by type simplification, which exploits deep connections between subtyping and the algebra of regular languages. An implementation is available online.},
  isbn = {978-1-4503-4660-3},
  keywords = {Algebra,Polymorphism,Subtyping,Type Inference}
}
% == BibTeX quality report for dolanPolymorphismSubtypingType2017:
% ? unused Library catalog ("ACM Digital Library")

@inproceedings{dossantosImpactsCodingPractices2018,
  title = {Impacts of Coding Practices on Readability},
  booktitle = {Proceedings of the 26th {{Conference}} on {{Program Comprehension}}},
  author = {{dos Santos}, Rodrigo Magalhães and Gerosa, Marco Aurélio},
  year = {2018},
  month = may,
  pages = {277--285},
  publisher = {{ACM}},
  address = {{Gothenburg Sweden}},
  doi = {10.1145/3196321.3196342},
  url = {https://dl.acm.org/doi/10.1145/3196321.3196342},
  urldate = {2022-05-18},
  abstract = {Several conventions and standards aim to improve maintainability of software code. However, low levels of code readability perceived by developers still represent a barrier to their daily work. In this paper, we describe a survey that assessed the impact of a set of Java coding practices on the readability perceived by software developers. While some practices promoted an enhancement of readability, others did not show statistically significant effects. Interestingly, one of the practices worsened the readability. Our results may help to identify coding conventions with a positive impact on readability and, thus, guide the creation of coding standards.},
  isbn = {978-1-4503-5714-2},
  langid = {english}
}
% == BibTeX quality report for dossantosImpactsCodingPractices2018:
% ? unused Conference name ("ICSE '18: 40th International Conference on Software Engineering")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{downenMakingFasterCurry2019,
  ids = {downenMakingFasterCurry2019a},
  title = {Making a Faster Curry with Extensional Types},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Symposium}} on {{Haskell}}},
  author = {Downen, Paul and Sullivan, Zachary and Ariola, Zena M. and Peyton Jones, Simon},
  year = {2019},
  month = aug,
  series = {Haskell 2019},
  pages = {58--70},
  publisher = {{Association for Computing Machinery}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3331545.3342594},
  url = {https://www.microsoft.com/en-us/research/publication/making-a-faster-curry-with-extensional-types/},
  urldate = {2020-06-14},
  abstract = {Curried functions apparently take one argument at a time, which is slow. So optimizing compilers for higher-order languages invariably have some mechanism for working around currying by passing several arguments at once, as many as the function can handle, which is known as its arity. But such mechanisms are often ad-hoc, and do not work at all in higher-order functions. We show how extensional, call-by-name functions have the correct behavior for directly expressing the arity of curried functions. And these extensional functions can stand side-by-side with functions native to practical programming languages, which do not use call-by-name evaluation. Integrating call-by-name with other evaluation strategies in the same intermediate language expresses the arity of a function in its type and gives a principled and compositional account of multi-argument curried functions. An unexpected, but significant, bonus is that our approach is equally suitable for a call-by-value language and a call-by-need language, and it can be readily integrated into an existing compilation framework.},
  isbn = {978-1-4503-6813-1},
  langid = {american},
  keywords = {arity,extensionality,type systems}
}
% == BibTeX quality report for downenMakingFasterCurry2019:
% ? unused Library catalog ("ACM Digital Library")

@inproceedings{downenSequentCalculusCompiler2016,
  ids = {downenSequentCalculusCompiler},
  title = {Sequent Calculus as a Compiler Intermediate Language},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Downen, Paul and Maurer, Luke and Ariola, Zena M. and Peyton Jones, Simon},
  year = {2016},
  month = sep,
  series = {{{ICFP}} 2016},
  pages = {74--88},
  publisher = {{Association for Computing Machinery}},
  address = {{Nara, Japan}},
  doi = {10.1145/2951913.2951931},
  url = {https://www.microsoft.com/en-us/research/publication/sequent-calculus-as-a-compiler-intermediate-language/},
  urldate = {2020-06-14},
  abstract = {The λ-calculus is popular as an intermediate language for practical compilers. But in the world of logic it has a lesser-known twin, born at the same time, called the sequent calculus. Perhaps that would make for a good intermediate language, too? To explore this question we designed Sequent Core, a practically-oriented core calculus based on the sequent calculus, and used it to re-implement a substantial chunk of the Glasgow Haskell Compiler.},
  isbn = {978-1-4503-4219-3},
  keywords = {Compiler optimizations,Continuations,Haskell,Intermediate representations,Natural deduction,Sequent calculus}
}
% == BibTeX quality report for downenSequentCalculusCompiler2016:
% ? unused Library catalog ("ACM Digital Library")

@inproceedings{dsilvaConflictdrivenConditionalTermination2015,
  title = {Conflict-Driven Conditional Termination},
  booktitle = {Computer Aided Verification},
  author = {D’Silva, Vijay and Urban, Caterina},
  editor = {Kroening, Daniel and Păsăreanu, Corina S.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {271--286},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-21668-3_16},
  url = {https://link.springer.com/content/pdf/10.1007%2F978-3-319-21668-3_16.pdf},
  abstract = {Conflict-driven learning, which is essential to the performance of sat and smt solvers, consists of a procedure that searches for a model of a formula, and refutation procedure for proving that no model exists. This paper shows that conflict-driven learning can improve the precision of a termination analysis based on abstract interpretation. We encode non-termination as satisfiability in a monadic second-order logic and use abstract interpreters to reason about the satisfiability of this formula. Our search procedure combines decisions with reachability analysis to find potentially non-terminating executions and our refutation procedure uses a conditional termination analysis. Our implementation extends the set of conditional termination arguments discovered by an existing termination analyzer.},
  isbn = {978-3-319-21668-3},
  langid = {english},
  keywords = {Abstract Domain,Conflict Analysis,Ranking Function,Reachability Analysis,Trace Formula}
}
% == BibTeX quality report for dsilvaConflictdrivenConditionalTermination2015:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Springer Link")

@article{dunfieldBidirectionalTyping2019,
  title = {Bidirectional Typing},
  author = {Dunfield, Joshua and Krishnaswami, Neel},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.05839 [cs]},
  eprint = {1908.05839},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1908.05839},
  urldate = {2020-06-22},
  abstract = {Bidirectional typing combines two modes of typing: type checking, which checks that a program satisfies a known type, and type synthesis, which determines a type from the program. Using checking enables bidirectional typing to break the decidability barrier of Damas-Milner approaches; using synthesis enables bidirectional typing to avoid the large annotation burden of explicitly typed languages. In addition, bidirectional typing improves error locality. We highlight the design principles that underlie bidirectional type systems, survey the development of bidirectional typing from the prehistoric period before Pierce and Turner's local type inference to the present day, and provide guidance for future investigations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  note = {Comment: 34 pages; submitted to ACM Computing Surveys}
}
% == BibTeX quality report for dunfieldBidirectionalTyping2019:
% ? Possibly abbreviated journal title arXiv:1908.05839 [cs]

@article{dyvbigMonadicFrameworkDelimited2007,
  ids = {dyvbigMonadicFrameworkDelimited2007a},
  title = {A Monadic Framework for Delimited Continuations},
  author = {Dyvbig, R. Kent and Peyton Jones, Simon and Sabry, Amr},
  year = {2007},
  month = nov,
  journal = {Journal of Functional Programming},
  volume = {17},
  number = {6},
  pages = {687--730},
  issn = {0956-7968},
  doi = {10.1017/S0956796807006259},
  url = {https://doi.org/10.1017/S0956796807006259},
  urldate = {2020-06-19},
  abstract = {Delimited continuations are more expressive than traditional abortive continuations and they apparently require a framework beyond traditional continuation-passing style (CPS). We show that this is not the case: standard CPS is sufficient to explain the common control operators for delimited continuations. We demonstrate this fact and present an implementation as a Scheme library. We then investigate a typed account of delimited continuations that makes explicit where control effects can occur. This results in a monadic framework for typed and encapsulated delimited continuations, which we design and implement as a Haskell library.}
}
% == BibTeX quality report for dyvbigMonadicFrameworkDelimited2007:
% ? unused Journal abbreviation ("J. Funct. Program.")
% ? unused Library catalog ("November 2007")

@incollection{economopoulosFasterScannerlessGLR2009,
  title = {Faster Scannerless {{GLR}} Parsing},
  booktitle = {Compiler {{Construction}}},
  author = {Economopoulos, Giorgios and Klint, Paul and Vinju, Jurgen},
  editor = {{de Moor}, Oege and Schwartzbach, Michael I.},
  year = {2009},
  volume = {5501},
  pages = {126--141},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-00722-4_10},
  url = {http://link.springer.com/10.1007/978-3-642-00722-4_10},
  urldate = {2020-06-15},
  abstract = {Analysis and renovation of large software portfolios requires syntax analysis of multiple, usually embedded, languages and this is beyond the capabilities of many standard parsing techniques. The traditional separation between lexer and parser falls short due to the limitations of tokenization based on regular expressions when handling multiple lexical grammars. In such cases scannerless parsing provides a viable solution. It uses the power of context-free grammars to be able to deal with a wide variety of issues in parsing lexical syntax. However, it comes at the price of less efficiency. The structure of tokens is obtained using a more powerful but more time and memory intensive parsing algorithm. Scannerless grammars are also more non-deterministic than their tokenized counterparts, increasing the burden on the parsing algorithm even further.},
  isbn = {978-3-642-00721-7 978-3-642-00722-4},
  langid = {english}
}
% == BibTeX quality report for economopoulosFasterScannerlessGLR2009:
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{eggerEnrichedEffectCalculus2014,
  title = {The Enriched Effect Calculus: Syntax and Semantics},
  shorttitle = {The Enriched Effect Calculus},
  author = {Egger, J. and Mogelberg, R. E. and Simpson, A.},
  year = {2014},
  month = jun,
  journal = {Journal of Logic and Computation},
  volume = {24},
  number = {3},
  pages = {615--654},
  issn = {0955-792X, 1465-363X},
  doi = {10.1093/logcom/exs025},
  url = {https://academic.oup.com/logcom/article-lookup/doi/10.1093/logcom/exs025},
  urldate = {2021-11-09},
  abstract = {This paper introduces the enriched effect calculus, which extends established type theories for computational effects with primitives from linear logic. The new calculus provides a formalism for expressing linear aspects of computational effects; for example, the linear usage of imperative features such as state and/or continuations.},
  langid = {english}
}
% == BibTeX quality report for eggerEnrichedEffectCalculus2014:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{elemamConfoundingEffectClass2001,
  title = {The Confounding Effect of Class Size on the Validity of Object-Oriented Metrics},
  author = {El Emam, K. and Benlarbi, S. and Goel, N. and Rai, S.N.},
  year = {2001},
  month = jul,
  journal = {IEEE Transactions on Software Engineering},
  volume = {27},
  number = {7},
  pages = {630--650},
  issn = {00985589},
  doi = {10.1109/32.935855},
  url = {http://ieeexplore.ieee.org/document/935855/},
  urldate = {2023-02-24},
  abstract = {ÐMuch effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. In this paper, we demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies: The metrics that are expected to be validated are indeed associated with fault-proneness. After controlling for size, none of the metrics we studied were associated with fault-proneness anymore. This demonstrates a strong size confounding effect and casts doubt on the results of previous object-oriented metrics validation studies. It is recommended that previous validation studies be reexamined to determine whether their conclusions would still hold after controlling for size and that future validation studies should always control for size.},
  langid = {english}
}
% == BibTeX quality report for elemamConfoundingEffectClass2001:
% ? unused Journal abbreviation ("IIEEE Trans. Software Eng.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{elizarovKotlinCoroutinesDesign2021,
  title = {Kotlin Coroutines: Design and Implementation},
  shorttitle = {Kotlin Coroutines},
  booktitle = {Proceedings of the 2021 {{ACM SIGPLAN International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}},
  author = {Elizarov, Roman and Belyaev, Mikhail and Akhin, Marat and Usmanov, Ilmir},
  year = {2021},
  month = oct,
  series = {Onward! 2021},
  pages = {68--84},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3486607.3486751},
  url = {https://doi.org/10.1145/3486607.3486751},
  urldate = {2022-11-29},
  abstract = {Asynchronous programming is having its “renaissance” moment in recent years. Created in the 1980s, it was in use for quite some time, but with the advent of multi-core processors, it has been sidestepped by multi-threaded programming, which was (for a long time) the de facto standard of performing concurrent computations. However, since the 2000s, more and more programming languages have begun to include the support for asynchronous programming, some built around asynchronicity from the start, others including it later in their evolution. In this paper, we explore the design and implementation of asynchronous programming in Kotlin, a multiplatform programming language from JetBrains, which uses coroutines for asynchronicity. Kotlin provides a compact built-in API for coroutine support, thus giving a lot of implementation freedom to the developer; this flexibility allows to transparently support different flavours of asynchronous programming within the same language. We overview existing approaches to asynchronous programming, zoom in and talk about coroutines in detail, and describe how they are used in Kotlin as the basis for asynchronous computations. Along the way, we show the flexibility of Kotlin coroutines, highlight several existing problems with asynchronicity, how they are fixed or worked-around in Kotlin, and also mention future directions asynchronous programming might explore.},
  isbn = {978-1-4503-9110-8},
  keywords = {asynchronous programming,continuations,coroutines,Kotlin,language design}
}
% == BibTeX quality report for elizarovKotlinCoroutinesDesign2021:
% ? unused Library catalog ("ACM Digital Library")

@article{endrullisCoinductiveFoundationsInfinitary2018,
  ids = {endrullisCOINDUCTIVEFOUNDATIONSINFINITARY},
  title = {Coinductive Foundations of Infinitary Rewriting and Infinitary Equational Logic},
  author = {Endrullis, Jörg and Hansen, Helle Hvid and Hendriks, Dimitri and Polonsky, Andrew and Silva, Alexandra},
  year = {2018},
  month = jan,
  eprint = {1706.00677},
  primaryclass = {cs},
  pages = {44},
  doi = {10.23638/LMCS-14(1:3)2018},
  url = {http://arxiv.org/abs/1706.00677},
  urldate = {2022-10-23},
  abstract = {We present a coinductive framework for defining and reasoning about the infinitary analogues of equational logic and term rewriting in a uniform, coinductive way. The setup captures rewrite sequences of arbitrary ordinal length, but it has neither the need for ordinals nor for metric convergence. This makes the framework especially suitable for formalizations in theorem provers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Logic in Computer Science},
  note = {Comment: arXiv admin note: substantial text overlap with arXiv:1505.01128, arXiv:1306.6224}
}
% == BibTeX quality report for endrullisCoinductiveFoundationsInfinitary2018:
% Missing required field 'journal'

@article{endrullisCoinductiveTreatmentInfinitary2013,
  title = {A Coinductive Treatment of Infinitary Term Rewriting},
  author = {Endrullis, J. and Hansen, H. H. and Hendriks, D. and Polonsky, A. and Silva, A.},
  year = {2013},
  journal = {16},
  publisher = {{[S.l.] : [S.n.]}},
  url = {https://repository.ubn.ru.nl/handle/2066/122925},
  urldate = {2022-07-27},
  abstract = {Workshop on Infinitary Rewriting 2013 Friday June 28, Eindhoven, The Netherlands},
  langid = {english},
  annotation = {Accepted: 2014-01-24T22:35:30Z}
}
% == BibTeX quality report for endrullisCoinductiveTreatmentInfinitary2013:
% ? unused Library catalog ("repository.ubn.ru.nl")

@article{endrullisHighlightsInfinitaryRewriting2012,
  ids = {endrullisHighlightsInfinitaryRewriting2012a},
  title = {Highlights in Infinitary Rewriting and Lambda Calculus},
  author = {Endrullis, Jörg and Hendriks, Dimitri and Klop, Jan Willem},
  year = {2012},
  month = dec,
  journal = {Theoretical Computer Science},
  series = {New {{Directions}} in {{Rewriting}} ({{Honoring}} the 60th {{Birthday}} of {{Yoshihito Toyama}})},
  volume = {464},
  pages = {48--71},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2012.08.018},
  url = {https://www.sciencedirect.com/science/article/pii/S030439751200792X},
  urldate = {2022-07-02},
  abstract = {We present some highlights from the emerging theory of infinitary rewriting, both for first-order term rewriting systems and λ-calculus. In the first section we introduce the framework of infinitary rewriting for first-order rewrite systems, so without bound variables. We present a recent observation concerning the continuity of infinitary rewriting. In the second section we present an excursion to the infinitary λ-calculus. After the main definitions, we mention a recent observation about infinite looping λ-terms, that is, terms that reduce in one step to themselves. Next we describe the fundamental trichotomy in the semantics of λ-calculus: Böhm trees, Lévy–Longo trees, and Berarducci trees. We conclude with a short description of a new refinement of Böhm tree semantics, called clocked semantics.},
  langid = {english}
}
% == BibTeX quality report for endrullisHighlightsInfinitaryRewriting2012:
% ? unused Library catalog ("ScienceDirect")

@article{endrullisInfinitaryTermRewriting2014,
  title = {Infinitary Term Rewriting for Weakly Orthogonal Systems: Properties and Counterexamples},
  shorttitle = {Infinitary Term Rewriting for Weakly Orthogonal Systems},
  author = {Endrullis, Jörg and Grabmayer, Clemens and Hendriks, Dimitri and Klop, Jan Willem and Oostrom, Vincent},
  editor = {Lynch, Christopher},
  year = {2014},
  month = jun,
  journal = {Logical Methods in Computer Science},
  volume = {10},
  number = {2},
  pages = {7},
  issn = {18605974},
  doi = {10.2168/LMCS-10(2:7)2014},
  url = {https://lmcs.episciences.org/752},
  urldate = {2022-07-10},
  abstract = {We present some contributions to the theory of infinitary rewriting for weakly orthogonal term rewrite systems, in which critical pairs may occur provided they are trivial. We show that the infinitary unique normal form property (UN∞) fails by an example of a weakly orthogonal TRS with two collapsing rules. By translating this example, we show that UN∞ also fails for the infinitary λβη-calculus.},
  langid = {english}
}
% == BibTeX quality report for endrullisInfinitaryTermRewriting2014:
% ? unused Journal abbreviation ("Log.Meth.Comput.Sci.")
% ? unused Library catalog ("DOI.org (Crossref)")

@incollection{erdwegLayoutsensitiveGeneralizedParsing2013,
  title = {Layout-Sensitive Generalized Parsing},
  booktitle = {Software {{Language Engineering}}},
  author = {Erdweg, Sebastian and Rendel, Tillmann and Kästner, Christian and Ostermann, Klaus},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Czarnecki, Krzysztof and Hedin, Görel},
  year = {2013},
  volume = {7745},
  pages = {244--263},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-36089-3_14},
  url = {http://link.springer.com/10.1007/978-3-642-36089-3_14},
  urldate = {2020-06-15},
  abstract = {The theory of context-free languages is well-understood and context-free parsers can be used as off-the-shelf tools in practice. In particular, to use a context-free parser framework, a user does not need to understand its internals but can specify a language declaratively as a grammar. However, many languages in practice are not context-free. One particularly important class of such languages is layout-sensitive languages, in which the structure of code depends on indentation and whitespace. For example, Python, Haskell, F\#, and Markdown use indentation instead of curly braces to determine the block structure of code. Their parsers (and lexers) are not declaratively specified but hand-tuned to account for layout-sensitivity.},
  isbn = {978-3-642-36088-6 978-3-642-36089-3},
  langid = {english}
}
% == BibTeX quality report for erdwegLayoutsensitiveGeneralizedParsing2013:
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Series title ("Lecture Notes in Computer Science")

@inproceedings{erdwegSoundOptimalIncremental2015b,
  ids = {erdwegSoundOptimalIncremental2015,erdwegSoundOptimalIncremental2015a},
  title = {A Sound and Optimal Incremental Build System with Dynamic Dependencies},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object-Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Erdweg, Sebastian and Lichter, Moritz and Weiel, Manuel},
  year = {2015},
  month = oct,
  pages = {89--106},
  publisher = {{ACM}},
  address = {{Pittsburgh PA USA}},
  doi = {10.1145/2814270.2814316},
  url = {https://dl.acm.org/doi/10.1145/2814270.2814316},
  urldate = {2023-12-08},
  abstract = {Build systems are used in all but the smallest software projects to invoke the right build tools on the right files in the right order. A build system must be sound (after a build, generated files consistently reflect the latest source files) and efficient (recheck and rebuild as few build units as possible). Contemporary build systems provide limited efficiency because they lack support for expressing finegrained file dependencies. We present a build system called pluto that supports the definition of reusable, parameterized, interconnected builders. When run, a builder notifies the build system about dynamically required and produced files as well as about other builders whose results are needed. To support fine-grained file dependencies, we generalize the traditional notion of time stamps to allow builders to declare their actual requirements on a file’s content. pluto collects the requirements and products of a builder with their stamps in a build summary. This enables pluto to provides provably sound and optimal incremental rebuilding. To support dynamic dependencies, our rebuild algorithm interleaves dependency analysis and builder execution and enforces invariants on the dependency graph through a dynamic analysis. We have developed pluto as a Java API and used it to implement more than 25 builders. We describe our experience with migrating a larger Ant build script to pluto and compare the respective build times.},
  isbn = {978-1-4503-3689-5},
  langid = {english}
}
% == BibTeX quality report for erdwegSoundOptimalIncremental2015b:
% ? unused Conference name ("SPLASH '15: Conference on Systems, Programming, Languages, and Applications: Software for Humanity")
% ? unused Library catalog ("DOI.org (Crossref)")

@phdthesis{erkokValueRecursionMonadic2002,
  title = {Value Recursion in Monadic Computations},
  author = {Erkok, Levent},
  year = {2002},
  month = oct,
  url = {http://leventerkok.github.io/papers/erkok-thesis.pdf},
  langid = {english},
  school = {Oregon Health and Science University}
}
% == BibTeX quality report for erkokValueRecursionMonadic2002:
% ? unused Archive ("10.6083/M4SQ8XBW")
% ? unused Library catalog ("Zotero")
% ? unused Number of pages ("170")

@mastersthesis{filinskiDeclarativeContinuationsCategorical1989,
  title = {Declarative Continuations and Categorical Duality},
  author = {Filinski, Andrzej},
  year = {1989},
  month = aug,
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.8729&rep=rep1&type=pdf},
  abstract = {This thesis presents a formalism for reasoning about continuations in a categorical setting. It points out how values and continuations ca n be seen as categorically dual concepts, and that this symmetry extends to not only data types, but also control structures, evaluation strategies and higher-order constructs. The central idea is a view of continuations as a declarative concept, rather than an imperative one, and the implicat ions of this make up the spine of the presentation. A symmetrical extension of the typed *-calculus is introduced, where values and continuations are treated as opposites, permitting a mirror-image syntax for dual categorical concepts like products and coproducts. An implementable semantic description and a static type system for this calculus are given. A purely categorical description of the language is also obtained, through a correspondence with a system of combinatory logic, similar to a cartesian closed category, but with a completely symmetrical set of axioms. Finally, a number of possible practical applications and directions for further research are suggested.},
  school = {University of Copenhagen}
}
% == BibTeX quality report for filinskiDeclarativeContinuationsCategorical1989:
% ? unused Library catalog ("CiteSeer")
% ? unused Type ("Master's")

@inproceedings{filinskiLinearContinuations1992,
  title = {Linear Continuations},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Filinski, Andrzej},
  year = {1992},
  month = feb,
  series = {{{POPL}} '92},
  pages = {27--38},
  publisher = {{Association for Computing Machinery}},
  address = {{Albuquerque, New Mexico, USA}},
  doi = {10.1145/143165.143174},
  url = {https://doi.org/10.1145/143165.143174},
  urldate = {2020-06-19},
  abstract = {We present a functional interpretation of classical linear logic based on the concept of linear continuations. Unlike their non-linear counterparts, such continuations lead to a model of control that does not inherently impose any particular evaluation strategy. Instead, such additional structure is expressed by admitting closely controlled copying and discarding of continuations. We also emphasize the importance of classicality in obtaining computationally appealing categorical models of linear logic and propose a simple “coreflective subcategory” interpretation of the modality “!”.},
  isbn = {978-0-89791-453-6}
}
% == BibTeX quality report for filinskiLinearContinuations1992:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@techreport{fisherCommonProgrammingLanguage1976,
  title = {A Common Programming Language for the {{Department}} of {{Defense--Background}} and Technical Requirements},
  author = {Fisher, David A.},
  year = {1976},
  month = jun,
  url = {https://apps.dtic.mil/sti/citations/ADA028297},
  urldate = {2024-01-14},
  abstract = {This paper presents the set of characteristics needed for a common programming language of embedded computer systems applications in the DoD. In addition, it describes the background, purpose, and organization of the DoD Common Programming Language efforts. It reviews the issues considered in developing the needed language characteristics, explains how certain trade-offs and potential conflicts were resolved, and discusses the criteria used to ensure that any language satisfying the criteria will be suitable for embedded computer applications, will not aggravate existing software problems, and will be suitable for standardization.},
  chapter = {Technical Reports},
  langid = {english}
}
% == BibTeX quality report for fisherCommonProgrammingLanguage1976:
% Missing required field 'institution'
% ? unused Library catalog ("apps.dtic.mil")

@article{flanaganEssenceCompilingContinuations1993,
  ids = {flanaganEssenceCompilingContinuations},
  title = {The Essence of Compiling with Continuations},
  author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
  year = {1993},
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {28},
  number = {6},
  pages = {237--247},
  issn = {0362-1340},
  doi = {10.1145/173262.155113},
  url = {https://dl.acm.org/doi/10.1145/173262.155113},
  urldate = {2023-05-03},
  abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the “continuation”). Since the nai¨ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters. A thorough analysis of the abstract machine for CPS terms show that the actions of the code generator invert the nai¨ve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.},
  langid = {english}
}
% == BibTeX quality report for flanaganEssenceCompilingContinuations1993:
% ? unused Journal abbreviation ("SIGPLAN Not.")
% ? unused Library catalog ("ACM Digital Library")

@article{forsterExpressivePowerUserdefined2017,
  title = {On the Expressive Power of User-Defined Effects: {{Effect}} Handlers, Monadic Reflection, Delimited Control},
  shorttitle = {On the Expressive Power of User-Defined Effects},
  author = {Forster, Yannick and Kammar, Ohad and Lindley, Sam and Pretnar, Matija},
  year = {2017},
  month = feb,
  journal = {arXiv:1610.09161 [cs]},
  eprint = {1610.09161},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1610.09161},
  urldate = {2021-11-29},
  abstract = {We compare the expressive power of three programming abstractions for user-defined computational effects: Bauer and Pretnar's effect handlers, Filinski's monadic reflection, and delimited control without answer-type-modification. This comparison allows a precise discussion about the relative expressiveness of each programming abstraction. It also demonstrates the sensitivity of the relative expressiveness of user-defined effects to seemingly orthogonal language features. We present three calculi, one per abstraction, extending Levy's call-by-push-value. For each calculus, we present syntax, operational semantics, a natural type-and-effect system, and, for effect handlers and monadic reflection, a set-theoretic denotational semantics. We establish their basic meta-theoretic properties: safety, termination, and, where applicable, soundness and adequacy. Using Felleisen's notion of a macro translation, we show that these abstractions can macro-express each other, and show which translations preserve typeability. We use the adequate finitary set-theoretic denotational semantics for the monadic calculus to show that effect handlers cannot be macro-expressed while preserving typeability either by monadic reflection or by delimited control. We supplement our development with a mechanised Abella formalisation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages}
}
% == BibTeX quality report for forsterExpressivePowerUserdefined2017:
% ? Possibly abbreviated journal title arXiv:1610.09161 [cs]

@incollection{forsterQuineNewFoundations2019,
  title = {Quine’s {{New Foundations}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Forster, Thomas},
  editor = {Zalta, Edward N.},
  year = {2019},
  edition = {Summer 2019},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  url = {https://plato.stanford.edu/archives/sum2019/entries/quine-nf/},
  urldate = {2021-03-04},
  abstract = {Quine’s system of axiomatic set theory, NF, takes its name from thetitle (“New Foundations for Mathematical Logic”) of the1937 article which introduced it (Quine [1937a]). The axioms of NF areextensionality:, together with stratified comprehension, which is to say alluniversal closures of formulæ like,  where ‘xxx’ is not free in ΦΦ\textbackslash Phi and ΦΦ\textbackslash Phi is(weakly) stratified. This last condition requires that there should bea function σσ\textbackslash sigma (a “stratification”) fromthe bound variables in ΦΦ\textbackslash Phi to an initial segmentof the natural numbers such that if ‘u∈vu∈vu \textbackslash in v’ is asubformula of ΦΦ\textbackslash Phi then σ(‘v’)=σ(‘u’)+1σ(‘v’)=σ(‘u’)+1\textbackslash sigma(`v\textbackslash rsquo) = \textbackslash sigma(`u\textbackslash rsquo) +1 and if ‘u=v’‘u=v’`u = v\textbackslash rsquo is a subformula of ΦΦ\textbackslash Phi thenσ(‘v’)=σ(‘u’)σ(‘v’)=σ(‘u’)\textbackslash sigma(`v\textbackslash rsquo) = \textbackslash sigma(`u\textbackslash rsquo). The origins of thisconstraint will be explained below., Some illustrations may help: x∈xx∈xx \textbackslash in x is not stratified. x∈yx∈yx \textbackslash iny is. Thus not every substitution-instance of a stratified formulais stratified.  y=℘(x)y=℘(x)y = \textbackslash wp(x) is stratified (the fancy P means powerset), with the variable yyy being given a type one higher than thetype given to xxx. To check this we have to write out ‘y=℘(x)y=℘(x)y =\textbackslash wp(x)’ in primitive notation. (In primitive notation, thisformula becomes: ∀z(z∈y↔∀w(w∈z→w∈x))∀z(z∈y↔∀w(w∈z→w∈x))\textbackslash forall z(z \textbackslash in y \textbackslash leftrightarrow \textbackslash forall w(w \textbackslash in z\textbackslash rightarrow w \textbackslash in x)). One can assign 0 to w,1w,1w, 1 to zzz,and 2 to xxx, to get a stratification of this formula.) In generalit is always necessary to write a formula out in primitivenotation – at least until one gets the hang of it., The observant reader will have spotted the appearance above of theexpression weakly stratified. What is this? The instance ofthe comprehension that says that x∪\{y\}x∪\{y\}x \textbackslash cup \textbackslash\{y\textbackslash\} always exists isstratified, and is therefore an axiom. So x∪\{y\}x∪\{y\}x \textbackslash cup \textbackslash\{y\textbackslash\} alwaysexists. So x∪\{x\}x∪\{x\}x \textbackslash cup \textbackslash\{x\textbackslash\} always exists, by substitution. Howeverthe instance of the comprehension scheme alleging its existence is notstratified. The term ‘x∪\{x\}x∪\{x\}x \textbackslash cup \textbackslash\{x\textbackslash\}’ is said tobe weakly stratified by which it is meant that it can bestratified if we are allowed to give different types to distinctoccurrences offree variables. Weak stratification is what is needed toadmit ab initio those instances of the comprehension schemethat give us substitution instances of stratified instances, and notrequire a detour such as the detour here through theexistence of x∪\{y\}x∪\{y\}x \textbackslash cup \textbackslash\{y\textbackslash\}.}
}
% == BibTeX quality report for forsterQuineNewFoundations2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Stanford Encyclopedia of Philosophy")

@misc{furiaApplyingBayesianAnalysis2021,
  title = {Applying {{Bayesian}} Analysis Guidelines to Empirical Software Engineering Data: The Case of Programming Languages and Code Quality},
  shorttitle = {Applying Bayesian Analysis Guidelines to Empirical Software Engineering Data},
  author = {Furia, Carlo A. and Torkar, Richard and Feldt, Robert},
  year = {2021},
  month = jul,
  number = {arXiv:2101.12591},
  eprint = {2101.12591},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2101.12591},
  urldate = {2023-12-27},
  abstract = {Statistical analysis is the tool of choice to turn data into information, and then information into empirical knowledge. The process that goes from data to knowledge is, however, long, uncertain, and riddled with pitfalls. To be valid, it should be supported by detailed, rigorous guidelines, which help ferret out issues with the data or model, and lead to qualified results that strike a reasonable balance between generality and practical relevance. Such guidelines are being developed by statisticians to support the latest techniques for Bayesian data analysis. In this article, we frame these guidelines in a way that is apt to empirical research in software engineering.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering}
}

@book{gammaDesignPatternsElements1994,
  title = {Design Patterns: Elements of Reusable Object-Oriented Software},
  shorttitle = {Design Patterns},
  author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John and Booch, Grady},
  year = {1994},
  month = oct,
  edition = {1st edition},
  publisher = {{Addison-Wesley Professional}},
  address = {{Reading, Mass}},
  abstract = {Capturing a wealth of experience about the design of object-oriented software, four top-notch designers present a catalog of simple and succinct solutions to commonly occurring design problems. Previously undocumented, these 23 patterns allow designers to create more flexible, elegant, and ultimately reusable designs without having to rediscover the design solutions themselves.  The authors begin by describing what patterns are and how they can help you design object-oriented software. They then go on to systematically name, explain, evaluate, and catalog recurring designs in object-oriented systems. With Design Patterns as your guide, you will learn how these important patterns fit into the software development process, and how you can leverage them to solve your own design problems most efficiently.  Each pattern describes the circumstances in which it is applicable, when it can be applied in view of other design constraints, and the consequences and trade-offs of using the pattern within a larger design. All patterns are compiled from real systems and are based on real-world examples. Each pattern also includes code that demonstrates how it may be implemented in object-oriented programming languages like C++ or Smalltalk.},
  isbn = {978-0-201-63361-0},
  langid = {english}
}
% == BibTeX quality report for gammaDesignPatternsElements1994:
% ? unused Library catalog ("Amazon")
% ? unused Number of pages ("416")

@article{ghilezanStrongNormalizationTypability1996,
  title = {Strong Normalization and Typability with Intersection Types},
  author = {Ghilezan, Silvia},
  year = {1996},
  month = jan,
  journal = {Notre Dame Journal of Formal Logic},
  volume = {37},
  number = {1},
  pages = {44--52},
  publisher = {{Duke University Press}},
  issn = {0029-4527, 1939-0726},
  doi = {10.1305/ndjfl/1040067315},
  url = {https://projecteuclid.org/journals/notre-dame-journal-of-formal-logic/volume-37/issue-1/Strong-Normalization-and-Typability-with-Intersection-Types/10.1305/ndjfl/1040067315.full},
  urldate = {2022-06-23},
  abstract = {A simple proof is given of the property that the set of strongly normalizing lambda terms coincides with the set of lambda terms typable in certain intersection type assignment systems.},
  keywords = {03B15,03B40,03B70,68Q55}
}
% == BibTeX quality report for ghilezanStrongNormalizationTypability1996:
% ? unused Library catalog ("Project Euclid")

@inproceedings{girardGeometryInteraction1989,
  ids = {girardGeometryInteraction1989a},
  title = {Towards a Geometry of Interaction},
  booktitle = {Categories in {{Computer Science}} and {{Logic}}},
  author = {Girard, Jean-Yves},
  editor = {Gray, J. W. and Scedrov, A.},
  year = {1989},
  series = {Contemporary {{Mathematics}}},
  volume = {92},
  pages = {69--108},
  publisher = {{American Mathematical Society}},
  address = {{University of Colorado in Boulder}},
  url = {https://jb55.com/linear/pdf/Towards%20a%20geometry%20of%20interaction.pdf},
  note = {Proceedings of the AMS-IMS-SIAM Joint Summer Research Conference, June 14–20, 1987, Boulder, Colorado; Contemporary Mathematics Volume 92}
}
% == BibTeX quality report for girardGeometryInteraction1989:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Google Scholar")

@article{girardLocusSolumRules2001,
  ids = {girardLocusSolumRules2001a},
  title = {Locus {{Solum}}: {{From}} the Rules of Logic to the Logic of Rules},
  shorttitle = {Locus {{Solum}}},
  author = {Girard, Jean-Yves},
  year = {2001},
  month = jun,
  journal = {Mathematical Structures in Computer Science},
  volume = {11},
  number = {3},
  pages = {301--506},
  publisher = {{Cambridge University Press}},
  issn = {1469-8072, 0960-1295},
  doi = {10.1017/S096012950100336X},
  url = {https://www.cambridge.org/core/journals/mathematical-structures-in-computer-science/article/abs/locus-solum-from-the-rules-of-logic-to-the-logic-of-rules/6318E18EA633F9692D9CDBA9DE4438C9},
  urldate = {2022-03-27},
  abstract = {Go back to An-fang, the Peace Square at An-Fang, the Beginning Place at An-Fang, where all things start (…) An-Fang was near a city, the only living city with a pre-atomic name (…) The headquarters of the People Programmer was at An-Fang, and there the mistake happened: A ruby trembled. Two tourmaline nets failed to rectify the laser beam. A diamond noted the error. Both the error and the correction went into the general computer. Cordwainer SmithThe Dead Lady of Clown Town, 1964.},
  langid = {english}
}

@article{goelDesignImplementationUse2019,
  title = {On the Design, Implementation, and Use of Laziness in {{R}}},
  author = {Goel, Aviral and Vitek, Jan},
  year = {2019},
  month = oct,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {OOPSLA},
  eprint = {1909.08958},
  primaryclass = {cs},
  pages = {1--27},
  issn = {2475-1421},
  doi = {10.1145/3360579},
  url = {http://arxiv.org/abs/1909.08958},
  urldate = {2022-10-25},
  abstract = {The R programming language has been lazy for over twenty-five years. This paper presents a review of the design and implementation of call-by-need in R, and a data-driven study of how generations of programmers have put laziness to use in their code. We analyze 16,707 packages and observe the creation of 270.9 B promises. Our data suggests that there is little supporting evidence to assert that programmers use laziness to avoid unnecessary computation or to operate over infinite data structures. For the most part R code appears to have been written without reliance on, and in many cases even knowledge of, delayed argument evaluation. The only significant exception is a small number of packages which leverage call-by-need for meta-programming. CCS Concepts: • General and reference → Empirical studies; • Software and its engineering → General programming languages; Scripting languages; Semantics.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Programming Languages,D.3},
  note = {Comment: 27 pages, 4 tables, 21 figures}
}
% == BibTeX quality report for goelDesignImplementationUse2019:
% ? unused Journal abbreviation ("Proc. ACM Program. Lang.")

@book{graverCostReportingElements1977,
  title = {Cost Reporting Elements and Activity Cost Tradeoffs for Defense System Software. {{Volume I}}.  {{Study}} Results.},
  shorttitle = {{{DTIC ADA053020}}},
  author = {Graver, C.A.},
  year = {1977},
  month = may,
  url = {http://archive.org/details/DTIC_ADA053020},
  urldate = {2023-12-30},
  abstract = {In April 1976, General Research Corporation (GRC) began a study of 'Life-Cycle Costing of Major Defense System Software and Computer Resources,' Contract F19628- 76-C-0180. The purpose was to assist Air Force Program Offices and staff agencies in estimating, reporting and controlling the life-cycle costs of software. The study was performed under direction of the Electronic Systems Division (AFSC), Computer Systems Engineering Office (TOI).},
  langid = {english},
  keywords = {*COMPUTER PROGRAMS,*COSTS,*DEFENSE SYSTEMS,DTIC Archive,ESTIMATES,GENERAL RESEARCH CORP SANTA BARBARA CALIF,{Graver,C  A},LIFE CYCLE COSTS,MAINTENANCE,MANHOURS,SIZES(DIMENSIONS)}
}
% == BibTeX quality report for graverCostReportingElements1977:
% Missing required field 'publisher'
% ? unused Library catalog ("Internet Archive")
% ? unused Number of pages ("303")

@misc{gravgaardElasticTabstopsBetter,
  title = {Elastic Tabstops - a Better Way to Indent and Align Code},
  author = {Gravgaard, Nick},
  url = {https://nickgravgaard.com/elastic-tabstops/},
  urldate = {2021-02-13},
  abstract = {Elastic tabstops - a better way to indent and align code},
  langid = {english}
}

@article{grzywaczDoesAmountInformation2022,
  title = {Does Amount of Information Support Aesthetic Values?},
  author = {Grzywacz, Norberto M. and Aleem, Hassan},
  year = {2022},
  journal = {Frontiers in Neuroscience},
  volume = {16},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2022.805658},
  urldate = {2024-01-08},
  abstract = {Obtaining information from the world is important for survival. The brain, therefore, has special mechanisms to extract as much information as possible from sensory stimuli. Hence, given its importance, the amount of available information may underlie aesthetic values. Such information-based aesthetic values would be significant because they would compete with others to drive decision-making. In this article, we ask, “What is the evidence that amount of information support aesthetic values?” An important concept in the measurement of informational volume is entropy. Research on aesthetic values has thus used Shannon entropy to evaluate the contribution of quantity of information. We review here the concepts of information and aesthetic values, and research on the visual and auditory systems to probe whether the brain uses entropy or other relevant measures, specially, Fisher information, in aesthetic decisions. We conclude that information measures contribute to these decisions in two ways: first, the absolute quantity of information can modulate aesthetic preferences for certain sensory patterns. However, the preference for volume of information is highly individualized, with information-measures competing with organizing principles, such as rhythm and symmetry. In addition, people tend to be resistant to too much entropy, but not necessarily, high amounts of Fisher information. We show that this resistance may stem in part from the distribution of amount of information in natural sensory stimuli. Second, the measurement of entropic-like quantities over time reveal that they can modulate aesthetic decisions by varying degrees of surprise given temporally integrated expectations. We propose that amount of information underpins complex aesthetic values, possibly informing the brain on the allocation of resources or the situational appropriateness of some cognitive models.}
}
% == BibTeX quality report for grzywaczDoesAmountInformation2022:
% ? unused Library catalog ("Frontiers")

@article{gucluturkDecomposingComplexityPreferences2019,
  title = {Decomposing Complexity Preferences for Music},
  author = {Güçlütürk, Yaǧmur and {van Lier}, Rob},
  year = {2019},
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00674},
  urldate = {2024-01-08},
  abstract = {Recently, we demonstrated complexity as a major factor for explaining individual differences in visual preferences for abstract digital art. We have shown that participants could best be separated into two groups based on their liking ratings for abstract digital art comprising geometric patterns: one group with a preference for complex visual patterns and another group with a preference for simple visual patterns. In the present study, building up on these results, we extended our investigations for complexity preferences from highly controlled visual stimuli to ecologically valid stimuli in the auditory modality. Similar to visual preferences, we showed that music preferences are highly influenced by stimulus complexity. We demonstrated this by clustering a large number of participants based on their liking ratings for song excerpts from various musical genres. Our results show that, based on their liking ratings, participants can best be separated into two groups: one group with a preference for more complex songs and another group with a preference for simpler songs. Finally, we considered various demographic and personal characteristics to explore differences between the groups, and reported that at least for the current data set age and gender to be significant factors separating the two groups.}
}
% == BibTeX quality report for gucluturkDecomposingComplexityPreferences2019:
% ? unused Library catalog ("Frontiers")

@inproceedings{guerriniOptimalImplementationInefficient2017,
  title = {Is the Optimal Implementation Inefficient? {{Elementarily}} Not.},
  shorttitle = {Is the Optimal Implementation Inefficient?},
  booktitle = {2nd {{International Conference}} on {{Formal Structures}} for {{Computation}} and {{Deduction}}},
  author = {Guerrini, Stefano and Solieri, Marco},
  year = {2017},
  month = sep,
  pages = {16 pages},
  publisher = {{Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany}},
  doi = {10.4230/LIPICS.FSCD.2017.17},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/7733/},
  urldate = {2021-09-06},
  abstract = {Sharing graphs are a local and asynchronous implementation of lambda-calculus beta-reduction (or linear logic proof-net cut-elimination) that avoids useless duplications. Empirical benchmarks suggest that they are one of the most efficient machineries, when one wants to fully exploit the higher-order features of lambda-calculus. However, we still lack confirming grounds with theoretical solidity to dispel uncertainties about the adoption of sharing graphs. Aiming at analysing in detail the worst-case overhead cost of sharing operators, we restrict to the case of elementary and light linear logic, two subsystems with bounded computational complexity of multiplicative exponential linear logic. In these two cases, the bookkeeping component is unnecessary, and sharing graphs are simplified to the so-called “abstract algorithm”. By a modular cost comparison over a syntactical simulation, we prove that the overhead of shared reductions is quadratically bounded to cost of the naive implementation, i.e. proof-net reduction. This result generalises and strengthens a previous complexity result, and implies that the price of sharing is negligible, if compared to the obtainable benefits on reductions requiring a large amount of duplication.},
  collaborator = {Herbstritt, Marc},
  langid = {english},
  keywords = {{000 Computer science, knowledge, general works},Computer Science},
  note = {\subsection{Other}

\par
Sharing graphs are a local and asynchronous implementation of lambda-calculus beta-reduction (or linear logic proof-net cut-elimination) that avoids useless duplications. Empirical benchmarks suggest that they are one of the most efficient machineries, when one wants to fully exploit the higher-order features of lambda-calculus. However, we still lack confirming grounds with theoretical solidity to dispel uncertainties about the adoption of sharing graphs. Aiming at analysing in detail the worst-case overhead cost of sharing operators, we restrict to the case of elementary and light linear logic, two subsystems with bounded computational complexity of multiplicative exponential linear logic. In these two cases, the bookkeeping component is unnecessary, and sharing graphs are simplified to the so-called "abstract algorithm". By a modular cost comparison over a syntactical simulation, we prove that the overhead of shared reductions is quadratically bounded to cost of the naive implementation, i.e. proof-net reduction. This result generalises and strengthens a previous complexity result, and implies that the price of sharing is negligible, if compared to the obtainable benefits on reductions requiring a large amount of duplication.}
}
% == BibTeX quality report for guerriniOptimalImplementationInefficient2017:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("DOI.org (Datacite)")
% ? unused Medium ("application/pdf")

@phdthesis{guerriniTheoreticalPracticalIssues1996,
  title = {Theoretical and Practical Issues of Optimal Implementations of Functional Languages},
  author = {Guerrini, Stefano},
  year = {1996},
  url = {https://www-lipn.univ-paris13.fr/~guerrini/mysite/sites/default/files/biblio/PhDThesis.pdf},
  school = {Università di Pisa. Dipartimento di Informatica}
}
% == BibTeX quality report for guerriniTheoreticalPracticalIssues1996:
% ? unused Library catalog ("Google Scholar")

@inproceedings{guyerFreeMeStaticAnalysis2006,
  ids = {guyerFreeMeStaticAnalysis},
  title = {Free-{{Me}}: A Static Analysis for Automatic Individual Object Reclamation},
  shorttitle = {Free-{{Me}}},
  booktitle = {Proceedings of the 27th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Guyer, Samuel Z. and McKinley, Kathryn S. and Frampton, Daniel},
  year = {2006},
  month = jun,
  series = {{{PLDI}} '06},
  pages = {364--375},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1133981.1134024},
  url = {https://doi.org/10.1145/1133981.1134024},
  urldate = {2023-07-18},
  abstract = {Garbage collection has proven benefits, including fewer memory related errors and reduced programmer effort. Garbage collection, however, trades space for time. It reclaims memory only when it is invoked: invoking it more frequently reclaims memory quickly, but incurs a significant cost; invoking it less frequently fills memory with dead objects. In contrast, explicit memory management provides prompt low cost reclamation, but at the expense of programmer effort.This work comes closer to the best of both worlds by adding novel compiler and runtime support for compiler inserted frees to a garbage-collected system. The compiler's free-me analysis identifies when objects become unreachable and inserts calls to free. It combines a lightweight pointer analysis with liveness information that detects when short-lived objects die. Our approach differs from stack and region allocation in two crucial ways. First, it frees objects incrementally exactly when they become unreachable, instead of based on program scope. Second, our system does not require allocation-site lifetime homogeneity, and thus frees objects on some paths and not on others. It also handles common patterns: it can free objects in loops and objects created by factory methods.We evaluate free() variations for free-list and bump-pointer allocators. Explicit freeing improves performance by promptly reclaiming objects and reducing collection load. Compared to marksweep alone, free-me cuts total time by 22\% on average, collector time by 50\% to 70\%, and allows programs to run in 17\% less memory. This combination retains the software engineering benefits of garbage collection while increasing space efficiency and improving performance, and thus is especially appealing for real-time and space constrained systems.},
  isbn = {978-1-59593-320-1},
  keywords = {adaptive,compiler-assisted,copying,generational,liveness,locality,mark-sweep,pointer analysis}
}
% == BibTeX quality report for guyerFreeMeStaticAnalysis2006:
% ? unused Library catalog ("ACM Digital Library")

@article{hackettCallbyneedClairvoyantCallbyvalue2019,
  title = {Call-by-Need Is Clairvoyant Call-by-Value},
  author = {Hackett, Jennifer and Hutton, Graham},
  year = {2019},
  month = jul,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {ICFP},
  pages = {1--23},
  issn = {2475-1421},
  doi = {10.1145/3341718},
  url = {https://www.cs.nott.ac.uk/~pszgmh/clairvoyant.pdf},
  urldate = {2022-01-20},
  abstract = {Call-by-need evaluation, also known as lazy evaluation, provides two key benefits: compositional programming and infinite data. The standard semantics for laziness is Launchbury’s natural semantics~DBLP:conf/popl/Launchbury93, which uses a heap to memoise the results of delayed evaluations. However, the stateful nature of this heap greatly complicates reasoning about the operational behaviour of lazy programs. In this article, we propose an alternative semantics for laziness,               clairvoyant evaluation               , that replaces the state effect with nondeterminism, and prove this semantics equivalent in a strong sense to the standard semantics. We show how this new semantics greatly simplifies operational reasoning, admitting much simpler proofs of a number of results from the literature, and how it leads to the first               denotational cost semantics               for lazy evaluation.},
  langid = {english}
}
% == BibTeX quality report for hackettCallbyneedClairvoyantCallbyvalue2019:
% ? unused Journal abbreviation ("Proc. ACM Program. Lang.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{hemannFrameworkExtendingMicrokanren2017,
  title = {A Framework for Extending Microkanren with Constraints},
  author = {Hemann, Jason and Friedman, Daniel P.},
  year = {2017},
  month = jan,
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {234},
  eprint = {1701.00633},
  primaryclass = {cs},
  pages = {135--149},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.234.10},
  url = {http://arxiv.org/abs/1701.00633},
  urldate = {2022-07-15},
  abstract = {We present a framework for building CLP languages with symbolic constraints based on microKanren, a domain-specific logic language shallowly embedded in Racket. We rely on Racket's macro system to generate a constraint solver and other components of the microKanren embedding. The framework itself and the constraints' implementations amounts to just over 100 lines of code. Our framework is both a teachable implementation for CLP as well as a test-bed and prototyping tool for symbolic constraint systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Programming Languages,D.1.6 Logic Programming,D.3.2 Constraint and Logic Languages,D.3.3 Constraints},
  note = {Comment: In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148}
}
% == BibTeX quality report for hemannFrameworkExtendingMicrokanren2017:
% ? unused Journal abbreviation ("Electron. Proc. Theor. Comput. Sci.")

@inproceedings{hemannMicroKanrenMinimalFunctional2013,
  title = {{{microKanren}}: A Minimal Functional Core for Relational Programming.},
  booktitle = {Proceedings of the 2013 {{Workshop}} on {{Scheme}} and {{Functional Programming}}},
  author = {Hemann, Jason and Friedman, Daniel P. .},
  year = {2013},
  url = {http://webyrd.net/scheme-2013/papers/HemannMuKanren2013.pdf},
  urldate = {2022-07-14}
}

@inproceedings{hendersonDeterminismAnalysisMercury1996,
  ids = {hendersonDeterminismAnalysisMercury},
  title = {Determinism Analysis in the {{Mercury}} Compiler},
  booktitle = {In {{Proceedings}} of the {{Australian Computer Science Conference}}},
  author = {Henderson, Fergus and Somogyi, Zoltan and Conway, Thomas},
  year = {1996},
  pages = {337--346},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.3967&rep=rep1&type=pdf},
  abstract = {Mercury is a new purely declarative logic programming language. The Mercury determinism system allows programmers to specify which predicates never fail and which predicates succeed at most once. This information allows the compiler to check the program for errors, pinpointing predicates that do not satisfy their declarations. This makes programmers significantly more productive and enhances the reliability of their programs. The Mercury compiler also uses determinism information to generate much faster code than any other logic programming system. This paper presents the algorithms used by the Mercury compiler to infer determinism information and to detect determinism errors.  Keywords: determinism, Mercury, program analysis, logic programming, programming language implementation. 1 Introduction  One of the fundamental differences between logic programming and other programming paradigms is the presence of so-called "don't-know" nondeterminism, whereby a predicate may return more than...}
}
% == BibTeX quality report for hendersonDeterminismAnalysisMercury1996:
% ? unused Library catalog ("CiteSeer")

@article{hirokawaDecreasingDiagramsRelative2009,
  title = {Decreasing Diagrams and Relative Termination},
  author = {Hirokawa, Nao and Middeldorp, Aart},
  year = {2009},
  month = oct,
  journal = {arXiv:0910.2853 [cs]},
  eprint = {0910.2853},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/0910.2853},
  urldate = {2021-09-14},
  abstract = {In this paper we use the decreasing diagrams technique to show that a left-linear term rewrite system R is confluent if all its critical pairs are joinable and the critical pair steps are relatively terminating with respect to R. We further show how to encode the rule-labeling heuristic for decreasing diagrams as a satisfiability problem. Experimental data for both methods are presented.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Symbolic Computation},
  note = {Comment: v3: missing references added}
}
% == BibTeX quality report for hirokawaDecreasingDiagramsRelative2009:
% ? Possibly abbreviated journal title arXiv:0910.2853 [cs]

@article{hirokawaStrategiesDecreasinglyConfluent2011,
  title = {Strategies for Decreasingly Confluent Rewrite Systems},
  author = {Hirokawa, Nao and Middeldorp, Aart},
  year = {2011},
  journal = {Reduction Strategies in Rewriting and Programming},
  pages = {23},
  url = {http://elp.webs.upv.es/workshops/wrs2011/pre-proceedings.pdf#page=31}
}
% == BibTeX quality report for hirokawaStrategiesDecreasinglyConfluent2011:
% ? unused Library catalog ("Google Scholar")

@article{holmesAPLProgrammingLanguage1978,
  title = {Is {{APL}} a {{Programming Language}}?},
  author = {Holmes, W. N.},
  year = {1978},
  month = may,
  journal = {The Computer Journal},
  volume = {21},
  number = {2},
  pages = {128--131},
  issn = {0010-4620},
  doi = {10.1093/comjnl/21.2.128},
  url = {https://doi.org/10.1093/comjnl/21.2.128},
  urldate = {2022-12-01},
  abstract = {A Journal article entitled ‘Is APL a viable programming language?’ (Earnshaw, 1975), seemed to answer its question with a Yes. This article, written by an APL enthusiast, suggests the answer should have been No, not because APL is not viable, but because it is not a programming language. This suggestion is supported by an examination of what APL is, and of what programming is. The conclusion is drawn that programming, in the occupational sense, should be distinguished from calculation, and that APL is suited to calculation but not to programming.}
}
% == BibTeX quality report for holmesAPLProgrammingLanguage1978:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Silverchair")

@book{holmesElementarySetTheory1998,
  title = {Elementary Set Theory with a Universal Set},
  author = {Holmes, M. Randall},
  year = {1998},
  publisher = {{Bruylant-Academia}},
  url = {https://randall-holmes.github.io/head.pdf},
  googlebooks = {\_vjuAAAAMAAJ},
  isbn = {978-2-87209-488-2},
  langid = {english}
}
% == BibTeX quality report for holmesElementarySetTheory1998:
% ? unused Library catalog ("Google Books")
% ? unused Number of pages ("252")

@inproceedings{hudakAggregateUpdateProblem1985,
  title = {The Aggregate Update Problem in Functional Programming Systems},
  booktitle = {Proceedings of the 12th {{ACM SIGACT-SIGPLAN}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '85},
  author = {Hudak, Paul and Bloss, Adrienne},
  year = {1985},
  pages = {300--314},
  publisher = {{ACM Press}},
  address = {{New Orleans, Louisiana, United States}},
  doi = {10.1145/318593.318660},
  url = {http://portal.acm.org/citation.cfm?doid=318593.318660},
  urldate = {2022-01-04},
  isbn = {978-0-89791-147-4},
  langid = {english}
}
% == BibTeX quality report for hudakAggregateUpdateProblem1985:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 12th ACM SIGACT-SIGPLAN symposium")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{hudakExpressivenessPurelyFunctional1989,
  title = {On the {{Expressiveness}} of {{Purely Functional I}}/{{O Systems}}},
  author = {Hudak, Paul and Sundaresh, Raman S},
  year = {1989},
  month = mar,
  pages = {28},
  abstract = {Functional programming languages have traditionally lacked complete, exible, and yet referentially transparent I/O mechanisms. Previous proposals for I/O have used either the notion of lazy streams or continuations to model interaction with the external world. We discuss and generalize these models and introduce a third, which we call the systems model, to perform I/O. The expressiveness of the styles are compared by means of an example. We then give a series of surprisingly simple translations between the three models, demonstrating that they are not as di erent as their programming styles suggest, and implying that the styles could be mixed within a single program.},
  langid = {english}
}
% == BibTeX quality report for hudakExpressivenessPurelyFunctional1989:
% Missing required field 'journal'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Zotero")

@techreport{hudakHaskellVsAda1994,
  title = {Haskell vs. {{Ada}} vs. {{C}}++ vs. {{Awk}} vs. ... {{An}} Experiment in Software Prototyping Productivity},
  author = {Hudak, Paul and Jones, Mark P},
  year = {1994},
  month = oct,
  number = {YALEU/DCS/RR-1049},
  institution = {{Yale University}},
  abstract = {We describe the results of an experiment in which several conventional programming languages, together with the functional language Haskell, were used to prototype a Naval Surface Warfare Center (NSWC) requirement for a Geometric Region Server. The resulting programs and development metrics were reviewed by a committee chosen by the Navy. The results indicate that the Haskell prototype took significantly less time to develop and was considerably more concise and easier to understand than the corresponding prototypes written in several different imperative languages, including Ada and C++.},
  langid = {english}
}
% == BibTeX quality report for hudakHaskellVsAda1994:
% ? unused Library catalog ("Zotero")

@inproceedings{hudakHistoryHaskellBeing2007,
  title = {A History of {{Haskell}}: Being Lazy with Class},
  shorttitle = {A History of {{Haskell}}},
  booktitle = {Proceedings of the Third {{ACM SIGPLAN}} Conference on {{History}} of Programming Languages},
  author = {Hudak, Paul and Hughes, John and Peyton Jones, Simon and Wadler, Philip},
  year = {2007},
  month = jun,
  publisher = {{ACM}},
  address = {{San Diego California}},
  doi = {10.1145/1238844.1238856},
  url = {https://dl.acm.org/doi/10.1145/1238844.1238856},
  urldate = {2022-06-03},
  abstract = {This paper describes the history of Haskell, including its genesis and principles, technical contributions, implementations and tools, and applications and impact.},
  isbn = {978-1-59593-766-7},
  langid = {english}
}
% == BibTeX quality report for hudakHistoryHaskellBeing2007:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("HOPL-III '07: ACM SIGPLAN History of Programming Languages Conference III")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{hughesWhyFunctionalProgramming1989,
  title = {Why Functional Programming Matters},
  author = {Hughes, J.},
  year = {1989},
  month = feb,
  journal = {The Computer Journal},
  volume = {32},
  number = {2},
  pages = {98--107},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/32.2.98},
  url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/32.2.98},
  urldate = {2022-01-20},
  abstract = {As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write and to debug, and provides a collection of modules that can be reused to reduce future programming costs. In this paper we show that two features of functional languages in particular, higher-order functions and lazy evaluation, can contribute significantly to modularity. As examples, we manipulate lists and trees, program several numerical algorithms, and implement the alpha-beta heuristic (an algorithm from Artificial Intelligence used in game-playing programs). We conclude that since modularity is the key to successful programming, functional programming offers important advantages for software development.},
  langid = {english}
}
% == BibTeX quality report for hughesWhyFunctionalProgramming1989:
% ? unused Library catalog ("DOI.org (Crossref)")

@techreport{ichbiahRationaleDesignADA1979,
  title = {Rationale for the Design of the {{ADA}} Programming Language},
  author = {Ichbiah, Jean D. and Heliard, Jean-Claude and Roubine, Olivier and Barnes, John G.P. and {Krieg-Brueckner}, Bernd and Wichmann, Brian A.},
  year = {1979},
  month = jun,
  number = {Volume 14. Number 6. Part B,},
  pages = {267},
  url = {https://apps.dtic.mil/sti/citations/ADA073854},
  urldate = {2024-01-08},
  abstract = {This document, the Rationale for the design of the Green programming language, and the companion Reference Manual, are the two defining documents for the Green language. They serve different purposes. The Reference Manual contains a complete and concise definition of the language. Following Wirth we believe in the virtue of having a rather short reference manual. This has the advantage of providing the information in a form that can easily be consulted, read and reread several times, as the basis for developing a good familarity with the language.},
  chapter = {Technical Reports},
  langid = {english}
}
% == BibTeX quality report for ichbiahRationaleDesignADA1979:
% Missing required field 'institution'
% ? unused Library catalog ("apps.dtic.mil")

@article{iversonNotationToolThought1980,
  title = {Notation as a Tool of Thought},
  author = {Iverson, Kenneth E.},
  year = {1980},
  month = aug,
  journal = {Communications of the ACM},
  volume = {23},
  number = {8},
  pages = {444--465},
  issn = {0001-0782},
  doi = {10.1145/358896.358899},
  url = {https://doi.org/10.1145/358896.358899},
  urldate = {2022-07-18},
  keywords = {APL,mathematical notation}
}
% == BibTeX quality report for iversonNotationToolThought1980:
% ? unused Journal abbreviation ("Commun. ACM")
% ? unused Library catalog ("Aug. 1980")

@misc{jakobsDifferentialModularSoftware,
  title = {Differential Modular Software Verification},
  author = {Jakobs, Marie-Christine},
  url = {https://www.sosy-lab.org/research/prs/2019-10-01-CPA19-Differntial-Verification.pdf},
  urldate = {2020-07-25}
}

@inproceedings{jamesTheseusHighLevel2014,
  title = {Theseus: A High Level Language for Reversible Computing},
  booktitle = {Work-in-Progress Report at {{Conference}} on {{Reversible Computation}}},
  author = {James, Roshan P and Sabry, Amr},
  year = {2014},
  pages = {12},
  url = {https://legacy.cs.indiana.edu/~sabry/papers/theseus.pdf},
  abstract = {Programming in a reversible language remains “different” than programming in conventional irreversible languages, requiring specialized abstractions and unique modes of thinking. We present a high level language for reversible programming, called Theseus, that meshes naturally with conventional programming language abstractions. Theseus has the look and feel of a conventional functional language while maintaining a close correspondence with the low-level family of languages Π based on type isomorphisms [9]. In contrast to the point-free combinators of Π, Theseus has variables and binding forms, algebraic data types, function definitions by pattern matching, and is Turing complete. The language is strongly typed and all well-typed programs are reversible. We explain the semantics of Theseus via a collection of progressively expressive examples and outline its correspondence to Π.},
  langid = {english}
}
% == BibTeX quality report for jamesTheseusHighLevel2014:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("Conference on Reversible Computation")
% ? unused Library catalog ("Zotero")

@incollection{johnsonComputationalComplexityGLR1991,
  title = {The {{Computational Complexity}} of {{GLR Parsing}}},
  booktitle = {Generalized {{LR Parsing}}},
  author = {Johnson, Mark},
  editor = {Tomita, Masaru},
  year = {1991},
  pages = {35--42},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4615-4034-2_3},
  url = {https://doi.org/10.1007/978-1-4615-4034-2_3},
  urldate = {2023-05-22},
  abstract = {The Tomita parsing algorithm adapts Knuth’s (1967) well-known parsing algorithm for LR(K) grammars to non-LR grammars, including ambiguous grammars. Knuth’s algorithm is provably efficient: it requires at most O(n|G|) units of time, where \textbackslash G\textbackslash{} is the size of (i.e. the number of symbols in) G and n is the length of the string to be parsed. This is often significantly better than the O(n3|G|2) worst case time required by standard parsing algorithms such as the Earley algorithm. Since the Tomita algorithm is closely related to Knuth’s algorithm, one might expect that it too is provably more efficient than the Earley algorithm, especially as actual computational implementations of Tomita’s algorithm outperform implementations of the Earley algorithm (Tomita 1986, 1987).},
  isbn = {978-1-4615-4034-2},
  langid = {english},
  keywords = {Computational Linguistics,Input Length,Input String,Input Symbol,Parse Tree}
}
% == BibTeX quality report for johnsonComputationalComplexityGLR1991:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Springer Link")

@article{johnsonFirstclassStoresPartial1994,
  title = {First-Class Stores and Partial Continuations in a Programming Language and Environment},
  author = {Johnson, Gregory F and Duggan, Dominic},
  year = {1994},
  month = mar,
  journal = {Computer Languages},
  volume = {20},
  number = {1},
  pages = {53--68},
  issn = {00960551},
  doi = {10.1016/0096-0551(94)90014-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0096055194900140},
  urldate = {2022-12-05},
  abstract = {Inthe GL programming language and its debugger, it is possible at essentially any point during program execution to capture the entire data state (the extant store) or the control state (the sequence of pending function invocations, represented using a new construct called partial continuations). These artifacts of program execution can be bound to identifiers or placed in storage cells and subsequently manipulated either from within the executing program or interactively from the debugging environment. The captured data states and control states can be interactively examined, reasoned about and experimentally executed. This paper describes the design of a language and debugging environment that supports these two capabilities, with particular attention given to the implications of having them both in the same language. For the new features to be useful they must be efficient, and a description is given of the techniques that were used to achieve an acceptable level of efficiency.},
  langid = {english}
}
% == BibTeX quality report for johnsonFirstclassStoresPartial1994:
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{johnsonStoresPartialContinuations1988,
  ids = {johnsonStoresPartialContinuations1988a},
  title = {Stores and Partial Continuations as First-Class Objects in a Language and Its Environment},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Johnson, G. F. and Duggan, D.},
  year = {1988},
  month = jan,
  series = {{{POPL}} '88},
  pages = {158--168},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/73560.73574},
  url = {https://doi.org/10.1145/73560.73574},
  urldate = {2022-12-02},
  isbn = {978-0-89791-252-5}
}
% == BibTeX quality report for johnsonStoresPartialContinuations1988:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@article{jonesCallbyvalueTerminationUntyped2008,
  title = {Call-by-Value Termination in the Untyped Lambda-Calculus},
  author = {Jones, Neil D. and Bohr, Nina},
  year = {2008},
  month = mar,
  journal = {Logical Methods in Computer Science},
  volume = {4},
  number = {1},
  eprint = {0801.0882},
  pages = {3},
  issn = {18605974},
  doi = {10.2168/LMCS-4(1:3)2008},
  url = {http://arxiv.org/abs/0801.0882},
  urldate = {2021-03-06},
  abstract = {A fully-automated algorithm is developed able to show that evaluation of a given untyped lambda-expression will terminate under CBV (call-by-value). The ``size-change principle'' from first-order programs is extended to arbitrary untyped lambda-expressions in two steps. The first step suffices to show CBV termination of a single, stand-alone lambda;-expression. The second suffices to show CBV termination of any member of a regular set of lambda-expressions, defined by a tree grammar. (A simple example is a minimum function, when applied to arbitrary Church numerals.) The algorithm is sound and proven so in this paper. The Halting Problem's undecidability implies that any sound algorithm is necessarily incomplete: some lambda-expressions may in fact terminate under CBV evaluation, but not be recognised as terminating. The intensional power of the termination algorithm is reasonably high. It certifies as terminating many interesting and useful general recursive algorithms including programs with mutual recursion and parameter exchanges, and Colson's ``minimum'' algorithm. Further, our type-free approach allows use of the Y combinator, and so can identify as terminating a substantial subset of PCF.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Programming Languages,D.3.1,F.3.2}
}
% == BibTeX quality report for jonesCallbyvalueTerminationUntyped2008:
% ? unused Journal abbreviation ("Log.Meth.Comput.Sci.")

@book{jonesEstimatingSoftwareCosts2007,
  title = {Estimating Software Costs: Bringing Realism to Estimating},
  shorttitle = {Estimating Software Costs},
  author = {Jones, Capers},
  year = {2007},
  month = may,
  edition = {2nd edition},
  publisher = {{McGraw Hill}},
  address = {{New York}},
  isbn = {978-0-07-148300-1},
  langid = {english}
}
% == BibTeX quality report for jonesEstimatingSoftwareCosts2007:
% ? unused Library catalog ("Amazon")
% ? unused Number of pages ("644")

@book{jonesImplementationFunctionalProgramming1987,
  title = {The Implementation of Functional Programming Languages},
  author = {Jones, Simon Peyton},
  year = {1987},
  publisher = {{Prentice-Hall}},
  url = {https://www.microsoft.com/en-us/research/publication/the-implementation-of-functional-programming-languages/},
  urldate = {2020-08-02},
  abstract = {My 1987 book is now out of print, but it is available~here in its entirety in PDF form.},
  langid = {american},
  note = {\textbf{Errata}
\par
\begin{itemize}

\item Section 5.2.4, p87.~ We need an extra rule

\end{itemize}

\par
match us [] E = E
\par
This accounts for the possibility that in the constructor rule (Section 5.2.4) there may be some non-nullary constructors for which there are no equations.
\par
\begin{itemize}

\item P168, line 2, “VAR” should be “TVAR”.

\end{itemize}}
}
% == BibTeX quality report for jonesImplementationFunctionalProgramming1987:
% ? unused Library catalog ("www.microsoft.com")

@techreport{jonesSoftwareEconomicsFunction2017,
  title = {Software Economics and Function Point Metrics: Thirty Years of {{IFPUG}} Progress},
  author = {Jones, Capers},
  year = {2017},
  month = apr,
  url = {https://www.ifpug.org/wp-content/uploads/2017/04/IYSM.-Thirty-years-of-IFPUG.-Software-Economics-and-Function-Point-Metrics-Capers-Jones.pdf},
  urldate = {2023-12-28}
}
% == BibTeX quality report for jonesSoftwareEconomicsFunction2017:
% Missing required field 'institution'

@inproceedings{jonesTacklingAwkwardSquad2001,
  ids = {jonesTacklingAwkwardSquad2001a},
  title = {Tackling the Awkward Squad: Monadic Input/Output, Concurrency, Exceptions, and Foreign-Language Calls in {{Haskell}}},
  booktitle = {Engineering Theories of Software Construction},
  author = {Jones, Simon Peyton},
  year = {2001},
  pages = {47--96},
  url = {https://www.microsoft.com/en-us/research/publication/tackling-awkward-squad-monadic-inputoutput-concurrency-exceptions-foreign-language-calls-haskell/},
  abstract = {I’ve revised the notes significantly, with the help of feedback from many people. Last update: 21 Feb 2001. PowerPoint slides Writing High-Performance Server Applications in Haskell, Case Study: A Haskell Web Server, Simon Marlow, Haskell Workshop, Montreal, Canada, Sept 2000. This paper describes the running example in the notes. ~ This tutorial focuses on explaining […]},
  langid = {american}
}
% == BibTeX quality report for jonesTacklingAwkwardSquad2001:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("www.microsoft.com")

@article{jonesWearingHairShirt2003,
  title = {Wearing the Hair Shirt: A Retrospective on {{Haskell}} (2003)},
  shorttitle = {Wearing the Hair Shirt},
  author = {Jones, Simon Peyton},
  year = {2003},
  month = jan,
  url = {https://www.microsoft.com/en-us/research/publication/wearing-hair-shirt-retrospective-haskell-2003/},
  urldate = {2020-08-02},
  abstract = {Haskell was 15 years old at the POPL’03 meeting, when I presented this talk: it was born at a meeting at the 1987 conference on Functional Programming and Computer Architecture (FPCA’87). In this talk, which is very much a personal view, I take a look back at the language, and try to tease out what […]},
  langid = {american}
}
% == BibTeX quality report for jonesWearingHairShirt2003:
% Missing required field 'journal'
% ? unused Library catalog ("www.microsoft.com")

@article{joostenTeachingFunctionalProgramming1993,
  title = {Teaching Functional Programming to First-Year Students},
  author = {Joosten, Stef and Berg, Klaas Van Den and Hoeven, Gerrit Van Der},
  year = {1993},
  month = jan,
  journal = {Journal of Functional Programming},
  volume = {3},
  number = {1},
  pages = {49--65},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796800000599},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/teaching-functional-programming-to-firstyear-students/041D6A27806B921685CD562695270216},
  urldate = {2022-11-14},
  abstract = {In the period 1986–1991, experiments have been carried out with an introductory course in computer programming, based on functional programming. Due to thorough educational design and evaluation, a successful course has been developed. This has led to a revision of the computer programming education in the first year of the computer science curriculum at the University of Twente.This article describes the approach, the aim of the computer programming course, the outline and subject matter of the course, and the evaluation. Educational research has been done to assess the quality of the course.},
  langid = {english}
}

@article{jorgensenInterpretationProblemsRelated2012,
  title = {Interpretation Problems Related to the Use of Regression Models to Decide on Economy of Scale in Software Development},
  author = {Jørgensen, Magne and Kitchenham, Barbara},
  year = {2012},
  month = nov,
  journal = {Journal of Systems and Software},
  volume = {85},
  number = {11},
  pages = {2494--2503},
  issn = {01641212},
  doi = {10.1016/j.jss.2012.05.068},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121212001549},
  urldate = {2023-12-26},
  langid = {english}
}
% == BibTeX quality report for jorgensenInterpretationProblemsRelated2012:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{jorgensenReviewStudiesExpert2004,
  title = {A Review of Studies on Expert Estimation of Software Development Effort},
  author = {Jørgensen, M.},
  year = {2004},
  month = feb,
  journal = {Journal of Systems and Software},
  volume = {70},
  number = {1-2},
  pages = {37--60},
  issn = {01641212},
  doi = {10.1016/S0164-1212(02)00156-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121202001565},
  urldate = {2023-12-26},
  abstract = {This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher’s search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following 12 expert estimation ‘‘best practice’’ guidelines are evaluated through the review: (1) evaluate estimation accuracy, but avoid high evaluation pressure; (2) avoid conflicting estimation goals; (3) ask the estimators to justify and criticize their estimates; (4) avoid irrelevant and unreliable estimation information; (5) use documented data from previous development tasks; (6) find estimation experts with relevant domain background and good estimation records; (7) Estimate top-down and bottom-up, independently of each other; (8) use estimation checklists; (9) combine estimates from different experts and estimation strategies; (10) assess the uncertainty of the estimate; (11) provide feedback on estimation accuracy and development task relations; and, (12) provide estimation training opportunities. We found supporting evidence for all 12 estimation principles, and provide suggestions on how to implement them in software organizations.},
  langid = {english}
}
% == BibTeX quality report for jorgensenReviewStudiesExpert2004:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{jurkiewiczCostAddressTranslation2014,
  title = {The Cost of Address Translation},
  author = {Jurkiewicz, Tomasz and Mehlhorn, Kurt},
  year = {2014},
  month = apr,
  journal = {arXiv:1212.0703 [cs]},
  eprint = {1212.0703},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1212.0703},
  urldate = {2022-01-04},
  abstract = {Modern computers are not random access machines (RAMs). They have a memory hierarchy, multiple cores, and virtual memory. In this paper, we address the computational cost of address translation in virtual memory. Starting point for our work is the observation that the analysis of some simple algorithms (random scan of an array, binary search, heapsort) in either the RAM model or the EM model (external memory model) does not correctly predict growth rates of actual running times. We propose the VAT model (virtual address translation) to account for the cost of address translations and analyze the algorithms mentioned above and others in the model. The predictions agree with the measurements. We also analyze the VAT-cost of cache-oblivious algorithms.},
  archiveprefix = {arxiv},
  keywords = {{68Q05, 68Q15, 03D15},B.8.2,C.4,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Performance,F.1.1,F.2.3},
  note = {Comment: A extended abstract of this paper was published in the proceedings of ALENEX13, New Orleans, USA}
}
% == BibTeX quality report for jurkiewiczCostAddressTranslation2014:
% ? Possibly abbreviated journal title arXiv:1212.0703 [cs]

@article{kahrsInfinitaryRewritingClosure2013,
  title = {Infinitary Rewriting: Closure Operators, Equivalences and Models},
  shorttitle = {Infinitary Rewriting},
  author = {Kahrs, Stefan},
  year = {2013},
  month = mar,
  journal = {Acta Informatica},
  volume = {50},
  number = {2},
  pages = {123--156},
  issn = {0001-5903, 1432-0525},
  doi = {10.1007/s00236-012-0174-y},
  url = {http://link.springer.com/10.1007/s00236-012-0174-y},
  urldate = {2022-10-22},
  abstract = {Infinitary Term Rewriting allows to express infinite terms and transfinite reductions that converge to those terms. Underpinning the machinery of infinitary rewriting are closure operators on relations that facilitate the formation of transfinite reductions and transfinite equivalence proofs. The literature on infinitary rewriting has largely neglected to single out such closure operators, leaving them implicit in definitions of (transfinite) rewrite reductions, or equivalence relations. This paper unpicks some of those definitions, extracting the underlying closure principles used, as well as constructing alternative operators that lead to alternative notions of reduction and equivalence. A consequence of this unpicking is an insight into the abstraction level at which these operators can be defined. Some of the material in this paper already appeared in Kahrs (2010). The paper also generalises the notion of equational model for infinitary rewriting. This leads to semantics-based notions of equivalence that tie in with the equivalences constructed from the closure operators.},
  langid = {english}
}
% == BibTeX quality report for kahrsInfinitaryRewritingClosure2013:
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{kahrsNonomegaoverlappingTRSsAre2016,
  title = {Non-Omega-Overlapping {{TRSs}} Are {{UN}}},
  booktitle = {1st {{International Conference}} on {{Formal Structures}} for {{Computation}} and {{Deduction}} ({{FSCD}} 2016)},
  author = {Kahrs, Stefan and Smith, Connor},
  editor = {Kesner, Delia and Pientka, Brigitte},
  year = {2016},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {52},
  pages = {22:1--22:17},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.FSCD.2016.22},
  url = {http://drops.dagstuhl.de/opus/volltexte/2016/5996},
  urldate = {2021-09-11},
  isbn = {978-3-95977-010-1},
  keywords = {consistency,omega-substitutions,uniqueness of normal forms}
}
% == BibTeX quality report for kahrsNonomegaoverlappingTRSsAre2016:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Dagstuhl Research Online Publication Server")

@inproceedings{kangFormalMemoryModel2015,
  title = {A Formal {{C}} Memory Model Supporting Integer-Pointer Casts},
  booktitle = {Proceedings of the 36th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Kang, Jeehoon and Hur, Chung-Kil and Mansky, William and Garbuzov, Dmitri and Zdancewic, Steve and Vafeiadis, Viktor},
  year = {2015},
  month = jun,
  pages = {326--335},
  publisher = {{ACM}},
  address = {{Portland OR USA}},
  doi = {10.1145/2737924.2738005},
  url = {https://dl.acm.org/doi/10.1145/2737924.2738005},
  urldate = {2021-06-14},
  abstract = {The ISO C standard does not specify the semantics of many valid programs that use non-portable idioms such as integer-pointer casts. Recent efforts at formal definitions and verified implementation of the C language inherit this feature. By adopting high-level abstract memory models, they validate common optimizations. On the other hand, this prevents reasoning about much low-level code relying on the behavior of common implementations, where formal verification has many applications.},
  isbn = {978-1-4503-3468-6},
  langid = {english}
}
% == BibTeX quality report for kangFormalMemoryModel2015:
% ? unused Conference name ("PLDI '15: ACM SIGPLAN Conference on Programming Language Design and Implementation")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{karazerisFinalCoalgebrasAccessible2011,
  title = {Final Coalgebras in Accessible Categories},
  author = {Karazeris, Panagis and Matzaris, Apostolos and Velebil, Jiří},
  year = {2011},
  month = oct,
  journal = {Mathematical Structures in Computer Science},
  volume = {21},
  number = {5},
  pages = {1067--1108},
  issn = {0960-1295, 1469-8072},
  doi = {10.1017/S0960129511000351},
  url = {https://www.cambridge.org/core/product/identifier/S0960129511000351/type/journal_article},
  urldate = {2021-03-11},
  abstract = {We propose a construction of the final coalgebra for a finitary endofunctor of a finitely accessible category and study conditions under which this construction is available. Our conditions always apply when the accessible category is cocomplete, and is thus a locally finitely presentable (l.f.p.) category, and we give an explicit and uniform construction of the final coalgebra in this case. On the other hand, our results also apply to some interesting examples of final coalgebras beyond the realm of l.f.p. categories. In particular, we construct the final coalgebra for every finitary endofunctor on the category of linear orders, and analyse Freyd's coalgebraic characterisation of the closed unit as an instance of this construction. We use and extend results of Tom Leinster, developed for his study of self-similar objects in topology, relying heavily on his formalism of modules (corresponding to endofunctors) and complexes for a module.},
  langid = {english}
}
% == BibTeX quality report for karazerisFinalCoalgebrasAccessible2011:
% ? unused Journal abbreviation ("Math. Struct. Comp. Sci.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{kennawayComparingCurriedUncurried1995,
  ids = {kennawayComparingCurriedUncurried},
  title = {Comparing Curried and Uncurried Rewriting},
  author = {Kennaway, Richard and Klop, Jan Willem and Sleep, Ronan and Vries, Fer-jan De},
  year = {1995},
  pages = {25},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.25.8087&rep=rep1&type=pdf},
  langid = {english}
}
% == BibTeX quality report for kennawayComparingCurriedUncurried1995:
% Missing required field 'journal'
% ? unused Library catalog ("Zotero")

@article{kennawayInfinitaryLambdaCalculus1997,
  title = {Infinitary Lambda Calculus},
  author = {Kennaway, J.R. and Klop, J.W. and Sleep, M.R. and {de Vries}, F.J.},
  year = {1997},
  month = mar,
  journal = {Theoretical Computer Science},
  volume = {175},
  number = {1},
  pages = {93--125},
  issn = {03043975},
  doi = {10.1016/S0304-3975(96)00171-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397596001715},
  urldate = {2022-11-05},
  abstract = {In a previous paper we have established the theory of transfinite reduction for orthogonal term rewriting systems. In this paper we perform the same task for the lambda calculus. From the viewpoint of infinitary rewriting, the Bohm model of the lambda calculus can be seen as an infinitary term model. In contrast to term rewriting, there are several different possible notions of infinite tc1m, which give rise to different Bohm-like models, which embody different notions of lazy or cager computation.},
  langid = {english}
}
% == BibTeX quality report for kennawayInfinitaryLambdaCalculus1997:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{kennawayMeaninglessTermsRewriting1999,
  title = {Meaningless Terms in Rewriting},
  author = {Kennaway, R. and van Oostrom, V. and de Vries, F.-J.},
  year = {1999},
  month = may,
  journal = {Artificial Intelligence Preprint Series},
  volume = {3},
  url = {https://dspace.library.uu.nl/handle/1874/26465},
  urldate = {2022-07-09},
  abstract = {We present an axiomatic approach to the concept of meaninglessness
 in finite and transfinite term rewriting and lambda calculus. We justify
 our axioms in several ways. They can be intuitively justified from the
 viewpoint of rewriting as computation. They are shown to imply important
 properties of meaninglessness: genericity of the class of meaningless
 terms, confluence modulo equality of meaningless terms, the consistency
 of equating all meaningless terms, and the construction of Böhm trees
 and models of rewrite systems. Finally, we show that they can be easily
 verified for many existing notions of meaninglessness, and easily refuted
 for some notions that are known not to be good characterisations of meaninglessness.},
  copyright = {Open Access (free)},
  langid = {english},
  annotation = {Accepted: 2008-03-06T08:01:52Z}
}
% == BibTeX quality report for kennawayMeaninglessTermsRewriting1999:
% ? unused Library catalog ("dspace.library.uu.nl")

@inproceedings{kennawayTransfiniteReductionsOrthogonal1991,
  title = {Transfinite Reductions in Orthogonal Term Rewriting Systems},
  booktitle = {Rewriting {{Techniques}} and {{Applications}}},
  author = {Kennaway, J. R. and Klop, J. W. and Sleep, M. R. and {de Vries}, F. J.},
  editor = {Book, Ronald V.},
  year = {1991},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--12},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-53904-2_81},
  url = {https://ir.cwi.nl/pub/2017/2017D.pdf},
  abstract = {Strongly convergent reduction is the fundamental notion of reduction in infinitary orthogonal term rewriting systems (OTRSs). For these we prove the Transfinite Parallel Moves Lemma and the Compressing Lemma. Strongness is necessary as shown by counterexamples. Normal forms, which we allow to be infinite, are unique, in contrast to ω-normal forms. Strongly converging fair reductions result in normal forms.In general OTRSs the infinite Church-Rosser Property fails for strongly converging reductions. However for Böhm reduction (as in Lambda Calculus, subterms without head normal forms may be replaced by ⊥) the infinite Church-Rosser property does hold. The infinite Church-Rosser Property for non-unifiable OTRSs follows. The top-terminating OTRSs of Dershowitz c.s. are examples of non-unifiable OTRSs.},
  isbn = {978-3-540-46383-2},
  langid = {english},
  keywords = {68Q50,Böhm Trees,F4.1,F4.2,head normal forms,infinitary rewriting,infinite Church-Rosser Properties,non-unifiable term rewriting systems,normal forms,orthogonal term rewriting systems,strong converging reductions}
}
% == BibTeX quality report for kennawayTransfiniteReductionsOrthogonal1991:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Springer Link")

@inproceedings{kennedyCompilingContinuationsContinued2007,
  ids = {kennedyCompilingContinuationsContinued},
  title = {Compiling with Continuations, Continued},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  author = {Kennedy, Andrew},
  year = {2007},
  month = oct,
  series = {{{ICFP}} '07},
  pages = {177--190},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1291151.1291179},
  url = {https://doi.org/10.1145/1291151.1291179},
  urldate = {2023-05-03},
  abstract = {We present a series of CPS-based intermediate languages suitable for functional language compilation, arguing that they have practical benefits over direct-style languages based on A-normal form (ANF) or monads. Inlining of functions demonstrates the benefits most clearly: in ANF-based languages, inlining involves a re-normalization step that rearranges let expressions and possibly introduces a new 'join point' function, and in monadic languages, commuting conversions must be applied; in contrast, inlining in our CPS language is a simple substitution of variables for variables. We present a contification transformation implemented by simple rewrites on the intermediate language. Exceptions are modelled using so-called 'double-barrelled' CPS. Subtyping on exception constructors then gives a very straightforward effect analysis for exceptions. We also show how a graph-based representation of CPS terms can be implemented extremely efficiently, with linear-time term simplification.},
  isbn = {978-1-59593-815-2},
  langid = {english},
  keywords = {continuation passing style,continuations,functional programming languages,monads,optimizing compilation}
}
% == BibTeX quality report for kennedyCompilingContinuationsContinued2007:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@inproceedings{ketemaViciousCirclesRewriting2005,
  title = {Vicious Circles in Rewriting Systems},
  booktitle = {5th {{International Workshop}} on {{Reduction Strategies}} in {{Rewriting}} and {{Programming}}},
  author = {Ketema, Jeroen and Klop, Jan Willem and {van Oostrom}, Vincent},
  year = {2005},
  month = apr,
  pages = {20},
  address = {{Nara, Japan}},
  url = {https://www.dicosmo.org/WRS05/proceedings/jeroen.pdf},
  abstract = {We continue our study of the difference between Weak Normalisation (WN) and Strong Normalisation (SN). We extend our earlier result that orthogonal TRSs with the property WN do not admit cyclic reductions, into three distinct directions: (i) to the higher-order case, where terms may contain bound variables, (ii) to the weakly orthogonal case, where rules may have (trivial) conflicts, and (iii) to weak head normalisation (WHN), where terms have head normal forms. By adapting the techniques introduced for each of the three extensions separately, we even are able to show the result generalises to each pair of combinations and to various λ-calculi. The combination of all three extensions remains open however.},
  langid = {english}
}
% == BibTeX quality report for ketemaViciousCirclesRewriting2005:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Zotero")

@inproceedings{kiselyovBacktrackingInterleavingTerminating2005,
  title = {Backtracking, Interleaving, and Terminating Monad Transformers: (Functional Pearl)},
  shorttitle = {Backtracking, Interleaving, and Terminating Monad Transformers},
  booktitle = {Proceedings of the Tenth {{ACM SIGPLAN}} International Conference on {{Functional}} Programming  - {{ICFP}} '05},
  author = {Kiselyov, Oleg and Shan, Chung-chieh and Friedman, Daniel P. and Sabry, Amr},
  year = {2005},
  pages = {192},
  publisher = {{ACM Press}},
  address = {{Tallinn, Estonia}},
  doi = {10.1145/1086365.1086390},
  url = {http://portal.acm.org/citation.cfm?doid=1086365.1086390},
  urldate = {2022-07-15},
  isbn = {978-1-59593-064-4},
  langid = {english}
}
% == BibTeX quality report for kiselyovBacktrackingInterleavingTerminating2005:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the tenth ACM SIGPLAN international conference")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{kiselyovHowWriteSeemingly2002,
  title = {How to Write Seemingly Unhygienic and Referentially Opaque Macros with Syntax-Rules},
  booktitle = {Third {{Workshop}} on {{Scheme}} and {{Functional Programming}}},
  author = {Kiselyov, Oleg},
  year = {2002},
  month = oct,
  pages = {77},
  url = {https://okmij.org/ftp/Scheme/Dirty-Macros.pdf}
}
% == BibTeX quality report for kiselyovHowWriteSeemingly2002:
% ? Unsure about the formatting of the booktitle

@misc{kiselyovManyFacesFixedpoint2013,
  title = {Many Faces of the Fixed-Point Combinator},
  author = {Kiselyov, Oleg},
  year = {2013},
  month = aug,
  journal = {okmij.org},
  url = {http://okmij.org/ftp/Computation/fixed-point-combinators.html},
  urldate = {2020-07-31}
}
% == BibTeX quality report for kiselyovManyFacesFixedpoint2013:
% ? Possibly abbreviated journal title okmij.org

@phdthesis{klopCombinatoryReductionSystems1980,
  title = {Combinatory Reduction Systems},
  author = {Klop, Jan Willem},
  year = {1980},
  month = jun,
  url = {https://eprints.illc.uva.nl/id/eprint/1876/},
  urldate = {2022-09-24},
  abstract = {In the present work we are exclusively concerned with the study of syntactical properties of λ—calculus (λ, for short), Combinatory Logic  (CL), Recursive Program Schemes, and in general, Term Rewriting Systems with bound variables; especially those syntactical properties which concern reductions. Hence the title of this thesis; Combinatory Reduction Systems  (CRS's)  is the name by which we refer to Term Rewriting Systems plus bound variables. The word 'combinatory' seems justified to us since it captures the essential feature of these reduction systems: subterms in a CRS-term are manipulated in a 'combinatory way'.},
  school = {Rijksuniversiteit Utrecht}
}
% == BibTeX quality report for klopCombinatoryReductionSystems1980:
% ? unused Library catalog ("eprints.illc.uva.nl")
% ? unused Type ("PhD")

@inproceedings{klopExtendedTermRewriting1991,
  title = {Extended Term Rewriting Systems},
  booktitle = {Conditional and {{Typed Rewriting Systems}}},
  author = {Klop, Jan Willem and {de Vrijer}, Roel},
  editor = {Kaplan, S. and Okada, M.},
  year = {1991},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {26--50},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-54317-1_79},
  url = {https://ir.cwi.nl/pub/20015/20015A.pdf},
  abstract = {In this paper we will consider some extensions of the usual term rewrite format, namely: term rewriting with conditions, infinitary term rewriting and term rewriting with bound variables. Rather than aiming at a complete survey, we discuss some aspects of these three extensions.},
  isbn = {978-3-540-47558-3},
  langid = {english},
  keywords = {Combinatory Logic,Normal Form,Reduction Rule,Reduction Sequence,Unique Normal}
}
% == BibTeX quality report for klopExtendedTermRewriting1991:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Springer Link")

@inproceedings{klopInfinitaryNormalization2005,
  title = {Infinitary Normalization},
  booktitle = {We {{Will Show Them}}! {{Essays}} in {{Honour}} of {{Dov Gabbay}}, {{Volume Two}}},
  author = {Klop, Jan Willem and de Vrijer, Roel C.},
  editor = {Artëmov, Sergei N. and Barringer, Howard and d'Avila Garcez, Artur S. and Lamb, Luís C. and Woods, John},
  year = {2005},
  pages = {169--192},
  publisher = {{College Publications}}
}
% == BibTeX quality report for klopInfinitaryNormalization2005:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("DBLP Computer Science Bibliography")

@article{knuthStructuredProgrammingGo1974,
  ids = {knuthStructuredProgrammingGo1974a},
  title = {Structured Programming with Go to Statements},
  author = {Knuth, Donald E.},
  year = {1974},
  month = dec,
  journal = {ACM Computing Surveys (CSUR)},
  volume = {6},
  number = {4},
  pages = {261--301},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/356635.356640},
  url = {http://dl.acm.org/doi/10.1145/356635.356640},
  urldate = {2020-08-25},
  langid = {english}
}
% == BibTeX quality report for knuthStructuredProgrammingGo1974:
% ? unused Journal abbreviation ("ACM Comput. Surv.")
% ? unused Library catalog ("ACM Digital Library")

@phdthesis{krishnamurthiLinguisticReuse2001,
  title = {Linguistic Reuse},
  author = {Krishnamurthi, Shriram},
  year = {2001},
  url = {https://scholarship.rice.edu/handle/1911/17993},
  urldate = {2021-08-06},
  abstract = {Programmers employ a multitude of languages to build systems. Some are general-purpose languages. Others are specific to individual domains. These assist programmers with at least three different tasks: domain modeling, system validation and representing the structure of their general purpose program. As a result, programming languages have become key factors in the software engineering process. They are, however, rarely codified into the process and treated systematically. My dissertation develops a framework to treat programming languages as software engineering artifacts. In this framework, languages are identifiable, reusable entities that programmers can compose and link to produce larger languages; furthermore, languages themselves meet the properties of software components. Programmers can augment this lateral growth of languages with vertical growth, by producing languages that synthesize languages. Thus, software construction becomes a multi-phase process. In later phases, programmers use languages to build programs; in earlier phases, they employ languages to construct languages. This treatment of languages as artifacts addresses several open questions.},
  langid = {english},
  school = {Rice University},
  annotation = {Accepted: 2009-06-04T08:41:52Z}
}
% == BibTeX quality report for krishnamurthiLinguisticReuse2001:
% ? unused Library catalog ("scholarship.rice.edu")
% ? unused Type ("PhD")

@inproceedings{kristensenWhenWhyWhy2007,
  title = {The When, Why and Why Not of the {{BETA}} Programming Language},
  booktitle = {Proceedings of the Third {{ACM SIGPLAN}} Conference on {{History}} of Programming Languages},
  author = {Kristensen, Bent Bruun and Madsen, Ole Lehrmann and {Møller-Pedersen}, Birger},
  year = {2007},
  month = jun,
  series = {{{HOPL III}}},
  pages = {10-1--10-57},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1238844.1238854},
  url = {https://beta.alexandra.dk/sites/default/files/pdf/BETA-HOPL-V4.7_ref.pdf},
  urldate = {2023-08-04},
  abstract = {This paper tells the story of the development of BETA: a programming language with just one abstraction mechanism, instead of one abstraction mechanism for each kind of program element (classes, types, procedures, functions, etc.). The paper explains how this single abstraction mechanism, the pattern, came about and how it was designed to be so powerful that it covered the other mechanisms. In addition to describing the technical challenge of capturing all programming elements with just one abstraction mechanism, the paper also explains how the language was based upon a modeling approach, so that it could be used for analysis, design and implementation. It also illustrates how this modeling approach guided and settled the design of specific language concepts. The paper compares the BETA programming language with other languages and explains how such a minimal language can still support modeling, even though it does not have some of the language mechanisms found in other object-oriented languages. Finally, the paper tries to convey the organization, working conditions and social life around the BETA project, which turned out to be a lifelong activity for Kristen Nygaard, the authors of this paper, and many others.},
  isbn = {978-1-59593-766-7},
  keywords = {history of programming,object-oriented analysis,object-oriented design,object-oriented modeling,object-oriented programming,programming languages}
}
% == BibTeX quality report for kristensenWhenWhyWhy2007:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("HOPL-III '07: ACM SIGPLAN History of Programming Languages Conference III")
% ? unused Library catalog ("ACM Digital Library")

@misc{lafontLinearLogicPages,
  title = {Linear Logic Pages},
  author = {Lafont, Yves},
  url = {http://iml.univ-mrs.fr/~lafont/pub/llpages.pdf},
  langid = {english}
}

@article{lagerstromIdentifyingFactorsAffecting2012,
  title = {Identifying Factors Affecting Software Development Cost and Productivity},
  author = {Lagerström, Robert and {von Würtemberg}, Liv Marcks and Holm, Hannes and Luczak, Oscar},
  year = {2012},
  month = jun,
  journal = {Software Quality Journal},
  volume = {20},
  number = {2},
  pages = {395--417},
  issn = {0963-9314, 1573-1367},
  doi = {10.1007/s11219-011-9137-8},
  url = {http://link.springer.com/10.1007/s11219-011-9137-8},
  urldate = {2023-12-26},
  abstract = {Software systems of today are often complex, making development costs difficult to estimate. This paper uses data from 50 projects performed at one of the largest banks in Sweden to identify factors that have an impact on software development cost. Correlation analysis of the relationship between factor states and project costs was assessed using ANOVA and regression analysis. Ten out of the original 31 factors turned out to have an impact on software development project cost at the Swedish bank including the: number of function points, involved risk, number of budget revisions, primary platform, project priority, commissioning body’s unit, commissioning body, number of project participants, project duration, and number of consultants. In order to be able to compare projects of different size and complexity, this study also considers the software development productivity defined as the amount of function points per working hour in a project. The study at the bank indicates that the productivity is affected by factors such as performance of estimation and prognosis efforts, project type, number of budget revisions, existence of testing conductor, presentation interface, and number of project participants. A discussion addressing how the productivity factors relate to cost estimation models and their factors is presented. Some of the factors found to have an impact on cost are already included in estimation models such as COCOMO II, TEAMATe, and SEER-SEM, for instance function points and software platform. Thus, this paper validates these wellknown factors for cost estimation. However, several of the factors found in this study are not included in established models for software development cost estimation. Thus, this paper also provides indications for possible extensions of these models.},
  langid = {english}
}
% == BibTeX quality report for lagerstromIdentifyingFactorsAffecting2012:
% ? unused Journal abbreviation ("Software Qual J")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{leaJavaForkJoin2000,
  ids = {leaJavaForkJoin},
  title = {A {{Java}} Fork/Join Framework},
  booktitle = {Proceedings of the {{ACM}} 2000 Conference on {{Java Grande}}},
  author = {Lea, Doug},
  year = {2000},
  month = jun,
  series = {{{JAVA}} '00},
  pages = {36--43},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/337449.337465},
  url = {https://doi.org/10.1145/337449.337465},
  urldate = {2021-07-09},
  abstract = {This paper describes the design, implementation, and performance of a Java framework for supporting a style of parallel programming in which problems are solved by (recursively) splitting them into subtasks that are solved in parallel, waiting for them to complete, and then composing results. The general design is a variant of the work−stealing framework devised for Cilk. The main implementation techniques surround efficient construction and management of tasks queues and worker threads. The measured performance shows good parallel speedups for most programs, but also suggests possible improvements.},
  isbn = {978-1-58113-288-5},
  langid = {english}
}
% == BibTeX quality report for leaJavaForkJoin2000:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@inproceedings{levyJumboLcalculus2006,
  title = {Jumbo λ-Calculus},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Levy, Paul Blain},
  editor = {Bugliesi, Michele and Preneel, Bart and Sassone, Vladimiro and Wegener, Ingo},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {444--455},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11787006_38},
  url = {https://www.cs.bham.ac.uk/~pbl/papers/jumboicalp.pdf},
  abstract = {We make an argument that, for any study involving computational effects such as divergence or continuations, the traditional syntax of simply typed lambda-calculus cannot be regarded as canonical, because standard arguments for canonicity rely on isomorphisms that may not exist in an effectful setting. To remedy this, we define a “jumbo lambda-calculus” that fuses the traditional connectives together into more general ones, so-called “jumbo connectives”. We provide two pieces of evidence for our thesis that the jumbo formulation is advantageous.Firstly, we show that the jumbo lambda-calculus provides a “complete” range of connectives, in the sense of including every possible connective that, within the beta-eta theory, possesses a reversible rule.Secondly, in the presence of effects, we show that there is no decomposition of jumbo connectives into non-jumbo ones that is valid in both call-by-value and call-by-name.},
  isbn = {978-3-540-35908-1},
  langid = {english}
}
% == BibTeX quality report for levyJumboLcalculus2006:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Springer Link")

@inproceedings{mackieGeometryInteractionMachine1995,
  title = {The Geometry of Interaction Machine},
  booktitle = {Proceedings of the 22nd {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '95},
  author = {Mackie, Ian},
  year = {1995},
  pages = {198--208},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, United States}},
  doi = {10.1145/199448.199483},
  url = {http://portal.acm.org/citation.cfm?doid=199448.199483},
  urldate = {2023-04-28},
  isbn = {978-0-89791-692-9},
  langid = {english}
}
% == BibTeX quality report for mackieGeometryInteractionMachine1995:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 22nd ACM SIGPLAN-SIGACT symposium")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{madisonRepeatedListeningIncreases2017,
  title = {Repeated Listening Increases the Liking for Music Regardless of Its Complexity: Implications for the Appreciation and Aesthetics of Music},
  shorttitle = {Repeated Listening Increases the Liking for Music Regardless of Its Complexity},
  author = {Madison, Guy and Schiölde, Gunilla},
  year = {2017},
  month = mar,
  journal = {Frontiers in Neuroscience},
  volume = {11},
  pages = {147},
  issn = {1662-4548},
  doi = {10.3389/fnins.2017.00147},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5374342/},
  urldate = {2024-01-08},
  abstract = {Psychological and aesthetic theories predict that music is appreciated at optimal, peak levels of familiarity and complexity, and that appreciation of music exhibits an inverted U-shaped relationship with familiarity as well as complexity. Because increased familiarity conceivably leads to improved processing and less perceived complexity, we test whether there is an interaction between familiarity and complexity. Specifically, increased familiarity should render the music subjectively less complex, and therefore move the apex of the U curve toward greater complexity. A naturalistic listening experiment was conducted, featuring 40 music examples (ME) divided by experts into 4 levels of complexity prior to the main experiment. The MEs were presented 28 times each across a period of approximately 4 weeks, and individual ratings were assessed throughout the experiment. Ratings of liking increased monotonically with repeated listening at all levels of complexity; both the simplest and the most complex MEs were liked more as a function of listening time, without any indication of a U-shaped relation. Although the MEs were previously unknown to the participants, the strongest predictor of liking was familiarity in terms of having listened to similar music before, i.e., familiarity with musical style. We conclude that familiarity is the single most important variable for explaining differences in liking among music, regardless of the complexity of the music.},
  pmcid = {PMC5374342},
  pmid = {28408864}
}
% == BibTeX quality report for madisonRepeatedListeningIncreases2017:
% ? unused Journal abbreviation ("Front Neurosci")
% ? unused Library catalog ("PubMed Central")

@book{madsenObjectOrientedProgrammingBeta1993,
  title = {Object-{{Oriented}} Programming in the {{Beta}} Programming Language},
  author = {Madsen, Ole Lehrmann and {Moller-Pedersen}, Birger and Nygaard, Kristen},
  year = {1993},
  month = jan,
  edition = {2nd edition},
  publisher = {{Assn for Computing Machinery}},
  address = {{Wokingham, England ; Reading, Mass}},
  url = {https://beta.cs.au.dk/Books/betabook.pdf},
  abstract = {Object-oriented programming originated with the Simula language developed by Kristen Nygaard in Oslo in the 1960s. Now, from the birthplace of OOP, comes the new BETA programming language, for which this book is both tutorial and reference. It provides a clear introduction to the basic concepts of OOP and to more advanced topics.},
  isbn = {978-0-201-62430-4},
  langid = {english}
}
% == BibTeX quality report for madsenObjectOrientedProgrammingBeta1993:
% ? unused Library catalog ("Amazon")
% ? unused Number of pages ("400")

@inproceedings{madsenWhatObjectorientedProgramming1988,
  title = {What Object-Oriented Programming May Be - and What It Does Not Have to Be},
  booktitle = {{{ECOOP}} ’88 {{European Conference}} on {{Object-Oriented Programming}}},
  author = {Madsen, Ole Lehrmann and {Møller-Pedersen}, Birger},
  editor = {Gjessing, Stein and Nygaard, Kristen},
  year = {1988},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--20},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45910-3_1},
  url = {https://tidsskrift.dk/daimipb/article/view/7627},
  abstract = {A conceptual framework for object-oriented programming is presented. The framework is independent of specific programming language constructs. It is illustrated how this framework is reflected in an object-oriented language and the language mechanisms are compared with the corresponding elements of other object-oriented languages. Main issues of object-oriented programming are considered on the basis of the framework presented here.},
  isbn = {978-3-540-45910-1},
  langid = {english},
  keywords = {Action Sequence,Message Passing,Part Object,Pattern Attribute,Program Execution}
}
% == BibTeX quality report for madsenWhatObjectorientedProgramming1988:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Springer Link")

@article{maraistCallbynameCallbyvalueCallbyneed1995,
  title = {Call-by-Name, Call-by-Value, Call-by-Need, and the Linear Lambda Calculus},
  author = {Maraist, John and Odersky, Martin and Turner, David N. and Wadler, Philip},
  year = {1995},
  month = jan,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{MFPS XI}}, {{Mathematical Foundations}} of {{Programming Semantics}}, {{Eleventh Annual Conference}}},
  volume = {1},
  pages = {370--392},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00022-2},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104000222},
  urldate = {2020-08-23},
  abstract = {Girard described two translations of intuitionistic logic into linear logic, one where A → B maps to (!A) –○ B, and another where it maps to !(A –○ B). We detail the action of these translations on terms, and show that the first corresponds to a call-by-name calculus, while the second corresponds to call-by-value. We further show that if the target of the translation is taken to be an affine calculus, where ! controls contraction but weakening is allowed everywhere, then the second translation corresponds to a call-by-need calculus, as recently defined by Ariola, Felleisen, Maraist, Odersky and Wadler. Thus the different calling mechanisms can be explained in terms of logical translations, bringing them into the scope of the Curry-Howard isomorphism.},
  langid = {english}
}
% == BibTeX quality report for maraistCallbynameCallbyvalueCallbyneed1995:
% ? unused Library catalog ("ScienceDirect")

@inproceedings{marlowDesugaringHaskellDonotation2016,
  title = {Desugaring {{Haskell}}'s Do-Notation into Applicative Operations},
  booktitle = {Proceedings of the 9th {{International Symposium}} on {{Haskell}}},
  author = {Marlow, Simon and Peyton Jones, Simon and Kmett, Edward and Mokhov, Andrey},
  year = {2016},
  month = sep,
  series = {Haskell 2016},
  pages = {92--104},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2976002.2976007},
  url = {https://doi.org/10.1145/2976002.2976007},
  urldate = {2021-07-16},
  abstract = {Monads have taken the world by storm, and are supported by do-notation (at least in Haskell). Programmers are increasingly waking up to the usefulness and ubiquity of Applicatives, but they have so far been hampered by the absence of supporting notation. In this paper we show how to re-use the very same do-notation to work for Applicatives as well, providing efficiency benefits for some types that are both Monad and Applicative, and syntactic convenience for those that are merely Applicative. The result is fully implemented as an optional extension in GHC, and is in use at Facebook to make it easy to write highly-parallel queries in a distributed system.},
  isbn = {978-1-4503-4434-0},
  langid = {american},
  keywords = {Applicative Functors,concurrency,Monads,parallelism}
}
% == BibTeX quality report for marlowDesugaringHaskellDonotation2016:
% ? unused Library catalog ("ACM Digital Library")

@article{marlowMakingFastCurry2006,
  ids = {marlowMakingFastCurry2006a},
  title = {Making a Fast Curry: Push/Enter vs. Eval/Apply for Higher-Order Languages},
  shorttitle = {Making a Fast Curry},
  author = {Marlow, Simon and Jones, Simon Peyton},
  year = {2006},
  month = jul,
  journal = {Journal of Functional Programming},
  volume = {16},
  number = {4-5},
  pages = {415--449},
  publisher = {{Cambridge University Press}},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796806005995},
  url = {https://www.cambridge.org/core/product/identifier/S0956796806005995/type/journal_article},
  urldate = {2020-06-15},
  abstract = {Higher-order languages that encourage currying are typically implemented using one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition and qualitative judgements to choose one model or the other. Our goal in this paper is to provide, for the first time, a more substantial basis for this choice, based on our qualitative and quantitative experience of implementing both models in a state-of-the-art compiler for Haskell. Our conclusion is simple, and contradicts our initial intuition: compiled implementations should use eval/apply.},
  langid = {english}
}
% == BibTeX quality report for marlowMakingFastCurry2006:
% ? unused Journal abbreviation ("J. Funct. Prog.")
% ? unused Library catalog ("DOI.org (Crossref)")

@incollection{martiniFineStructureExponential1995,
  ids = {martiniFineStructureExponential1995a},
  title = {On the Fine Structure of the Exponential Rule},
  booktitle = {Advances in {{Linear Logic}}},
  author = {Martini, S. and Masini, A.},
  editor = {Girard, Jean-Yves and Lafont, Yves and Regnier, Laurent},
  year = {1995},
  pages = {197--210},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511629150.010},
  url = {https://pdfs.semanticscholar.org/b2cb/538c8ef21af42e48134a17a3c62ce5167837.pdf},
  urldate = {2020-08-01},
  abstract = {We present natural deduction systems for fragments of intuitionistic linear logic obtained by dropping weakening and contractions also on !-pre xed formulas. The systems are based on a twodimensional generalization of the notion of sequent, which accounts for a clean formulation of the introduction/elimination rules of the modality. Moreover, the di erent subsystems are obtained in a modular way, by simple conditions on the elimination rule for !. For the proposed systems we introduce a notion of reduction and we prove a normalization theorem.},
  isbn = {978-0-511-62915-0},
  langid = {english},
  note = {Proceedings of the Workshop on Linear Logic, Ithaca, New York, June 1993}
}
% == BibTeX quality report for martiniFineStructureExponential1995:
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{martinPEARLAgeThree1979,
  title = {{{PEARL}} at the Age of Three: {{Case}} Study of Development and Application of a Common High Order Realtime Programming Language},
  shorttitle = {{{PEARL}} at the Age of Three},
  booktitle = {Proceedings of the 4th International Conference on {{Software}} Engineering},
  author = {Martin, T.},
  year = {1979},
  month = sep,
  series = {{{ICSE}} '79},
  pages = {100--109},
  publisher = {{IEEE Press}},
  address = {{Munich, Germany}},
  urldate = {2023-12-30},
  abstract = {Programming language PEARL for industrial process automation has been developed since 1969. After extensive studies its final definition was frozen in 1976. Since then considerable experience has been gained with some 10 implementations and 140 applications. With PEARL at the age of three, these experiences are published in this paper for the first time and offered as a case study. The paper starts with a characterization of PEARL by indicating the requirements for such a tool, by classifying it with respect to other language types and by pleading for embedded realtime features. As PEARL has been submitted to ISO for international standardization, the question of how it meets the requirements set up by ISO is more thoroughly answered in Appendix A. The report of experience is given separately for implementation, application, and standardization with as much quantitative results as possible, including compiler behaviour, application areas, and economical benefit. For readers not yet familiar with PEARL a tutorial introduction by means of a simple but completely}
}
% == BibTeX quality report for martinPEARLAgeThree1979:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@article{mauborgneIncrementalUniqueRepresentation2000,
  title = {An Incremental Unique Representation for Regular Trees},
  author = {Mauborgne, Laurent},
  year = {2000},
  month = dec,
  journal = {Nordic Journal of Computing},
  volume = {7},
  number = {4},
  pages = {290--311},
  issn = {1236-6064},
  abstract = {In order to deal with infinite regular trees (or other pointed graph structures) efficiently, we give new algorithms to store such structures. The trees are stored in such a way that their representation is unique and shares substructures as much as possible. This maximal sharing allows substantial memory gain and speed up over previous techniques. For example, equality testing becomes constant time (instead of O(nlog(n))). The algorithms are incremental, and as such allow good reactive behavior. These new algorithms are then applied in a representation of sets of trees. The expressive power of this new representation is exactly what is needed by the original set-based analyses of Heintze and Jaffar [1990], or Heintze [1994].},
  langid = {english},
  keywords = {cartesian approximation,infinite trees,sharing,tree skeletons}
}
% == BibTeX quality report for mauborgneIncrementalUniqueRepresentation2000:
% ? unused Journal abbreviation ("Nordic J. of Computing")
% ? unused Library catalog ("Zotero")

@phdthesis{mauborgneRepresentationSetsTrees1999,
  title = {Representation of Sets of Trees for Abstract Interpretation},
  author = {Mauborgne, Laurent},
  year = {1999},
  month = nov,
  journal = {PhD Thesis},
  url = {http://software.imdea.org/~mauborgn/publi/t.pdf},
  school = {Ecole Polytechnique}
}
% == BibTeX quality report for mauborgneRepresentationSetsTrees1999:
% ? unused Library catalog ("Google Scholar")
% ? unused Number of pages ("197")

@article{maurerCompilingContinuations2016,
  ids = {maurerCompilingContinuation,maurerCompilingContinuations2016a},
  title = {Compiling without Continuations},
  author = {Maurer, Luke and Ariola, Zena and Downen, Paul and Jones, Simon Peyton},
  year = {2016},
  month = nov,
  url = {https://www.microsoft.com/en-us/research/publication/compiling-without-continuations/},
  urldate = {2020-05-13},
  abstract = {Many fields of study in compilers give rise to the concept of~ a join point — a place where different execution paths come together. ~ While they have often been treated by representing them as functions or~ continuations, we believe it is time to study them in their own right. We show~ that adding them …},
  langid = {american}
}
% == BibTeX quality report for maurerCompilingContinuations2016:
% Missing required field 'journal'
% ? unused Library catalog ("www.microsoft.com")

@article{maziarzHashingModuloAlphaEquivalence2021,
  title = {Hashing {{Modulo Alpha-Equivalence}}},
  author = {Maziarz, Krzysztof and Ellis, Tom and Lawrence, Alan and Fitzgibbon, Andrew and Jones, Simon Peyton},
  year = {2021},
  pages = {17},
  abstract = {Syntax Tree (AST), which represents computational expressions using a tree structure. Subtrees of such an AST —referred to as subexpressions — are useful, because they often correspond to semantically meaningful parts of the program, such as functions. Many applications need to quickly identify all equivalent subexpressions in an AST. Examples include common subexpression elimination (CSE), as mentioned above; structure sharing to save memory, by representing all occurrences of the same subexpression by a pointer to a single shared tree; or pre-processing for machine learning, where subexpression equivalence can be used as an additional feature, for example by turning an AST into a graph with equality links.},
  langid = {english}
}
% == BibTeX quality report for maziarzHashingModuloAlphaEquivalence2021:
% Missing required field 'journal'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Zotero")

@article{mccallNewLookNovice2019,
  title = {A New Look at Novice Programmer Errors},
  author = {McCall, Davin and Kölling, Michael},
  year = {2019},
  month = dec,
  journal = {ACM Transactions on Computing Education},
  volume = {19},
  number = {4},
  pages = {1--30},
  issn = {1946-6226},
  doi = {10.1145/3335814},
  url = {https://dl.acm.org/doi/10.1145/3335814},
  urldate = {2022-10-15},
  abstract = {The types of programming errors that novice programmers make and struggle to resolve have long been of interest to researchers. Various past studies have analyzed the frequency of compiler diagnostic messages. This information, however, does not have a direct correlation to the types of errors students make, due to the inaccuracy and imprecision of diagnostic messages. Furthermore, few attempts have been made to determine the severity of different kinds of errors in terms other than frequency of occurrence. Previously, we developed a method for meaningful categorization of errors, and produced a frequency distribution of these error categories; in this paper, we extend the previous method to also make a determination of error difficulty, in order to give a better measurement of the overall severity of different kinds of errors. An error category hierarchy was developed and validated, and errors in snapshots of students source code were categorized accordingly. The result is a frequency table of logical error categories rather than diagnostic messages. Resolution time for each of the analyzed errors was calculated, and the average resolution time for each category of error was determined; this defines an error difficulty score. The combination of frequency and difficulty allow us to identify the types of error that are most problematic for novice programmers. The results show that ranking errors by severity—a product of frequency and difficulty—yields a significantly different ordering than ranking them by frequency alone, indicating that error frequency by itself may not be a suitable indicator for which errors are actually the most problematic for students. CCS Concepts: • Social and professional topics → Computing education.},
  langid = {english}
}
% == BibTeX quality report for mccallNewLookNovice2019:
% ? unused Journal abbreviation ("ACM Trans. Comput. Educ.")
% ? unused Library catalog ("DOI.org (Crossref)")

@incollection{mcdermottExtendedCallbypushvalueReasoning2019,
  title = {Extended Call-by-Push-Value: Reasoning about Effectful Programs and Evaluation Order},
  shorttitle = {Extended Call-by-Push-Value},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {McDermott, Dylan and Mycroft, Alan},
  editor = {Caires, Luís},
  year = {2019},
  volume = {11423},
  pages = {235--262},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-17184-1_9},
  url = {http://link.springer.com/10.1007/978-3-030-17184-1_9},
  urldate = {2021-11-09},
  abstract = {Traditionally, reasoning about programs under varying evaluation regimes (call-by-value, call-by-name etc.) was done at the metalevel, treating them as term rewriting systems. Levy’s call-by-push-value (CBPV) calculus provides a more powerful approach for reasoning, by treating CBPV terms as a common intermediate language which captures both call-by-value and call-by-name, and by allowing equational reasoning about changes to evaluation order between or within programs. We extend CBPV to additionally deal with call-by-need, which is nontrivial because of shared reductions. This allows the equational reasoning to also support call-by-need. As an example, we then prove that callby-need and call-by-name are equivalent if nontermination is the only side-effect in the source language.},
  isbn = {978-3-030-17183-4 978-3-030-17184-1},
  langid = {english}
}
% == BibTeX quality report for mcdermottExtendedCallbypushvalueReasoning2019:
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Series title ("Lecture Notes in Computer Science")

@inproceedings{meyerovichSocioPLTPrinciplesProgramming2012,
  title = {Socio-{{PLT}}: Principles for Programming Language Adoption},
  shorttitle = {Socio-{{PLT}}},
  booktitle = {Proceedings of the {{ACM}} International Symposium on {{New}} Ideas, New Paradigms, and Reflections on Programming and Software - {{Onward}}! '12},
  author = {Meyerovich, Leo A. and Rabkin, Ariel S.},
  year = {2012},
  pages = {39},
  publisher = {{ACM Press}},
  address = {{Tucson, Arizona, USA}},
  doi = {10.1145/2384592.2384597},
  url = {http://dl.acm.org/citation.cfm?doid=2384592.2384597},
  urldate = {2020-06-15},
  abstract = {Why do some programming languages fail and others succeed? What does the answer tell us about programming language design, implementation, and principles? To help answer these and other questions, we argue for examining the sociological groundings of programming language theory: socio-PLT.},
  isbn = {978-1-4503-1562-3},
  langid = {english}
}
% == BibTeX quality report for meyerovichSocioPLTPrinciplesProgramming2012:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the ACM international symposium")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{miaraProgramIndentationComprehensibility1983,
  title = {Program Indentation and Comprehensibility},
  author = {Miara, Richard J. and Musselman, Joyce A. and Navarro, Juan A. and Shneiderman, Ben},
  year = {1983},
  month = nov,
  journal = {Communications of the ACM},
  volume = {26},
  number = {11},
  pages = {861--867},
  issn = {0001-0782},
  doi = {10.1145/182.358437},
  url = {https://dl.acm.org/doi/10.1145/182.358437},
  urldate = {2023-03-29},
  keywords = {indentation,program format,program readability}
}
% == BibTeX quality report for miaraProgramIndentationComprehensibility1983:
% ? unused Journal abbreviation ("Commun. ACM")
% ? unused Library catalog ("ACM Digital Library")

@incollection{middeldorpModularAspectsProperties1989,
  title = {Modular Aspects of Properties of Term Rewriting Systems Related to Normal Forms},
  booktitle = {Rewriting {{Techniques}} and {{Applications}}},
  author = {Middeldorp, Aart},
  editor = {Goos, G. and Hartmanis, J. and Barstow, D. and Brauer, W. and Brinch Hansen, P. and Gries, D. and Luckham, D. and Moler, C. and Pnueli, A. and Seegmüller, G. and Stoer, J. and Wirth, N. and Dershowitz, Nachum},
  year = {1989},
  volume = {355},
  pages = {263--277},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-51081-8_113},
  url = {http://link.springer.com/10.1007/3-540-51081-8_113},
  urldate = {2021-09-14},
  abstract = {In this paper we prove that the property of having unique normal forms is preserved under disjoint union. We show that two related properties do not exhibit this kind of modularity.},
  isbn = {978-3-540-51081-9 978-3-540-46149-4},
  langid = {english}
}
% == BibTeX quality report for middeldorpModularAspectsProperties1989:
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{mightParsingDerivativesFunctional2011,
  title = {Parsing with Derivatives: A Functional Pearl},
  shorttitle = {Parsing with Derivatives},
  author = {Might, Matthew and Darais, David and Spiewak, Daniel},
  year = {2011},
  month = sep,
  journal = {ACM SIGPLAN Notices},
  volume = {46},
  number = {9},
  pages = {189--195},
  issn = {0362-1340},
  doi = {10.1145/2034574.2034801},
  url = {https://doi.org/10.1145/2034574.2034801},
  urldate = {2023-05-18},
  abstract = {We present a functional approach to parsing unrestricted context-free grammars based on Brzozowski's derivative of regular expressions. If we consider context-free grammars as recursive regular expressions, Brzozowski's equational theory extends without modification to context-free grammars (and it generalizes to parser combinators). The supporting actors in this story are three concepts familiar to functional programmers - laziness, memoization and fixed points; these allow Brzozowski's original equations to be transliterated into purely functional code in about 30 lines spread over three functions. Yet, this almost impossibly brief implementation has a drawback: its performance is sour - in both theory and practice. The culprit? Each derivative can double the size of a grammar, and with it, the cost of the next derivative. Fortunately, much of the new structure inflicted by the derivative is either dead on arrival, or it dies after the very next derivative. To eliminate it, we once again exploit laziness and memoization to transliterate an equational theory that prunes such debris into working code. Thanks to this compaction, parsing times become reasonable in practice. We equip the functional programmer with two equational theories that, when combined, make for an abbreviated understanding and implementation of a system for parsing context-free languages.},
  keywords = {context-free grammar,derivative,formal languages,parser combinator,parsing,regular expressions}
}
% == BibTeX quality report for mightParsingDerivativesFunctional2011:
% ? unused Journal abbreviation ("SIGPLAN Not.")
% ? unused Library catalog ("ACM Digital Library")

@incollection{millerOverviewLinearLogic2004,
  title = {An Overview of Linear Logic Programming},
  booktitle = {Linear {{Logic}} in {{Computer Science}}},
  author = {Miller, Dale},
  editor = {Ehrhard, Thomas and Girard, Jean-Yves and Ruet, Paul and Scott, Philip},
  year = {2004},
  month = nov,
  edition = {1},
  pages = {119--150},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511550850.004},
  url = {https://www.cambridge.org/core/product/identifier/CBO9780511550850A013/type/book_part},
  urldate = {2021-04-02},
  abstract = {Logic programming can be given a foundation in sequent calculus by viewing computation as the process of building a cut-free sequent proof bottom-up. The first accounts of logic programming as proof search were given in classical and intuitionistic logic. Given that linear logic allows richer sequents and richer dynamics in the rewriting of sequents during proof search, it was inevitable that linear logic would be used to design new and more expressive logic programming languages. We overview how linear logic has been used to design such new languages and describe briefly some applications and implementation issues for them.},
  isbn = {978-0-521-60857-2 978-0-511-55085-0},
  langid = {english}
}
% == BibTeX quality report for millerOverviewLinearLogic2004:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{mokhovBuildSystemsCarte2020,
  ids = {mokhovBuildSystemsCarte2020a},
  title = {Build Systems à La Carte: Theory and Practice},
  shorttitle = {Build Systems à La Carte},
  author = {Mokhov, Andrey and Mitchell, Neil and Peyton Jones, Simon},
  year = {2020},
  journal = {Journal of Functional Programming},
  volume = {30},
  pages = {e11},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796820000088},
  url = {https://ndmitchell.com/downloads/paper-build_systems_a_la_carte_theory_and_practice-21_apr_2020.pdf},
  urldate = {2020-06-11},
  abstract = {Build systems are awesome, terrifying – and unloved. They are used by every developer around the world, but are rarely the object of study. In this paper, we offer a systematic, and executable, framework for developing and comparing build systems, viewing them as related points in a landscape rather than as isolated phenomena. By teasing apart existing build systems, we can recombine their components, allowing us to prototype new build systems with desired properties.},
  langid = {english}
}
% == BibTeX quality report for mokhovBuildSystemsCarte2020:
% ? unused Journal abbreviation ("J. Funct. Prog.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{monperrusExplainableSoftwareBot2019,
  title = {Explainable Software Bot Contributions: Case Study of Automated Bug Fixes},
  shorttitle = {Explainable Software Bot Contributions},
  booktitle = {2019 {{IEEE}}/{{ACM}} 1st {{International Workshop}} on {{Bots}} in {{Software Engineering}} ({{BotSE}})},
  author = {Monperrus, Martin},
  year = {2019},
  month = may,
  eprint = {1905.02597},
  primaryclass = {cs},
  pages = {12--15},
  doi = {10.1109/BotSE.2019.00010},
  url = {http://arxiv.org/abs/1905.02597},
  urldate = {2023-06-19},
  abstract = {In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering}
}
% == BibTeX quality report for monperrusExplainableSoftwareBot2019:
% ? Unsure about the formatting of the booktitle

@inproceedings{morrisettRefiningFirstclassStores1993,
  title = {Refining First-Class Stores},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} Workshop on State in Programming Languages},
  author = {Morrisett, J. Gregory},
  year = {1993},
  pages = {73--87},
  publisher = {{Citeseer}}
}
% == BibTeX quality report for morrisettRefiningFirstclassStores1993:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Google Scholar")

@article{naikTypeSystemEquivalent2008,
  title = {A Type System Equivalent to a Model Checker},
  author = {Naik, Mayur and Palsberg, Jens},
  year = {2008},
  month = aug,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {30},
  number = {5},
  pages = {1--24},
  issn = {0164-0925, 1558-4593},
  doi = {10.1145/1387673.1387678},
  url = {https://dl.acm.org/doi/10.1145/1387673.1387678},
  urldate = {2021-07-15},
  abstract = {Type systems and model checking are two prevalent approaches to program verification. A prominent difference between them is that type systems are typically defined in a syntactic and modular style whereas model checking is usually performed in a semantic and whole-program style. This difference between the two approaches makes them complementary to each other: type systems are good at explaining why a program was accepted while model checkers are good at explaining why a program was rejected.             We present a type system that is equivalent to a model checker for verifying temporal safety properties of imperative programs. The model checker is natural and may be instantiated with any finite-state abstraction scheme such as predicate abstraction. The type system, which is also parametric, type checks exactly those programs that are accepted by the model checker. It uses a variant of function types to capture flow sensitivity and intersection and union types to capture context sensitivity. Our result sheds light on the relationship between type systems and model checking, provides a methodology for studying their relative expressiveness, is a step towards sharing results between the two approaches, and motivates synergistic program analyses involving interplay between them.},
  langid = {english}
}
% == BibTeX quality report for naikTypeSystemEquivalent2008:
% ? unused Journal abbreviation ("ACM Trans. Program. Lang. Syst.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{najafiBisectingCommitsModeling2019,
  title = {Bisecting Commits and Modeling Commit Risk during Testing},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}  - {{ESEC}}/{{FSE}} 2019},
  author = {Najafi, Armin and Rigby, Peter C. and Shang, Weiyi},
  year = {2019},
  pages = {279--289},
  publisher = {{ACM Press}},
  address = {{Tallinn, Estonia}},
  doi = {10.1145/3338906.3338944},
  url = {https://users.encs.concordia.ca/~shang/pubs/Armin_FSE_2019.pdf},
  urldate = {2020-07-06},
  abstract = {Software testing is one of the costliest stages in the software development life cycle. One approach to reducing the test execution cost is to group changes and test them as a batch (i.e. batch testing). However, when tests fail in a batch, commits in the batch need to be re-tested to identify the cause of the failure, i.e. the culprit commit. The re-testing is typically done through bisection (i.e. a binary search through the commits in a batch). Intuitively, the effectiveness of batch testing highly depends on the size of the batch. Larger batches require fewer initial test runs, but have a higher chance of a test failure that can lead to expensive test re-runs to find the culprit. We are unaware of research that investigates and simulates the impact of batch sizes on the cost of testing in industry. In this work, we first conduct empirical studies on the effectiveness of batch testing in three large-scale industrial software systems at Ericsson. Using 9 months of testing data, we simulate batch sizes from 1 to 20 and find the most cost-effective BatchSize for each project. Our results show that batch testing saves 72\% of test executions compared to testing each commit individually. In a second simulation, we incorporate flaky tests that pass and fail on the same commit as they are a significant source of additional test executions on large projects. We model the degree of flakiness for each project and find that test flakiness reduces the cost savings to 42\%. In a third simulation, we guide bisection to reduce the likelihood of batch-testing failures. We model the riskiness of each commit in a batch using a bug model and a test execution history model. The risky commits are tested individually, while the less risky commits are tested in a single larger batch. Culprit predictions with our approach reduce test executions up to 9\% compared to Ericsson’s current bisection approach.},
  isbn = {978-1-4503-5572-8},
  langid = {english}
}
% == BibTeX quality report for najafiBisectingCommitsModeling2019:
% ? unused Conference name ("the 2019 27th ACM Joint Meeting")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{nigamAlgorithmicSpecificationsLinear2009,
  ids = {nigamAlgorithmicSpecificationsLinear2009a},
  title = {Algorithmic Specifications in Linear Logic with Subexponentials},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN}} Conference on {{Principles}} and Practice of Declarative Programming - {{PPDP}} '09},
  author = {Nigam, Vivek and Miller, Dale},
  year = {2009},
  pages = {129},
  publisher = {{ACM Press}},
  address = {{Coimbra, Portugal}},
  doi = {10.1145/1599410.1599427},
  url = {http://portal.acm.org/citation.cfm?doid=1599410.1599427},
  urldate = {2021-03-26},
  abstract = {The linear logic exponentials !, ? are not canonical: one can add to linear logic other such operators, say !l, ?l, which may or may not allow contraction and weakening, and where l is from some pre-ordered set of labels. We shall call these additional operators subexponentials and use them to assign locations to multisets of formulas within a linear logic programming setting. Treating locations as subexponentials greatly increases the algorithmic expressiveness of logic. To illustrate this new expressiveness, we show that focused proof search can be precisely linked to a simple algorithmic specification language that contains while-loops, conditionals, and insertion into and deletion from multisets. We also give some general conditions for when a focused proof step can be executed in constant time. In addition, we propose a new logical connective that allows for the creation of new subexponentials, thereby further augmenting the algorithmic expressiveness of logic.},
  isbn = {978-1-60558-568-0},
  langid = {english}
}
% == BibTeX quality report for nigamAlgorithmicSpecificationsLinear2009:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 11th ACM SIGPLAN conference")
% ? unused Library catalog ("DOI.org (Crossref)")

@misc{nishanovFibersMagnifyingGlass2018,
  title = {Fibers under the Magnifying Glass},
  author = {Nishanov, Gor},
  year = {2018},
  month = nov,
  url = {http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2018/p1364r0.pdf},
  urldate = {2021-11-04}
}

@book{normanLivingComplexity2010,
  title = {Living with Complexity},
  author = {Norman, Donald A.},
  year = {2010},
  month = jan,
  publisher = {{Mit Pr}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-01486-1},
  langid = {english}
}
% == BibTeX quality report for normanLivingComplexity2010:
% ? unused Library catalog ("Amazon")
% ? unused Number of pages ("298")

@article{norvigDesignPatternsDynamic1996,
  title = {Design Patterns in Dynamic Programming},
  author = {Norvig, Peter},
  year = {1996},
  month = may,
  langid = {english}
}
% == BibTeX quality report for norvigDesignPatternsDynamic1996:
% Missing required field 'journal'
% ? unused Library catalog ("Zotero")

@article{nougrahiyaIMOPSourcetosourceCompiler,
  title = {{{IMOP}}: A Source-to-Source Compiler Framework for {{OpenMP C}} Programs},
  author = {Nougrahiya, Aman and Nandivada, V Krishna},
  langid = {english}
}
% == BibTeX quality report for nougrahiyaIMOPSourcetosourceCompiler:
% Missing required field 'journal'
% Missing required field 'year'
% ? unused Library catalog ("Zotero")

@incollection{nygaardDevelopmentSIMULALanguages1978,
  title = {The Development of the {{SIMULA}} Languages},
  booktitle = {History of Programming Languages},
  author = {Nygaard, Kristen and Dahl, Ole-Johan},
  year = {1978},
  month = jun,
  pages = {439--480},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/800025.1198392},
  urldate = {2022-12-20},
  isbn = {978-0-12-745040-7}
}
% == BibTeX quality report for nygaardDevelopmentSIMULALanguages1978:
% ? unused Library catalog ("ACM Digital Library")

@misc{obrienDesignPatterns15,
  title = {Design Patterns 15 Years Later: An Interview},
  author = {O'Brien, Larry},
  journal = {InformIT},
  url = {https://www.informit.com/articles/article.aspx?p=1404056},
  urldate = {2023-07-26},
  collaborator = {Gamma, Erich and Helm, Richard and Johnson, Ralph}
}

@inproceedings{oderskySecondLookOverloading1995,
  title = {A Second Look at Overloading},
  booktitle = {Proceedings of the Seventh International Conference on {{Functional}} Programming Languages and Computer Architecture  - {{FPCA}} '95},
  author = {Odersky, Martin and Wadler, Philip and Wehr, Martin},
  year = {1995},
  pages = {135--146},
  publisher = {{ACM Press}},
  address = {{La Jolla, California, United States}},
  doi = {10.1145/224164.224195},
  url = {http://portal.acm.org/citation.cfm?doid=224164.224195},
  urldate = {2022-06-07},
  isbn = {978-0-89791-719-3},
  langid = {english}
}
% == BibTeX quality report for oderskySecondLookOverloading1995:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the seventh international conference")
% ? unused Library catalog ("DOI.org (Crossref)")

@book{odochertyObjectorientedAnalysisDesign2005,
  title = {Object-Oriented Analysis and Design: Understanding System Development with {{UML}} 2.0},
  shorttitle = {Object-Oriented Analysis and Design},
  author = {O'Docherty, Mike},
  year = {2005},
  publisher = {{Wiley}},
  address = {{Chichester, England ; Hoboken, NJ}},
  isbn = {978-0-470-09240-8},
  lccn = {QA76.64 .O35 2005},
  keywords = {Computer software,Development,Object-oriented programming (Computer science)}
}
% == BibTeX quality report for odochertyObjectorientedAnalysisDesign2005:
% ? unused Library catalog ("Library of Congress ISBN")
% ? unused Number of pages ("559")

@article{okasakiCallbyneedContinuationpassingStyle1994,
  ids = {okasakiCallbyneedContinuationpassingStyle1994a},
  title = {Call-by-Need and Continuation-Passing Style},
  author = {Okasaki, Chris and Lee, Peter and Tarditi, David},
  year = {1994},
  month = jan,
  journal = {LISP and Symbolic Computation},
  volume = {7},
  number = {1},
  pages = {57--81},
  issn = {1573-0557},
  doi = {10.1007/BF01019945},
  url = {https://www.researchgate.net/profile/Peter-Lee-88/publication/220606923_Call-by-Need_and_Continuation-Passing_Style/links/55633a2108ae8c0cab3509ba/Call-by-Need-and-Continuation-Passing-Style.pdf},
  urldate = {2023-05-02},
  abstract = {This paper examines the transformation of call-by-need λ terms into continuation-passing style (CPS). It begins by presenting a simple transformation of call-by-need λ terms into program graphs and a reducer for such graphs. From this, an informal derivation is carried out, resulting in a translation from λ terms into self-reducing program graphs, where the graphs are represented as CPS terms involving storage operations. Though informal, the derivation proceeds in simple steps, and the resulting translation is taken to be our canonical CPS transformation for call-by-need λ terms.},
  langid = {english},
  keywords = {Call-by-need,Continuation-passing Style,Continuations,Functional Programming,Lazy Evaluation}
}
% == BibTeX quality report for okasakiCallbyneedContinuationpassingStyle1994:
% ? unused Library catalog ("Springer Link")

@book{okasakiPurelyFunctionalData1998,
  title = {Purely Functional Data Structures},
  author = {Okasaki, Chris},
  year = {1998},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, U.K. ; New York}},
  isbn = {978-0-521-63124-2},
  langid = {english},
  lccn = {QA76.9.D35 O35 1998},
  keywords = {Data structures (Computer science),Functional programming languages}
}
% == BibTeX quality report for okasakiPurelyFunctionalData1998:
% ? unused Library catalog ("Library of Congress ISBN")
% ? unused Number of pages ("220")

@techreport{palaimoComparisonRADCNASA1982,
  title = {A Comparison of {{RADC}} and {{NASA}}/{{SEL}} Software Development Data},
  author = {Palaimo, John},
  year = {1982},
  month = dec,
  number = {ADA131756},
  url = {https://apps.dtic.mil/sti/citations/ADA131756},
  urldate = {2023-12-30},
  abstract = {In September of 1978, Richard Nelson of Rome Air Development Center RADC completed a report entitled Software Data Collection and Analysis in which he examined several statistical relationships within the RADC Software Productivity Database. The relationships studied attempted to relate the size of a software project with various other metrics describing the development process The seven primary relationships studied by Nelson are given below Project Size vs. Productivity source lines per manmonth Project Size vs. Development Effort manmonths Project Size vs. Development Duration months Project Size vs. Average Manloading manmonths per month Project Size vs. Number of Errors Project Size vs. Spatial Error Rate number of errors per 1000 source lines Project Size vs. Effort Based Error Rate number of errors per 10 manmonths of development effort. This report summarizes the results of a similar examination of all but one of these relationships when data from the NASASEL databases is merged with the RADC data. The relationship between Project Size and Average Manloading will not be examined because of the different methods used on computing this metric for the two databases. However, another possible relationship, given below, is examined. Project Size vs. Temporal Error Rate number of errors per month of development time.},
  chapter = {Technical Reports},
  langid = {english}
}
% == BibTeX quality report for palaimoComparisonRADCNASA1982:
% Missing required field 'institution'
% ? unused Library catalog ("apps.dtic.mil")
% ? unused Series title ("DACS technical monograph series")

@inproceedings{pankratiusCombiningFunctionalImperative2012,
  title = {Combining Functional and Imperative Programming for Multicore Software: {{An}} Empirical Study Evaluating {{Scala}} and {{Java}}},
  shorttitle = {Combining Functional and Imperative Programming for Multicore Software},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Pankratius, Victor and Schmidt, Felix and Garreton, Gilda},
  year = {2012},
  month = jun,
  pages = {123--133},
  publisher = {{IEEE}},
  address = {{Zurich}},
  doi = {10.1109/ICSE.2012.6227200},
  url = {https://web.archive.org/web/20120704064618id_/http://www.rz.uni-karlsruhe.de/~kb95/papers/pankratius-Scala-ICSE12-preprint.pdf},
  urldate = {2022-11-10},
  abstract = {Recent multi-paradigm programming languages combine functional and imperative programming styles to make software development easier. Given today’s proliferation of multicore processors, parallel programmers are supposed to benefit from this combination, as many difficult problems can be expressed more easily in a functional style while others match an imperative style. Due to a lack of empirical evidence from controlled studies, however, important software engineering questions are largely unanswered. Our paper is the first to provide thorough empirical results by using Scala and Java as a vehicle in a controlled comparative study on multicore software development. Scala combines functional and imperative programming while Java focuses on imperative sharedmemory programming. We study thirteen programmers who worked on three projects, including an industrial application, in both Scala and Java. In addition to the resulting 39 Scala programs and 39 Java programs, we obtain data from an industry software engineer who worked on the same project in Scala. We analyze key issues such as effort, code, language usage, performance, and programmer satisfaction. Contrary to popular belief, the functional style does not lead to bad performance. Average Scala run-times are comparable to Java, lowest run-times are sometimes better, but Java scales better on parallel hardware. We confirm with statistical significance Scala’s claim that Scala code is more compact than Java code, but clearly refute other claims of Scala on lower programming effort and lower debugging effort. Our study also provides explanations for these observations and shows directions on how to improve multi-paradigm languages in the future.},
  isbn = {978-1-4673-1066-6 978-1-4673-1067-3},
  langid = {english}
}
% == BibTeX quality report for pankratiusCombiningFunctionalImperative2012:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("2012 34th International Conference on Software Engineering (ICSE 2012)")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{parkesComplexityofstrategicbehaviorComparisonSchulze2021,
  title = {A Complexity-of-Strategic-Behavior Comparison between {{Schulze}}'s Rule and Ranked Pairs},
  author = {Parkes, David and Xia, Lirong},
  year = {2021},
  month = sep,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {26},
  number = {1},
  pages = {1429--1435},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v26i1.8258},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/8258},
  urldate = {2024-01-07},
  abstract = {Schulze’s rule and ranked pairs are two Condorcet methods that both satisfy many natural axiomatic properties. Schulze’s rule is used in the elections of many organizations, including the Wikimedia Foundation, the Pirate Party of Sweden and Germany, the Debian project, and the Gento Project. Both rules are immune to control by cloning alternatives, but little is otherwise known about their strategic robustness, including resistance to manipulation by one or more voters, control by adding or deleting alternatives, adding or deleting votes, and bribery. Considering computational barriers, we show that these types of strategic behavior are NP-hard for ranked pairs (both constructive, in making an alternative a winner, and destructive, in precluding an alternative from being a winner). Schulze’s rule, in comparison, remains vulnerable at least to constructive manipulation by a single voter and destructive manipulation by a coalition. As the first such polynomialtime rule known to resist all such manipulations, and considering also the broad axiomatic support, ranked pairs seems worthwhile to consider for practical applications.},
  langid = {english}
}
% == BibTeX quality report for parkesComplexityofstrategicbehaviorComparisonSchulze2021:
% ? unused Journal abbreviation ("AAAI")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{parnasAbstractTypesDefined1976,
  title = {Abstract Types Defined as Classes of Variables},
  booktitle = {Proceedings of the 1976 Conference on {{Data}} : {{Abstraction}}, Definition and Structure},
  author = {Parnas, D. L. and Shore, John E. and Weiss, David},
  year = {1976},
  month = mar,
  pages = {149--154},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800237.807133},
  url = {https://dl.acm.org/doi/10.1145/800237.807133},
  urldate = {2023-08-29},
  abstract = {The concept of “type” has been used without a precise definition in discussions about programming languages for 20 years. Before the concept of user defined data types was introduced, a definition was not necessary for discussions of specific programming languages. The meaning of the term was implicit in the small list of possible types supported by the language. There was even enough similarity between different languages so that this form of definition allowed discussions of languages in general. The need for a widely accepted definition of type became clear in discussions of languages that allow users to add to the set of possible types without altering the compiler. In such languages, the concept of type is no longer implicitly defined by the set of built-in types. A consistent language must be based on a clearer definition of the notion of type than we now have.},
  isbn = {978-1-4503-7898-7}
}
% == BibTeX quality report for parnasAbstractTypesDefined1976:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@article{parreauxUltimateConditionalSyntax,
  title = {The {{Ultimate Conditional Syntax}}},
  author = {Parreaux, Lionel},
  abstract = {ML-language dialects and related typically support expressive pattern-matching syntaxes which allow programmers to write concise, expressive, and type-safe code to manipulate algebraic data types. Many features have been proposed to enhance the expressiveness of these pattern-matching syntaxes, such as pattern bindings, pattern alternatives (aka disjunction), pattern conjunction, view patterns, pattern guards, ‘if-let’ patterns, multi-way if-expressions, etc.},
  langid = {english}
}
% == BibTeX quality report for parreauxUltimateConditionalSyntax:
% Missing required field 'journal'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Zotero")

@article{peytonjonesSecretsGlasgowHaskell2002,
  ids = {jonesSecretsGlasgowHaskell2002},
  title = {Secrets of the {{Glasgow Haskell Compiler}} Inliner},
  author = {Peyton Jones, Simon and Marlow, Simon},
  year = {2002},
  month = jul,
  journal = {Journal of Functional Programming},
  volume = {12},
  number = {4},
  pages = {393--434},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796802004331},
  url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2002/07/inline.pdf},
  urldate = {2020-07-01},
  abstract = {Higher-order languages, such as Haskell, encourage the programmer to build abstractions by composing functions. A good compiler must inline many of these calls to recover an e ciently executable program.},
  langid = {english}
}
% == BibTeX quality report for peytonjonesSecretsGlasgowHaskell2002:
% ? unused Journal abbreviation ("J. Funct. Prog.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{peytonjonesSemanticsImpreciseExceptions1999,
  ids = {jonesSemanticsImpreciseException,jonesSemanticsImpreciseExceptions1999},
  title = {A Semantics for Imprecise Exceptions},
  author = {Peyton Jones, Simon and Reid, Alastair and Henderson, Fergus and Hoare, Tony and Marlow, Simon},
  year = {1999},
  month = may,
  journal = {ACM SIGPLAN Notices},
  volume = {34},
  number = {5},
  pages = {25--36},
  issn = {0362-1340},
  doi = {10.1145/301631.301637},
  url = {https://doi.org/10.1145/301631.301637},
  urldate = {2022-07-25},
  abstract = {Some modern superscalar microprocessors provide only imprecise exceptions. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance.}
}
% == BibTeX quality report for peytonjonesSemanticsImpreciseExceptions1999:
% ? unused Journal abbreviation ("SIGPLAN Not.")
% ? unused Library catalog ("May 1999")

@incollection{pfenningChurchCurryCombining2008,
  title = {Church and {{Curry}}: Combining Intrinsic and Extrinsic Typing},
  booktitle = {Reasoning in Simple Type Theory: {{Festschrift}} in Honour of {{Peter B}}. {{Andrews}} on His 70th Birthday},
  author = {Pfenning, Frank},
  editor = {Benzmüller, Christoph and Brown, Chad E. and Siekmann, Jörg and Statman, Richard},
  year = {2008},
  month = dec,
  series = {Studies in Logic: {{Mathematical}} Logic and Foundations},
  volume = {17},
  pages = {288--323},
  publisher = {{College Publications}},
  address = {{London}},
  url = {https://www.cs.cmu.edu/~fp/papers/andrews08.pdf},
  isbn = {978-1-904987-70-3},
  langid = {english},
  note = {Includes bibliographical references
\par
unpublished}
}
% == BibTeX quality report for pfenningChurchCurryCombining2008:
% ? unused Library catalog ("K10plus ISBN")

@article{pippengerPureImpureLisp1997,
  title = {Pure versus Impure {{Lisp}}},
  author = {Pippenger, Nicholas},
  year = {1997},
  month = mar,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {19},
  number = {2},
  pages = {223--238},
  issn = {0164-0925, 1558-4593},
  doi = {10.1145/244795.244798},
  url = {https://dl.acm.org/doi/10.1145/244795.244798},
  urldate = {2022-01-06},
  langid = {english}
}
% == BibTeX quality report for pippengerPureImpureLisp1997:
% ? unused Journal abbreviation ("ACM Trans. Program. Lang. Syst.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{pradelGoodBadUgly2015,
  title = {The Good, the Bad, and the Ugly: An Empirical Study of Implicit Type Conversions in {{JavaScript}}},
  shorttitle = {The Good, the Bad, and the Ugly},
  booktitle = {29th {{European Conference}} on {{Object-Oriented Programming}} ({{ECOOP}} 2015)},
  author = {Pradel, Michael and Sen, Koushik},
  editor = {Boyland, John Tang},
  year = {2015},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {37},
  pages = {519--541},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.ECOOP.2015.519},
  url = {http://drops.dagstuhl.de/opus/volltexte/2015/5236},
  urldate = {2022-07-25},
  isbn = {978-3-939897-86-6},
  langid = {english},
  keywords = {{000 Computer science, knowledge, general works},Computer Science,Dynamically typed languages,JavaScript,Type coercions,Types},
  note = {\subsection{Other}

Most popular programming languages support situations where a value of one type is converted into a value of another type without any explicit cast. Such implicit type conversions, or type coercions, are a highly controversial language feature. Proponents argue that type coercions enable writing concise code. Opponents argue that type coercions are error-prone and that they reduce the understandability of programs. This paper studies the use of type coercions in JavaScript, a language notorious for its widespread use of coercions. We dynamically analyze hundreds of programs, including real-world web applications and popular  benchmark programs. We find that coercions are widely used (in 80.42\% of all function executions) and that most coercions are likely to be harmless (98.85\%). Furthermore, we identify a set of rarely occurring and potentially harmful coercions that safer subsets of JavaScript or future language designs may want to disallow. Our results suggest that type coercions are significantly less evil than commonly assumed and that analyses targeted at real-world JavaScript programs must consider coercions.}
}
% == BibTeX quality report for pradelGoodBadUgly2015:
% ? Unsure about the formatting of the booktitle
% ? unused Artwork size ("23 pages")
% ? unused Library catalog ("Dagstuhl Research Online Publication Server")
% ? unused Medium ("application/pdf")

@article{precheltEmpiricalComparisonJava2000,
  title = {An Empirical Comparison of {{C}}, {{C}}++, {{Java}}, {{Perl}}, {{Python}}, {{Rexx}}, and {{Tcl}} for a Search/String-Processing Program.},
  author = {Prechelt, Lutz},
  year = {2000},
  month = apr,
  abstract = {80 implementations of the same set of requirements, created by 74 different programmers in various languages, are compared for several properties, such as run time, memory consumption, source text length, comment density, program structure, reliability, and the amount of effort required for writing them. The results indicate that, for the given programming problem, ``scripting languages'' (Perl, Python, Rexx, Tcl) are more productive than conventional languages. In terms of run time and memory consumption, they often turn out better than Java and not much worse than C or C++. In general, the differences between languages tend to be smaller than the typical differences due to different programmers within the same language.}
}
% == BibTeX quality report for precheltEmpiricalComparisonJava2000:
% Missing required field 'journal'
% ? unused Library catalog ("ResearchGate")

@techreport{precheltPlat_FormsWebDevelopment2007,
  title = {Plat\_{{Forms}}: The Web Development Platform Comparison — Evaluation and Results},
  author = {Prechelt, Lutz},
  year = {2007},
  month = jun,
  number = {B-07-10},
  institution = {{Institut für Informatik, Freie Universität Berlin}},
  abstract = {Plat\_Forms” is a competition in which top-class teams of three professional programmers competed to implement the same requirements for a web-based system within 30 hours, each team using a different technology platform (Java EE, PHP, or Perl). Plat\_Forms intends to provide new insights into the real (rather than purported) pros, cons, and emergent properties of each platform.},
  langid = {english}
}
% == BibTeX quality report for precheltPlat_FormsWebDevelopment2007:
% ? unused Library catalog ("Zotero")

@techreport{proustASAPStaticPossible2017,
  title = {{{ASAP}}: {{As Static As Possible}} Memory Management},
  author = {Proust, Raphaël L},
  year = {2017},
  month = jul,
  number = {UCAM-CL-TR-908},
  pages = {145},
  institution = {{University of Cambridge Computer Laboratory}},
  url = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.pdf},
  langid = {english}
}
% == BibTeX quality report for proustASAPStaticPossible2017:
% ? unused Library catalog ("Zotero")

@article{pulteSimplifyingARMConcurrency2017,
  title = {Simplifying {{ARM}} Concurrency: Multicopy-Atomic Axiomatic and Operational Models for {{ARMv8}}},
  shorttitle = {Simplifying {{ARM}} Concurrency},
  author = {Pulte, Christopher and Flur, Shaked and Deacon, Will and French, Jon and Sarkar, Susmit and Sewell, Peter},
  year = {2017},
  month = dec,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {POPL},
  pages = {19:1--19:29},
  doi = {10.1145/3158107},
  url = {https://doi.org/10.1145/3158107},
  urldate = {2021-07-19},
  abstract = {ARM has a relaxed memory model, previously specified in informal prose for ARMv7 and ARMv8. Over time, and partly due to work building formal semantics for ARM concurrency, it has become clear that some of the complexity of the model is not justified by the potential benefits. In particular, the model was originally non-multicopy-atomic: writes could become visible to some other threads before becoming visible to all — but this has not been exploited in production implementations, the corresponding potential hardware optimisations are thought to have insufficient benefits in the ARM context, and it gives rise to subtle complications when combined with other ARMv8 features. The ARMv8 architecture has therefore been revised: it now has a multicopy-atomic model. It has also been simplified in other respects, including more straightforward notions of dependency, and the architecture now includes a formal concurrency model. In this paper we detail these changes and discuss their motivation. We define two formal concurrency models: an operational one, simplifying the Flowing model of Flur et al., and the axiomatic model of the revised ARMv8 specification. The models were developed by an academic group and by ARM staff, respectively, and this extended collaboration partly motivated the above changes. We prove the equivalence of the two models. The operational model is integrated into an executable exploration tool with new web interface, demonstrated by exhaustively checking the possible behaviours of a loop-unrolled version of a Linux kernel lock implementation, a previously known bug due to unprevented speculation, and a fixed version.},
  keywords = {Axiomatic,Operational,Relaxed Memory Models,Semantics}
}
% == BibTeX quality report for pulteSimplifyingARMConcurrency2017:
% ? unused Journal abbreviation ("Proc. ACM Program. Lang.")
% ? unused Library catalog ("January 2018")

@article{quinceyImplicationsAdoptingPlane2016,
  title = {Implications of Adopting Plane Angle as a Base Quantity in the {{SI}}},
  author = {Quincey, Paul and Brown, Richard J. C.},
  year = {2016},
  month = jun,
  journal = {Metrologia},
  volume = {53},
  number = {3},
  eprint = {1604.02373},
  pages = {998--1002},
  publisher = {{IOP Publishing}},
  issn = {0026-1394, 1681-7575},
  doi = {10.1088/0026-1394/53/3/998},
  url = {http://arxiv.org/abs/1604.02373},
  urldate = {2022-04-08},
  abstract = {The treatment of angles within the SI is anomalous compared with other quantities, and there is a case for removing this anomaly by declaring plane angle to be an additional base quantity within the system. It is shown that this could bring several benefits in terms of treating angle on an equal basis with other metrics, removing potentially harmful ambiguities, and bringing SI units more in line with concepts in basic physics, but at the expense of significant upheaval to familiar equations within mathematics and physics. This paper sets out the most important of these changes so that an alternative unit system containing angle as a base quantity can be seen in the round, irrespective of whether it is ever widely adopted. The alternative formulas and units can be treated as the underlying, more general equations of mathematical physics, independent of the units used for angle, which are conventionally simplified by implicitly assuming that the unit used for angle is the radian.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Physics - Classical Physics,{Physics - Data Analysis, Statistics and Probability}},
  note = {Comment: 9 pages; accepted by Metrologia}
}

@article{quinceyRangeOptionsHandling2016,
  title = {The Range of Options for Handling Plane Angle and Solid Angle within a System of Units},
  author = {Quincey, Paul},
  year = {2016},
  month = apr,
  journal = {Metrologia},
  volume = {53},
  number = {2},
  pages = {840--845},
  issn = {0026-1394, 1681-7575},
  doi = {10.1088/0026-1394/53/2/840},
  url = {https://iopscience.iop.org/article/10.1088/0026-1394/53/2/840},
  urldate = {2022-04-10}
}
% == BibTeX quality report for quinceyRangeOptionsHandling2016:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{quinceyReplyCommentAngles2022,
  title = {Reply to Comment on `{{Angles}} in the {{SI}}: A Detailed Proposal for Solving the Problem'},
  shorttitle = {Reply to {{Comment}} on `{{Angles}} in the {{SI}}},
  author = {Quincey, Paul},
  year = {2022},
  month = may,
  journal = {Metrologia},
  volume = {59},
  number = {3},
  pages = {038002},
  publisher = {{IOP Publishing}},
  issn = {0026-1394},
  doi = {10.1088/1681-7575/ac5434},
  url = {https://doi.org/10.1088/1681-7575/ac5434},
  urldate = {2022-05-21},
  abstract = {The comment by Leonard (2022 Metrologia 59 038001) primarily proposes that if angle is treated as a base quantity, with the radian as its base unit, it would be wrong to change the units for torque (from N m to J rad−1), angular momentum (from J s to J s rad−1) and moment of inertia (from kg m2 to kg m2 rad−2), as was proposed in the letter being commented on (Quincey 2021 Metrologia 58 053002). This reply clarifies the situation by looking directly at the consequences of the two proposals. Apart from the comfort of retaining the familiar units for these quantities, the benefit of Leonard’s proposal would be the preservation of a few favoured equations used in specific situations, while the general relationships between many physical quantities would need to change. The revised units proposed in the letter would leave all the established general relationships unchanged, and are the best option for allowing the longstanding problem of angles being wrongly treated as numbers within the SI to be resolved. This reply includes some historical context, which describes how Euler implicitly introduced the idea that ‘the radian is another name for the number one’ into the mathematics used for rotating objects, at a time long before anyone had thought about unit systems.},
  langid = {english}
}
% == BibTeX quality report for quinceyReplyCommentAngles2022:
% ? unused Library catalog ("Institute of Physics")

@inproceedings{ramalheteEfficientAlgorithmsPersistent2021,
  title = {Efficient Algorithms for Persistent Transactional Memory},
  booktitle = {Proceedings of the 26th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Ramalhete, Pedro and Correia, Andreia and Felber, Pascal},
  year = {2021},
  month = feb,
  series = {{{PPoPP}} '21},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3437801.3441586},
  url = {https://doi.org/10.1145/3437801.3441586},
  urldate = {2021-11-11},
  abstract = {Durable techniques coupled with transactional semantics provide to application developers the guarantee that data is saved consistently in persistent memory (PM), even in the event of a non-corrupting failure. Persistence fences and flush instructions are known to have a significant impact on the throughput of persistent transactions. In this paper we explore different trade-offs in terms of memory usage vs. number of fences and flushes. We present two new algorithms, named Trinity and Quadra, for durable transactions on PM and implement each of them in the form of a user-level library persistent transactional memory (PTM). Quadra achieves the lower bound with respect to the number of persistence fences and executes one flush instruction per modified cache line. Trinity can be easily combined with concurrency control techniques based on fine grain locking, and we have integrated it with our TL2 adaptation, with eager locking and write-through update strategy. Moreover, the combination of Trinity and TL2 into a PTM provides good scalability for data structures and workloads with a disjoint access pattern. We used this disjoint PTM to implement a key-value (KV) store with durable linearizable transactions. When compared with previous work, our TL2 KV store provides better throughput in nearly all experiments.},
  isbn = {978-1-4503-8294-6},
  keywords = {crash resilience,persistent memory,transactions}
}
% == BibTeX quality report for ramalheteEfficientAlgorithmsPersistent2021:
% ? unused Library catalog ("ACM Digital Library")

@article{rivasNotionsComputationMonoids2014,
  title = {Notions of Computation as Monoids},
  author = {Rivas, Exequiel and Jaskelioff, Mauro},
  year = {2014},
  month = may,
  journal = {Journal of Functional Programming},
  volume = {27},
  publisher = {{Cambridge University Press}},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796817000132},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/notions-of-computation-as-monoids/70019FC0F2384270E9F41B9719042528},
  urldate = {2022-06-08},
  abstract = {There are different notions of computation, the most popular being monads, applicative functors, and arrows. In this article, we show that these three notions can be seen as instances of a unifying abstract concept: monoids in monoidal categories. We demonstrate that even when working at this high level of generality, one can obtain useful results. In particular, we give conditions under which one can obtain free monoids and Cayley representations at the level of monoidal categories, and we show that their concretisation results in useful constructions for monads, applicative functors, and arrows. Moreover, by taking advantage of the uniform presentation of the three notions of computation, we introduce a principled approach to the analysis of the relation between them.},
  langid = {english}
}

@inproceedings{sabryReasoningProgramsContinuationpassing1992,
  title = {Reasoning about Programs in Continuation-Passing Style.},
  booktitle = {Proceedings of the 1992 {{ACM}} Conference on {{LISP}} and Functional Programming},
  author = {Sabry, Amr and Felleisen, Matthias},
  year = {1992},
  month = jan,
  series = {{{LFP}} '92},
  pages = {288--298},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/141471.141563},
  url = {https://dl.acm.org/doi/10.1145/141471.141563},
  urldate = {2023-04-21},
  abstract = {Plotkin's λ-value calculus is sound but incomplete for reasoning about βeegr;-transformations on programs in continuation-passing style (CPS). To find a complete extension, we define a new, compactifying CPS transformation and an “inverse”mapping, un-CPS, both of which are interesting in their own right. Using the new CPS transformation, we can determine the precise language of CPS terms closed under β7eegr;-transformations. Using the un-CPS transformation, we can derive a set of axioms such that every equation between source programs is provable if and only if βη can prove the corresponding equation between CPS programs. The extended calculus is equivalent to an untyped variant of Moggi's computational λ-calculus.},
  isbn = {978-0-89791-481-9}
}
% == BibTeX quality report for sabryReasoningProgramsContinuationpassing1992:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@article{sammetWhyAdaNot1986,
  title = {Why {{Ada}} Is Not Just Another Programming Language},
  author = {Sammet, Jean E},
  year = {1986},
  journal = {Communications of the ACM},
  volume = {29},
  number = {8},
  langid = {english}
}
% == BibTeX quality report for sammetWhyAdaNot1986:
% ? unused Library catalog ("Zotero")

@article{scottSPPFStyleParsingEarley2008,
  ids = {scottSPPFStyleParsingEarley2008a},
  title = {{{SPPF-Style}} Parsing from {{Earley}} Recognisers},
  author = {Scott, Elizabeth},
  year = {2008},
  month = apr,
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = {203},
  number = {2},
  pages = {53--67},
  issn = {15710661},
  doi = {10.1016/j.entcs.2008.03.044},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1571066108001497},
  urldate = {2020-06-15},
  abstract = {In its recogniser form, Earley’s algorithm for testing whether a string can be derived from a grammar is worst case cubic on general context free grammars (CFG). Earley gave an outline of a method for turning his recognisers into parsers, but it turns out that this method is incorrect. Tomita’s GLR parser returns a shared packed parse forest (SPPF) representation of all derivations of a given string from a given CFG but is worst case unbounded polynomial order. We have given a modified worst-case cubic version, the BRNGLR algorithm, that, for any string and any CFG, returns a binarised SPPF representation of all possible derivations of a given string. In this paper we apply similar techniques to develop two versions of an Earley parsing algorithm that, in worst-case cubic time, return an SPPF representation of all derivations of a given string from a given CFG.},
  langid = {english}
}
% == BibTeX quality report for scottSPPFStyleParsingEarley2008:
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{severiDecomposingLatticeMeaningless2011,
  title = {Decomposing the Lattice of Meaningless Sets in the Infinitary Lambda Calculus},
  booktitle = {Logic, {{Language}}, {{Information}} and {{Computation}}},
  author = {Severi, Paula and {de Vries}, Fer-Jan},
  editor = {Beklemishev, Lev D. and {de Queiroz}, Ruy},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {210--227},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-20920-8_22},
  abstract = {The notion of a meaningless set has been defined for infinitary lambda calculus axiomatically. Standard examples of such sets are sets of terms that have no head normal form, the set of terms without weak head normal form and the set of rootactive terms. In this paper, we study the way the intervals decompose as union of more elementary ones. We also analyse the distribution of the sets of meaningless terms in the lattice by selecting some sets as key vertices and study the cardinality in the intervals between key vertices. As an application, we prove that the lattice of meaningless sets is neither distributive nor modular. Interestingly, the example translates into a simple counterexample that the lattice of lambda theories is not modular.},
  isbn = {978-3-642-20920-8},
  langid = {english},
  keywords = {Complete Lattice,Induction Hypothesis,Lambda Calculus,Normal Form,Reduction Sequence}
}
% == BibTeX quality report for severiDecomposingLatticeMeaningless2011:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Springer Link")

@article{sewellX86TSORigorousUsable2010,
  title = {X86-{{TSO}}: A Rigorous and Usable Programmer's Model for X86 Multiprocessors},
  shorttitle = {X86-{{TSO}}},
  author = {Sewell, Peter and Sarkar, Susmit and Owens, Scott and Nardelli, Francesco Zappa and Myreen, Magnus O.},
  year = {2010},
  month = jul,
  journal = {Communications of the ACM},
  volume = {53},
  number = {7},
  pages = {89--97},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1785414.1785443},
  url = {https://dl.acm.org/doi/10.1145/1785414.1785443},
  urldate = {2021-07-09},
  abstract = {Exploiting the multiprocessors that have recently become ubiquitous requires high-performance and reliable concurrent systems code, for concurrent data structures, operating system kernels, synchronisation libraries, compilers, and so on. However, concurrent programming, which is always challenging, is made much more so by two problems. First, real multiprocessors typically do not provide the sequentially consistent memory that is assumed by most work on semantics and verification. Instead, they have relaxed memory models, varying in subtle ways between processor families, in which different hardware threads may have only loosely consistent views of a shared memory. Second, the public vendor architectures, supposedly specifying what programmers can rely on, are often in ambiguous informal prose (a particularly poor medium for loose specifications), leading to widespread confusion.},
  langid = {english}
}
% == BibTeX quality report for sewellX86TSORigorousUsable2010:
% ? unused Journal abbreviation ("Commun. ACM")
% ? unused Library catalog ("DOI.org (Crossref)")

@misc{shalBuildSystemRules2009,
  title = {Build System Rules and Algorithms},
  author = {Shal, Mike},
  year = {2009},
  publisher = {{gittup.org}},
  url = {http://gittup.org/tup/build_system_rules_and_algorithms.pdf},
  urldate = {2021-01-22}
}

@phdthesis{shamirFixedpointsRecursiveDefinitions1976,
  title = {The Fixedpoints of Recursive Definitions},
  author = {Shamir, Adi},
  year = {1976},
  month = oct,
  address = {{Rehovot}},
  url = {https://weizmann.primo.exlibrisgroup.com/permalink/972WIS_INST/1d4esio/alma990002185270203596},
  collaborator = {Manna, Zohar},
  langid = {english},
  school = {Weizmann Institute of Science},
  keywords = {WISOA}
}
% == BibTeX quality report for shamirFixedpointsRecursiveDefinitions1976:
% ? unused Number of pages ("316")
% ? unused Type ("PhD")

@article{shirahataFixpointTheoremLinear1999,
  title = {Fixpoint Theorem in Linear Set Theory},
  author = {Shirahata, Masaru},
  year = {1999},
  month = dec,
  pages = {10},
  abstract = {In this paper, we first show that the fixpoint term can be constructed for any formula in the system of linear set theory with equality and pairing. We then prove that all the total recursive functions are numeralwise representable in such a system. Furthermore, we observe that the additive infinitary extension of the system would become inconsistent if the extensionality principle were added.},
  langid = {english}
}
% == BibTeX quality report for shirahataFixpointTheoremLinear1999:
% Missing required field 'journal'
% ? unused Library catalog ("Zotero")

@article{shirahataLinearConservativeExtension1996,
  title = {A Linear Conservative Extension of {{Zermelo-Fraenkel}} Set Theory},
  author = {Shirahata, Masaru},
  year = {1996},
  month = may,
  journal = {Studia Logica},
  volume = {56},
  number = {3},
  pages = {361--392},
  issn = {0039-3215, 1572-8730},
  doi = {10.1007/BF00372772},
  url = {http://link.springer.com/10.1007/BF00372772},
  urldate = {2021-03-06},
  abstract = {In this paper, we develop the system LZF of set theory with the unrestricted comprehension in full linear logic and show that LZF is a conservative extension of ZFi.e., the Zermelo-Fraenkel set theory without the axiom of regularity. We formulate LZF as a sequent calculus with abstraction terms and prove the partial cut-elimination theorem for it. The cut-elimination result ensures the subterm property for those formulas which contain only terms corresponding to sets in ZF-. This implies that LZF is a conservative extension of ZF- and therefore the former is consistent relative to the latter.},
  langid = {english}
}
% == BibTeX quality report for shirahataLinearConservativeExtension1996:
% ? unused Journal abbreviation ("Stud Logica")
% ? unused Library catalog ("DOI.org (Crossref)")

@phdthesis{shirahataLinearSetTheory1994,
  title = {Linear {{Set Theory}}},
  author = {Shirahata, Masaru},
  year = {1994},
  month = feb,
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.7077&rep=rep1&type=pdf},
  lccn = {9430011},
  school = {Stanford University}
}
% == BibTeX quality report for shirahataLinearSetTheory1994:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("ProQuest Dissertations Publishing")
% ? unused Type ("PhD Thesis")

@inproceedings{shirahataLinearSetTheory1998,
  title = {Linear Set Theory with Strict Comprehension},
  booktitle = {Proceedings of the {{Sixth Asian Logic Conference}}},
  author = {Shirahata, Masaru},
  year = {1998},
  month = apr,
  pages = {223--245},
  publisher = {{WORLD SCIENTIFIC / S'PORE UNIV PRESS (PTE) LTD}},
  address = {{Beijing, China}},
  doi = {10.1142/9789812812940_0013},
  url = {http://www.worldscientific.com/doi/abs/10.1142/9789812812940_0013},
  urldate = {2021-03-06},
  abstract = {In this paper, we study the extensionality axiom in the set theory with the unrestricted comprehension based on linear logic. We rst review Grishin's result which shows the imcompatibility of the extensionality axiom and the unrestricted comprehesion in linear set theory. As one way to rectify this situation, we introduce the notion of \textbackslash strict comprehension" and formulate a system of linear set theory which contains the extensionality and the strict comprehesion. The consistency of such a system is then proved by a simple cut-elimination argument.},
  isbn = {978-981-02-3432-4 978-981-281-294-0},
  langid = {english}
}
% == BibTeX quality report for shirahataLinearSetTheory1998:
% ? unused Library catalog ("DOI.org (Crossref)")

@techreport{shiversBottomupVreductionUplinks2004,
  title = {Bottom-up β-Reduction: Uplinks and λ-{{DAGs}} (Extended Version)},
  shorttitle = {Bottom-up β-Reduction},
  author = {Shivers, Olin and Wand, Mitchell},
  year = {2004},
  institution = {{Citeseer}}
}
% == BibTeX quality report for shiversBottomupVreductionUplinks2004:
% ? unused Library catalog ("Google Scholar")

@inproceedings{shiVirtualMachineShowdown2005,
  ids = {shiVirtualMachineShowdown,shiVirtualMachineShowdown2008},
  title = {Virtual Machine Showdown: Stack versus Registers},
  shorttitle = {Virtual Machine Showdown},
  booktitle = {Proceedings of the 1st {{ACM}}/{{USENIX}} International Conference on {{Virtual}} Execution Environments},
  author = {Shi, Yunhe and Gregg, David and Beatty, Andrew and Ertl, M. Anton},
  year = {2005},
  month = jun,
  series = {{{VEE}} '05},
  pages = {153--163},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1064979.1065001},
  url = {https://dl.acm.org/doi/10.1145/1328195.1328197},
  urldate = {2021-06-29},
  abstract = {Virtual machines (VMs) are commonly used to distribute programs in an architecture-neutral format, which can easily be interpreted or compiled. A long-running question in the design of VMs is whether stack architecture or register architecture can be implemented more efficiently with an interpreter. We extend existing work on comparing virtual stack and virtual register architectures in two ways. Firstly, our translation from stack to register code is much more sophisticated. The result is that we eliminate an average of more than 47\% of executed VM instructions, with the register machine bytecode size only 25\% larger than that of the corresponding stack bytecode. Secondly we present an implementation of a register machine in a fully standard-compliant implementation of the Java VM. We find that, on the Pentium 4, the register architecture requires an average of 32.3\% less time to execute standard benchmarks if dispatch is performed using a C switch statement. Even if more efficient threaded dispatch is available (which requires labels as first class values), the reduction in running time is still approximately 26.5\% for the register architecture.},
  isbn = {978-1-59593-047-7},
  keywords = {interpreter,Interpreter,register architecture,stack architecture,virtual machine}
}
% == BibTeX quality report for shiVirtualMachineShowdown2005:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@article{shulmanLinearLogicConstructive2018,
  title = {Linear Logic for Constructive Mathematics},
  author = {Shulman, Michael},
  year = {2018},
  month = may,
  journal = {arXiv:1805.07518 [math]},
  eprint = {1805.07518},
  primaryclass = {math},
  url = {http://arxiv.org/abs/1805.07518},
  urldate = {2021-03-04},
  abstract = {We show that numerous distinctive concepts of constructive mathematics arise automatically from an interpretation of "linear higher-order logic" into intuitionistic higher-order logic via a Chu construction. This includes apartness relations, complemented subsets, anti-subgroups and anti-ideals, strict and non-strict order pairs, cut-valued metrics, and apartness spaces. We also explain the constructive bifurcation of classical concepts using the choice between multiplicative and additive linear connectives. Linear logic thus systematically "constructivizes" classical definitions and deals automatically with the resulting bookkeeping, and could potentially be used directly as a basis for constructive mathematics in place of intuitionistic logic.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Logic},
  note = {Comment: 39 pages}
}
% == BibTeX quality report for shulmanLinearLogicConstructive2018:
% ? Possibly abbreviated journal title arXiv:1805.07518 [math]

@phdthesis{shuttFexprsBasisLisp2010,
  title = {Fexprs as the Basis of {{Lisp}} Function Application or \$vau : The Ultimate Abstraction},
  author = {Shutt, John N},
  year = {2010},
  month = aug,
  url = {https://web.wpi.edu/Pubs/ETD/Available/etd-090110-124904/},
  langid = {english},
  school = {WORCESTER POLYTECHNIC INSTITUTE}
}
% == BibTeX quality report for shuttFexprsBasisLisp2010:
% ? unused Library catalog ("Zotero")
% ? unused Number of pages ("416")
% ? unused Type ("PhD Thesis")

@article{siegmundMeasuringModelingProgramming2014,
  title = {Measuring and Modeling Programming Experience},
  author = {Siegmund, Janet and Kästner, Christian and Liebig, Jörg and Apel, Sven and Hanenberg, Stefan},
  year = {2014},
  month = oct,
  journal = {Empirical Software Engineering},
  volume = {19},
  number = {5},
  pages = {1299--1334},
  issn = {1573-7616},
  doi = {10.1007/s10664-013-9286-4},
  url = {https://doi.org/10.1007/s10664-013-9286-4},
  urldate = {2023-02-13},
  abstract = {Programming experience is an important confounding parameter in controlled experiments regarding program comprehension. In literature, ways to measure or control programming experience vary. Often, researchers neglect it or do not specify how they controlled for it. We set out to find a well-defined understanding of programming experience and a way to measure it. From published comprehension experiments, we extracted questions that assess programming experience. In a controlled experiment, we compare the answers of computer-science students to these questions with their performance in solving program-comprehension tasks. We found that self estimation seems to be a reliable way to measure programming experience. Furthermore, we applied exploratory and confirmatory factor analyses to extract and evaluate a model of programming experience. With our analysis, we initiate a path toward validly and reliably measuring and describing programming experience to better understand and control its influence in program-comprehension experiments.},
  langid = {english},
  keywords = {Controlled experiments,Programming experience,Questionnaire}
}
% == BibTeX quality report for siegmundMeasuringModelingProgramming2014:
% ? unused Journal abbreviation ("Empir Software Eng")
% ? unused Library catalog ("Springer Link")

@inproceedings{simonsenWeakConvergenceUniform2010,
  title = {Weak Convergence and Uniform Normalization in Infinitary Rewriting},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Rewriting Techniques}} and {{Applications}}},
  author = {Simonsen, Jakob Grue},
  editor = {Lynch, Christopher},
  year = {2010},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {6},
  pages = {311--324},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.RTA.2010.311},
  url = {http://drops.dagstuhl.de/opus/volltexte/2010/2660},
  urldate = {2022-07-25},
  abstract = {We study infinitary term rewriting systems containing finitely many rules. For these, we show that if a weakly convergent reduction is not strongly convergent, it contains a term that reduces to itself in one step (but the step itself need not be part of the reduction). Using this result, we prove the starkly surprising result that for any orthogonal system with finitely many rules, the system is weakly normalizing under weak convergence iff it is strongly normalizing under weak convergence iff it is weakly normalizing under strong convergence iff it is strongly normalizing under strong convergence.},
  isbn = {978-3-939897-18-7},
  langid = {english},
  keywords = {{000 Computer science, knowledge, general works},Computer Science,Infinitary rewriting,uniform normalization,weak convergence},
  note = {\subsection{Other}

We study infinitary term rewriting systems containing finitely many rules. For these, we show that if a weakly convergent reduction is not strongly convergent, it contains a term that reduces to itself in one step (but the step itself need not be part of the reduction). Using this result, we prove
the starkly surprising result 
that for any orthogonal system with finitely many rules, the system is
weakly normalizing under weak convergence if\{f\} it is strongly normalizing under weak convergence if\{f\} it is weakly normalizing under strong convergence if\{f\} it is strongly normalizing under strong convergence.

As further corollaries, we derive a number of new results for weakly convergent rewriting: Systems with finitely many rules enjoy unique normal forms, and acyclic orthogonal systems are confluent. Our results suggest that it may be possible to recover some of the positive results for strongly convergent rewriting in the setting of weak convergence, if systems with finitely many rules are considered. Finally, we give a number of counterexamples showing failure of most of the results when infinite sets of rules are allowed.}
}
% == BibTeX quality report for simonsenWeakConvergenceUniform2010:
% ? unused Artwork size ("14 pages")
% ? unused Library catalog ("Dagstuhl Research Online Publication Server")
% ? unused Medium ("application/pdf")

@article{singerFunctionalBabyTalk2018,
  title = {Functional Baby Talk: Analysis of Code Fragments from Novice {{Haskell}} Programmers},
  shorttitle = {Functional Baby Talk},
  author = {Singer, Jeremy and Archibald, Blair},
  year = {2018},
  month = may,
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {270},
  pages = {37--51},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.270.3},
  url = {http://arxiv.org/abs/1805.05126v1},
  urldate = {2022-05-23},
  langid = {english}
}
% == BibTeX quality report for singerFunctionalBabyTalk2018:
% ? unused Journal abbreviation ("Electron. Proc. Theor. Comput. Sci.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{snowdonAccurateRuntimePrediction2007,
  title = {Accurate Run-Time Prediction of Performance Degradation under Frequency Scaling},
  booktitle = {2007 Workshop on Operating System Platforms for Embedded Real-Time Applications},
  author = {Snowdon, David and Van Der Linden, Godfrey and Petters, Stefan and Heiser, Gernot},
  year = {2007},
  publisher = {{NICTA}},
  issn = {1833-9646},
  doi = {10.26190/unsworks/517},
  url = {http://hdl.handle.net/1959.4/39905},
  urldate = {2023-08-07},
  abstract = {Dynamic voltage and frequency scaling is employed to minimise energy consumption in mobile devices. The energy required to execute a piece of software is highly depedent on its execution time, and devices are typically subject to timeliness or quality-of-service constraints. For both these reasons, the performance at a proposed frequency setpoint must be accurately estimated. The frequently-made assumption that performance scales linearly with core frequency has shown to be incorrect, and better performance models are required which take into account the effects, and frequency setting, of the memory architecture. This paper presents a methodology, based on off-line hardware characterisation and runtime workload characterisation, for the generation of an execution time model. Its evaluation shows that it provides a highly accurate (to within 2\% on average) prediction of performance at arbitrary frequency settings and that the models can be used to implement operating-system level dynamic voltage and frequency scaling schemes for embedded systems.},
  langid = {english}
}
% == BibTeX quality report for snowdonAccurateRuntimePrediction2007:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("unsworks.unsw.edu.au")

@article{snyderEncapsulationInheritanceObjectoriented1986,
  title = {Encapsulation and Inheritance in Object-Oriented Programming Languages},
  author = {Snyder, Alan},
  year = {1986},
  month = nov,
  journal = {ACM SIGPLAN Notices},
  volume = {21},
  number = {11},
  pages = {38--45},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/960112.28702},
  url = {https://dl.acm.org/doi/10.1145/960112.28702},
  urldate = {2022-12-26},
  abstract = {Object-oriented programming is a practical and useful programming methodology that encourages modular design and software reuse. Most object-oriented programming languages support               data abstraction               by preventing an object from being manipulated except via its defined external operations. In most languages, however, the introduction of               inheritance               severely compromises the benefits of this encapsulation. Furthermore, the use of inheritance itself is globally visible in most languages, so that changes to the inheritance hierarchy cannot be made safely. This paper examines the relationship between inheritance and encapsulation and develops requirements for full support of encapsulation with inheritance.},
  langid = {english}
}
% == BibTeX quality report for snyderEncapsulationInheritanceObjectoriented1986:
% ? unused Journal abbreviation ("SIGPLAN Not.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{sparksSuperstructuralReversibleLogic2014,
  title = {Superstructural Reversible Logic},
  booktitle = {3rd {{International Workshop}} on {{Linearity}}},
  author = {Sparks, Z A and Sabry, Amr},
  year = {2014},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.720.5692&rep=rep1&type=pdf},
  langid = {english}
}
% == BibTeX quality report for sparksSuperstructuralReversibleLogic2014:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Zotero")

@article{sperberGenerationLRParsers2000,
  title = {Generation of {{LR}} Parsers by Partial Evaluation},
  author = {Sperber, Michael and Thiemann, Peter},
  year = {2000},
  month = mar,
  journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume = {22},
  number = {2},
  pages = {224--264},
  issn = {0164-0925, 1558-4593},
  doi = {10.1145/349214.349219},
  url = {http://dl.acm.org/doi/10.1145/349214.349219},
  urldate = {2020-06-15},
  langid = {english}
}
% == BibTeX quality report for sperberGenerationLRParsers2000:
% ? unused Journal abbreviation ("ACM Trans. Program. Lang. Syst.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{staplesFormalSpecificationsBetter2013,
  title = {Formal Specifications Better than Function Points for Code Sizing},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Staples, Mark and Kolanski, Rafal and Klein, Gerwin and Lewis, Corey and Andronick, June and Murray, Toby and Jeffery, Ross and Bass, Len},
  year = {2013},
  month = may,
  pages = {1257--1260},
  publisher = {{IEEE}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1109/ICSE.2013.6606692},
  url = {http://ieeexplore.ieee.org/document/6606692/},
  urldate = {2023-12-27},
  abstract = {Size and effort estimation is a significant challenge for the management of large-scale formal verification projects. We report on an initial study of relationships between the sizes of artefacts from the development of seL4, a formally-verified embedded systems microkernel. For each API function we first determined its COSMIC Function Point (CFP) count (based on the seL4 user manual), then sliced the formal specifications and source code, and performed a normalised line count on these artefact slices. We found strong and significant relationships between the sizes of the artefact slices, but no significant relationships between them and the CFP counts. Our finding that CFP is poorly correlated with lines of code is based on just one system, but is largely consistent with prior literature. We find CFP is also poorly correlated with the size of formal specifications. Nonetheless, lines of formal specification correlate with lines of source code, and this may provide a basis for size prediction in future formal verification projects. In future work we will investigate proof sizing.},
  isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
  langid = {english}
}
% == BibTeX quality report for staplesFormalSpecificationsBetter2013:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{stefikEmpiricalComparisonAccuracy2011,
  title = {An Empirical Comparison of the Accuracy Rates of Novices Using the Quorum, Perl, and Randomo Programming Languages},
  booktitle = {Proceedings of the 3rd {{ACM SIGPLAN}} Workshop on {{Evaluation}} and Usability of Programming Languages and Tools - {{PLATEAU}} '11},
  author = {Stefik, Andreas and Siebert, Susanna and Stefik, Melissa and Slattery, Kim},
  year = {2011},
  pages = {3},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/2089155.2089159},
  url = {http://dl.acm.org/citation.cfm?doid=2089155.2089159},
  urldate = {2022-06-18},
  abstract = {We present here an empirical study comparing the accuracy rates of novices writing software in three programming languages: Quorum, Perl, and Randomo. The first language, Quorum, we call an evidence-based programming language, where the syntax, semantics, and API designs change in correspondence to the latest academic research and literature on programming language usability. Second, while Perl is well known, we call Randomo a Placebo-language, where some of the syntax was chosen with a random number generator and the ASCII table. We compared novices that were programming for the first time using each of these languages, testing how accurately they could write simple programs using common program constructs (e.g., loops, conditionals, functions, variables, parameters). Results showed that while Quorum users were afforded significantly greater accuracy compared to those using Perl and Randomo, Perl users were unable to write programs more accurately than those using a language designed by chance.},
  isbn = {978-1-4503-1024-6},
  langid = {english}
}
% == BibTeX quality report for stefikEmpiricalComparisonAccuracy2011:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 3rd ACM SIGPLAN workshop")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{stefikEmpiricalInvestigationProgramming2013,
  title = {An Empirical Investigation into Programming Language Syntax},
  author = {Stefik, Andreas and Siebert, Susanna},
  year = {2013},
  month = nov,
  journal = {ACM Transactions on Computing Education},
  volume = {13},
  number = {4},
  pages = {1--40},
  issn = {1946-6226, 1946-6226},
  doi = {10.1145/2534973},
  url = {https://dl.acm.org/doi/10.1145/2534973},
  urldate = {2021-02-13},
  langid = {english},
  keywords = {Novice Programmers,Programming Languages,Syntax}
}
% == BibTeX quality report for stefikEmpiricalInvestigationProgramming2013:
% ? unused Journal abbreviation ("ACM Trans. Comput. Educ.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{stentIntelligibilityFastSynthesized2011,
  title = {On the Intelligibility of Fast Synthesized Speech for Individuals with Early-Onset Blindness},
  booktitle = {The Proceedings of the 13th International {{ACM SIGACCESS}} Conference on {{Computers}} and Accessibility - {{ASSETS}} '11},
  author = {Stent, Amanda and Syrdal, Ann and Mishra, Taniya},
  year = {2011},
  pages = {211},
  publisher = {{ACM Press}},
  address = {{Dundee, Scotland, UK}},
  doi = {10.1145/2049536.2049574},
  url = {http://dl.acm.org/citation.cfm?doid=2049536.2049574},
  urldate = {2022-11-30},
  abstract = {People with visual disabilities increasingly use text-to-speech synthesis as a primary output modality for interaction with computers. Surprisingly, there have been no systematic comparisons of the performance of different text-to-speech systems for this user population. In this paper we report the results of a pilot experiment on the intelligibility of fast synthesized speech for individuals with early-onset blindness. Using an open-response recall task, we collected data on four synthesis systems representing two major approaches to text-to-speech synthesis: formant-based synthesis and concatenative unit selection synthesis. We found a significant effect of speaking rate on intelligibility of synthesized speech, and a trend towards significance for synthesizer type. In post-hoc analyses, we found that participant-related factors, including age and familiarity with a synthesizer and voice, also affect intelligibility of fast synthesized speech.},
  isbn = {978-1-4503-0920-2},
  langid = {english}
}
% == BibTeX quality report for stentIntelligibilityFastSynthesized2011:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("The proceedings of the 13th international ACM SIGACCESS conference")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{stirlingDecidabilityHigherorderMatching2009,
  title = {Decidability of Higher-Order Matching},
  author = {Stirling, Colin},
  editor = {Sassone, Vladimiro},
  year = {2009},
  month = jul,
  journal = {Logical Methods in Computer Science},
  volume = {5},
  number = {3},
  pages = {2},
  issn = {18605974},
  doi = {10.2168/LMCS-5(3:2)2009},
  url = {https://lmcs.episciences.org/757},
  urldate = {2021-11-15},
  abstract = {We show that the higher-order matching problem is decidable using a gametheoretic argument.},
  langid = {english}
}
% == BibTeX quality report for stirlingDecidabilityHigherorderMatching2009:
% ? unused Journal abbreviation ("Log.Meth.Comput.Sci.")
% ? unused Library catalog ("DOI.org (Crossref)")

@phdthesis{strakaFunctionalDataStructures2013,
  title = {Functional Data Structures and Algorithms},
  author = {Straka, Milan},
  year = {2013},
  month = sep,
  url = {https://dspace.cuni.cz/handle/20.500.11956/52896},
  urldate = {2022-01-04},
  abstract = {Functional programming is a well established programming paradigm and is becoming increasingly popular, even in industrial and commercial appli- cations. Data structures used in functional languages are principally persistent, that is, they preserve previous versions of themselves when modified. The goal of this work is to broaden the theory of persistent data structures and devise efficient implementations of data structures to be used in functional languages. Arrays are without any question the most frequently used data structure. Despite being conceptually very simple, no persistent array with constant time access operation exists. We describe a simplified implementation of a fully per- sistent array with asymptotically optimal amortized complexity Θ(log log n) and especially a nearly optimal worst-case implementation. Additionally, we show how to effectively perform a garbage collection on a persistent array. The most efficient data structures are not necessarily based on asymptotically best structures. On that account, we also focus on data structure...},
  langid = {american},
  school = {Computer Science Institute of Charles University},
  annotation = {Accepted: 2018-11-30T13:00:49Z}
}
% == BibTeX quality report for strakaFunctionalDataStructures2013:
% ? unused Library catalog ("dspace.cuni.cz")

@article{strassburgerDeepInferenceExpansion2019,
  title = {Deep Inference and Expansion Trees for Second-Order Multiplicative Linear Logic},
  author = {STRAßBURGER, Lutz},
  year = {2019},
  month = sep,
  journal = {Mathematical Structures in Computer Science},
  volume = {29},
  number = {8},
  pages = {1030--1060},
  issn = {0960-1295, 1469-8072},
  doi = {10.1017/S0960129518000385},
  url = {https://www.cambridge.org/core/product/identifier/S0960129518000385/type/journal_article},
  urldate = {2022-08-11},
  abstract = {In this paper, we introduce the notion of expansion tree for linear logic. As in Miller's original work, we have a shallow reading of an expansion tree that corresponds to the conclusion of the proof, and a deep reading which is a formula that can be proved by propositional rules. We focus our attention to MLL2, and we also present a deep inference system for that logic. This allows us to give a syntactic proof to a version of Herbrand's theorem.},
  langid = {english}
}
% == BibTeX quality report for strassburgerDeepInferenceExpansion2019:
% ? unused Journal abbreviation ("Math. Struct. Comp. Sci.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{stroustrupWhyNotJust1995,
  title = {Why {{C}}++ Is Not Just an Object-Oriented Programming Language},
  booktitle = {Addendum to the Proceedings of the 10th Annual Conference on {{Object-oriented}} Programming Systems, Languages, and Applications},
  author = {Stroustrup, Bjarne},
  year = {1995},
  month = oct,
  series = {{{OOPSLA}} '95},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/260094.260207},
  url = {https://www.stroustrup.com/oopsla.pdf},
  urldate = {2023-07-22},
  isbn = {978-0-89791-721-6}
}
% == BibTeX quality report for stroustrupWhyNotJust1995:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@article{suhEMPExecutionTime2017,
  ids = {suhEMPExecutionTime2017a},
  title = {{{EMP}}: Execution Time Measurement Protocol for Compute-Bound Programs},
  shorttitle = {{{EMP}}},
  author = {Suh, Young-Kyoon and Snodgrass, Richard T. and Kececioglu, John D. and Downey, Peter J. and Maier, Robert S. and Yi, Cheng},
  year = {2017},
  journal = {Software: Practice and Experience},
  volume = {47},
  number = {4},
  pages = {559--597},
  issn = {1097-024X},
  doi = {10.1002/spe.2476},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2476},
  urldate = {2023-08-07},
  abstract = {Measuring execution time is one of the most used performance evaluation techniques in computer science research. Inaccurate measurements cannot be used for a fair performance comparison between programs. Despite the prevalence of its use, the intrinsic variability in the time measurement makes it hard to obtain repeatable and accurate timing results of a program running on an operating system. We propose a novel execution time measurement protocol (termed EMP) for measuring the execution time of a compute-bound program on Linux, while minimizing that measurement's variability. During the development of execution time measurement protocol, we identified several factors that disturb execution time measurement. We introduce successive refinements to the protocol by addressing each of these factors, in concert, reducing variability by more than an order of magnitude. We also introduce a new visualization technique, what we term ‘dual-execution scatter plot’ that highlights infrequent, long-running daemons, differentiating them from frequent and/or short-running daemons. Our empirical results show that the proposed protocol successfully achieves three major aspects—precision, accuracy, and scalability—in execution time measurement that can work for open-source and proprietary software. Copyright © 2017 John Wiley \& Sons, Ltd.},
  copyright = {Copyright © 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {compute-bound programs,execution time,measurement,protocol}
}
% == BibTeX quality report for suhEMPExecutionTime2017:
% ? unused extra: _eprint ("https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2476")
% ? unused Library catalog ("Wiley Online Library")

@article{sutterFamilyLanguages2000,
  title = {The {{C}} Family of Languages},
  author = {Sutter, Herb},
  year = {2000},
  month = jul,
  journal = {Java Report},
  volume = {5},
  number = {7},
  url = {http://www.gotw.ca/publications/c_family_interview.htm},
  urldate = {2021-10-28},
  collaborator = {Ritchie, Dennis and Stroustrup, Bjarne and Gosling, James}
}

@incollection{swierstraCombinatorParsingShort2009,
  title = {Combinator Parsing: A Short Tutorial},
  shorttitle = {Combinator Parsing},
  booktitle = {Language {{Engineering}} and {{Rigorous Software Development}}: {{International LerNet ALFA Summer School}} 2008, {{Piriapolis}}, {{Uruguay}}, {{February}} 24 - {{March}} 1, 2008, {{Revised Tutorial Lectures}}},
  author = {Swierstra, S. Doaitse},
  editor = {Bove, Ana and Barbosa, Luís Soares and Pardo, Alberto and Pinto, Jorge Sousa},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {5520},
  pages = {252--300},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03153-3_6},
  url = {http://www.cs.uu.nl/research/techreps/repo/CS-2008/2008-044.pdf},
  urldate = {2023-05-18},
  abstract = {There are numerous ways to implement a parser for a given syntax; using parser combinators is a powerful approach to parsing which derives much of its power and expressiveness from the type system and semantics of the host programming language. This tutorial begins with the construction of a small library of parsing combinators. This library introduces the basics of combinator parsing and, more generally, demonstrates how domain specific embedded languages are able to leverage the facilities of the host language. After having constructed our small combinator library, we investigate some shortcomings of the naïve implementation introduced in the first part, and incrementally develop an implementation without these problems. Finally we discuss some further extensions of the presented library and compare our approach with similar libraries.},
  isbn = {978-3-642-03152-6 978-3-642-03153-3},
  langid = {english},
  keywords = {Attribute Grammar,Error Message,Pocket Calculator,Sequential Composition,Terminal Symbol}
}
% == BibTeX quality report for swierstraCombinatorParsingShort2009:
% ? unused Library catalog ("Springer Link")

@article{taivalsaariNotionInheritance1996,
  title = {On the Notion of Inheritance},
  author = {Taivalsaari, Antero},
  year = {1996},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {28},
  number = {3},
  pages = {438--479},
  issn = {0360-0300},
  doi = {10.1145/243439.243441},
  url = {https://doi.org/10.1145/243439.243441},
  urldate = {2022-12-14},
  abstract = {One of the most intriguing—and at the same time most problematic—notions in object-oriented programing is inheritance. Inheritance is commonly regarded as the feature that distinguishes object-oriented programming from other modern programming paradigms, but researchers rarely agree on its meaning and usage. Yet inheritance of often hailed as a solution to many problems hampering software development, and many of the alleged benefits of object-oriented programming, such as improved conceptual modeling and reusability, are largely credited to it. This article aims at a comprehensive understanding of inheritance, examining its usage, surveying its varieties, and presenting a simple taxonomy of mechanisms that can be seen as underlying different inheritance models.},
  keywords = {delegation,incremental modification,inheritance,language constructs,object-oriented programming,programming languages}
}
% == BibTeX quality report for taivalsaariNotionInheritance1996:
% ? unused Journal abbreviation ("ACM Comput. Surv.")
% ? unused Library catalog ("Sept. 1996")

@incollection{temperoWhatProgrammersInheritance2013,
  ids = {temperoWhatProgrammersInheritance2013a,temperoWhatProgrammersInheritance2013b},
  title = {What Programmers Do with Inheritance in {{Java}}},
  booktitle = {{{ECOOP}} 2013 – {{Object-Oriented Programming}}},
  author = {Tempero, Ewan and Yang, Hong Yul and Noble, James},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Castagna, Giuseppe},
  year = {2013},
  volume = {7920},
  pages = {577--601},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39038-8_24},
  url = {http://link.springer.com/10.1007/978-3-642-39038-8_24},
  urldate = {2022-08-31},
  abstract = {Inheritance is a distinguishing feature of object-oriented programming languages, but its application in practice remains poorly understood. Programmers employ inheritance for a number of different purposes: to provide subtyping, to reuse code, to allow subclasses to customise superclasses’ behaviour, or just to categorise objects. We present an empirical study of 93 open-source Java software systems consisting over over 200,000 classes and interfaces, supplemented by longitudinal analyses of 43 versions of two systems. Our analysis finds inheritance is used for two main reasons: to support subtyping and to permit what we call external code reuse. This is the first empirical study to indicate what programmers do with inheritance.},
  isbn = {978-3-642-39037-1 978-3-642-39038-8},
  langid = {english}
}
% == BibTeX quality report for temperoWhatProgrammersInheritance2013:
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{teneC4ContinuouslyConcurrent2011,
  ids = {teneC4ContinuouslyConcurrent},
  title = {C4: The Continuously Concurrent Compacting Collector},
  shorttitle = {C4},
  author = {Tene, Gil and Iyengar, Balaji and Wolf, Michael},
  year = {2011},
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {46},
  number = {11},
  pages = {79--88},
  issn = {0362-1340},
  doi = {10.1145/2076022.1993491},
  url = {https://doi.org/10.1145/2076022.1993491},
  urldate = {2022-06-08},
  abstract = {C4, the Continuously Concurrent Compacting Collector, an updated generational form of the Pauseless GC Algorithm [7], is introduced and described, along with details of its implementation on modern X86 hardware. It uses a read barrier to support concur- rent compaction, concurrent remapping, and concurrent incremental update tracing. C4 differentiates itself from other generational garbage collectors by supporting simultaneous-generational concurrency: the different generations are collected using concurrent (non stop-the-world) mechanisms that can be simultaneously and independently active. C4 is able to continuously perform concurrent young generation collections, even during long periods of concurrent full heap collection, allowing C4 to sustain high allocation rates and maintain the efficiency typical to generational collectors, without sacrificing response times or reverting to stop-the-world operation. Azul systems has been shipping a commercial implementation of the Pauseless GC mechanism, since 2005. Three successive generations of Azul's Vega series systems relied on custom multi-core processors and a custom OS kernel to deliver both the scale and features needed to support Pauseless GC. In 2010, Azul released its first software-only commercial implementation of C4 for modern commodity X86 hardware, using Linux kernel enhancements to support the required feature set. We discuss implementa- tion details of C4 on X86, including the Linux virtual and physical memory management enhancements that were used to support the high rate of virtual memory operations required for sustained pauseless operation. We discuss updates to the collector's manage- ment of the heap for efficient generational collection and provide throughput and pause time data while running sustained workloads.},
  langid = {english},
  keywords = {concurrent,garbage collection,genera- tional,linux,pauseless,read barrier,virtual memory}
}
% == BibTeX quality report for teneC4ContinuouslyConcurrent2011:
% ? unused Journal abbreviation ("SIGPLAN Not.")
% ? unused Library catalog ("November 2011")

@inproceedings{tewDevelopingValidatedAssessment2010,
  title = {Developing a Validated Assessment of Fundamental {{CS1}} Concepts},
  booktitle = {Proceedings of the 41st {{ACM}} Technical Symposium on {{Computer}} Science Education},
  author = {Tew, Allison Elliott and Guzdial, Mark},
  year = {2010},
  month = mar,
  pages = {97--101},
  publisher = {{ACM}},
  address = {{Milwaukee Wisconsin USA}},
  doi = {10.1145/1734263.1734297},
  url = {https://dl.acm.org/doi/10.1145/1734263.1734297},
  urldate = {2023-02-13},
  abstract = {Previous studies of student programming ability have raised questions about students’ ability to problem solve, read and analyze code, and understand introductory computing concepts. However, it is unclear whether these results are the product of failures of student comprehension or our inability to accurately measure their performance. We propose a method for creating a language independent CS1 assessment instrument and present the results of our analysis used to define the common conceptual content that will serve as the framework for the exam. We conclude with a discussion of future work and our progress towards developing the assessment.},
  isbn = {978-1-4503-0006-3},
  langid = {english}
}
% == BibTeX quality report for tewDevelopingValidatedAssessment2010:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("SIGCSE10: The 41st ACM Technical Symposium on Computer Science Education")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{tewFCS1LanguageIndependent2011,
  title = {The {{FCS1}}: A Language Independent Assessment of {{CS1}} Knowledge},
  shorttitle = {The {{FCS1}}},
  booktitle = {Proceedings of the 42nd {{ACM}} Technical Symposium on {{Computer}} Science Education},
  author = {Tew, Allison Elliott and Guzdial, Mark},
  year = {2011},
  month = mar,
  pages = {111--116},
  publisher = {{ACM}},
  address = {{Dallas TX USA}},
  doi = {10.1145/1953163.1953200},
  url = {https://dl.acm.org/doi/10.1145/1953163.1953200},
  urldate = {2023-02-13},
  abstract = {A primary goal of many CS education projects is to determine the extent to which a given intervention has had an impact on student learning. However, computing lacks valid assessments for pedagogical or research purposes. Without such valid assessments, it is difficult to accurately measure student learning or establish a relationship between the instructional setting and learning outcomes.},
  isbn = {978-1-4503-0500-6},
  langid = {english}
}
% == BibTeX quality report for tewFCS1LanguageIndependent2011:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("SIGCSE '11: The 42nd ACM Technical Symposium on Computer Science Education")
% ? unused Library catalog ("DOI.org (Crossref)")

@book{tomitaEfficientParsingNatural1986,
  ids = {tomitaEfficientParsingNatural1986a},
  title = {Efficient Parsing for Natural Language},
  author = {Tomita, Masaru},
  year = {1986},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4757-1885-0},
  url = {http://link.springer.com/10.1007/978-1-4757-1885-0},
  urldate = {2020-06-15},
  isbn = {978-1-4419-5198-4 978-1-4757-1885-0},
  langid = {english},
  keywords = {algorithms,cognition,expert system,grammar,machine translation,natural language,natural language processing,Parsing,proving,speech recognition}
}
% == BibTeX quality report for tomitaEfficientParsingNatural1986:
% ? unused Library catalog ("DOI.org (Crossref)")

@book{tremblayTheoryPracticeCompiler1985,
  title = {The {{Theory}} and {{Practice}} of {{Compiler Writing}}},
  author = {Tremblay, Jean-Paul and Sorenson, Paul G.},
  year = {1985},
  month = jan,
  edition = {First Edition},
  publisher = {{McGraw-Hill College}},
  address = {{New York}},
  abstract = {Hardcover},
  isbn = {978-0-07-065161-6},
  langid = {english}
}
% == BibTeX quality report for tremblayTheoryPracticeCompiler1985:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Amazon")
% ? unused Number of pages ("816")

@article{trinderAlgorithmStrategyParallelism1998,
  title = {Algorithm + Strategy = Parallelism},
  author = {Trinder, P. W. and Hammond, K. and Loidl, H.-W. and Jones, Simon Peyton},
  year = {1998},
  month = jan,
  journal = {Journal of Functional Programming},
  volume = {8},
  url = {https://www.microsoft.com/en-us/research/publication/algorithm-strategy-parallelism/},
  urldate = {2020-08-02},
  abstract = {The process of writing large parallel programs is complicated by the eed to specify both the parallel behaviour of the program and the algorithm that is to be used to compute its result. This paper introduces evaluation strategies: lazy higher-order functions that control the parallel evaluation of non-strict functional languages. Using evaluation strategies, it is […]},
  langid = {american}
}
% == BibTeX quality report for trinderAlgorithmStrategyParallelism1998:
% ? unused Library catalog ("www.microsoft.com")

@article{uustaluComonadicNotionsComputation2008,
  title = {Comonadic Notions of Computation},
  author = {Uustalu, Tarmo and Vene, Varmo},
  year = {2008},
  month = jun,
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = {203},
  number = {5},
  pages = {263--284},
  issn = {15710661},
  doi = {10.1016/j.entcs.2008.05.029},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1571066108003435},
  urldate = {2023-01-17},
  abstract = {We argue that symmetric (semi)monoidal comonads provide a means to structure context-dependent notions of computation such as notions of dataflow computation (computation on streams) and of tree relabelling as in attribute evaluation. We propose a generic semantics for extensions of simply typed lambda calculus with context-dependent operations analogous to the Moggi-style semantics for effectful languages based on strong monads. This continues the work in the early 90s by Brookes, Geva and Van Stone on the use of computational comonads in intensional semantics.},
  langid = {english}
}
% == BibTeX quality report for uustaluComonadicNotionsComputation2008:
% ? unused Library catalog ("DOI.org (Crossref)")

@phdthesis{vanoostromConfluenceAbstractHigherorder1994,
  title = {Confluence for Abstract and Higher-Order Rewriting},
  author = {{van Oostrom}, V.},
  year = {1994},
  school = {Vrije Universiteit Amsterdam}
}
% == BibTeX quality report for vanoostromConfluenceAbstractHigherorder1994:
% ? unused Type ("PhD")

@article{venturinizilliReductionGraphsLambda1984,
  ids = {venturiniReductionGraphsLambda1983},
  title = {Reduction Graphs in the Lambda Calculus},
  author = {Venturini Zilli, Marissa},
  year = {1984},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {29},
  number = {3},
  pages = {251--275},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(84)90002-1},
  url = {https://www.sciencedirect.com/science/article/pii/0304397584900021},
  urldate = {2022-06-08},
  abstract = {In this paper properties of the reduction graphs of lambda terms are studied and some classes of reduction graphs are characterized. Condensed reduction graphs obtained by dividing out ‘cyclic equivalence’, and spectra, the partially ordered set of all reductions, are also considered. The partial ordering in the spectrum can be seen as a measure for the ‘significance’ of a reduction; reductions to the normal form and (more generally) cofinal reductions are the most significant reductions. The spectrum is proved to be the completion of the condensed reduction graph.},
  langid = {english}
}
% == BibTeX quality report for venturinizilliReductionGraphsLambda1984:
% ? unused Library catalog ("ScienceDirect")

@book{visserSyntaxDefinitionLanguage1997,
  title = {Syntax Definition for Language Prototyping},
  author = {Visser, Eelco},
  year = {1997},
  publisher = {{University}},
  address = {{Amsterdam}},
  isbn = {978-90-74795-75-3},
  langid = {english},
  note = {Zugl.: Amsterdam, Univ., Diss., 1997}
}
% == BibTeX quality report for visserSyntaxDefinitionLanguage1997:
% ? unused Library catalog ("K10plus ISBN")
% ? unused Number of pages ("383")

@incollection{voigtlanderAsymptoticImprovementComputations2008,
  title = {Asymptotic Improvement of Computations over Free Monads},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {Voigtländer, Janis},
  editor = {Audebaud, Philippe and {Paulin-Mohring}, Christine},
  year = {2008},
  volume = {5133},
  pages = {388--403},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-540-70594-9_20},
  url = {https://www.janis-voigtlaender.eu/papers/AsymptoticImprovementOfComputationsOverFreeMonads.pdf},
  urldate = {2021-07-09},
  abstract = {We present a low-effort program transformation to improve the efficiency of computations over free monads in Haskell. The development is calculational and carried out in a generic setting, thus applying to a variety of datatypes. An important aspect of our approach is the utilisation of type class mechanisms to make the transformation as transparent as possible, requiring no restructuring of code at all. There is also no extra support necessary from the compiler (apart from an up-to-date type checker). Despite this simplicity of use, our technique is able to achieve true asymptotic runtime improvements. We demonstrate this by examples for which the complexity is reduced from quadratic to linear.},
  isbn = {978-3-540-70593-2 978-3-540-70594-9},
  langid = {english}
}
% == BibTeX quality report for voigtlanderAsymptoticImprovementComputations2008:
% ? unused Library catalog ("DOI.org (Crossref)")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{vukmirovicEfficientFullHigherorder2021,
  title = {Efficient Full Higher-Order Unification},
  author = {Vukmirović, Petar and Bentkamp, Alexander and Nummelin, Visa},
  year = {2021},
  month = dec,
  journal = {Logical Methods in Computer Science},
  volume = {Volume 17, Issue 4},
  publisher = {{Episciences.org}},
  doi = {10.46298/lmcs-17(4:18)2021},
  url = {https://lmcs.episciences.org/8837/pdf},
  urldate = {2022-06-14},
  abstract = {We developed a procedure to enumerate complete sets of higher-order unifiers based on work by Jensen and Pietrzykowski. Our procedure removes many redundant unifiers by carefully restricting the search space and tightly integrating decision procedures for fragments that admit a finite complete set of unifiers. We identify a new such fragment and describe a procedure for computing its unifiers. Our unification procedure, together with new higher-order term indexing data structures, is implemented in the Zipperposition theorem prover. Experimental evaluation shows a clear advantage over Jensen and Pietrzykowski's procedure.}
}
% == BibTeX quality report for vukmirovicEfficientFullHigherorder2021:
% ? unused Library catalog ("lmcs.episciences.org")

@inproceedings{wadlerCallbyvalueDualCallbyname2003,
  ids = {wadlerCallbyValueDualCallbyName},
  title = {Call-by-Value Is Dual to Call-by-Name},
  booktitle = {Proceedings of the Eighth {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  author = {Wadler, Philip},
  year = {2003},
  month = aug,
  series = {{{ICFP}} '03},
  pages = {189--201},
  publisher = {{Association for Computing Machinery}},
  address = {{Uppsala, Sweden}},
  doi = {10.1145/944705.944723},
  url = {http://homepages.inf.ed.ac.uk/wadler/papers/dual/dual.pdf},
  urldate = {2020-06-17},
  abstract = {The rules of classical logic may be formulated in pairs corresponding to De Morgan duals: rules about \& are dual to rules about V. A line of work, including that of Filinski (1989), Griffin (1990), Parigot (1992), Danos, Joinet, and Schellinx (1995), Selinger (1998,2001), and Curien and Herbelin (2000), has led to the startling conclusion that call-by-value is the de Morgan dual of call-by-name.This paper presents a dual calculus that corresponds to the classical sequent calculus of Gentzen (1935) in the same way that the lambda calculus of Church (1932,1940) corresponds to the intuitionistic natural deduction of Gentzen (1935). The paper includes crisp formulations of call-by-value and call-by-name that are obviously dual; no similar formulations appear in the literature. The paper gives a CPS translation and its inverse, and shows that the translation is both sound and complete, strengthening a result in Curien and Herbelin (2000).},
  isbn = {978-1-58113-756-9},
  keywords = {Curry-Howard correspondence,De Morgan dual,lambda calculus,lambda mu calculus,logic,natural deduction,sequent calculus}
}
% == BibTeX quality report for wadlerCallbyvalueDualCallbyname2003:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("International Conference on Functional Programming")
% ? unused Library catalog ("ACM Digital Library")

@inproceedings{wadlerEssenceFunctionalProgramming1992,
  title = {The Essence of Functional Programming},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '92},
  author = {Wadler, Philip},
  year = {1992},
  pages = {1--14},
  publisher = {{ACM Press}},
  address = {{Albuquerque, New Mexico, United States}},
  doi = {10.1145/143165.143169},
  url = {http://portal.acm.org/citation.cfm?doid=143165.143169},
  urldate = {2021-11-22},
  abstract = {This paper explores the use monads to structure functional programs. No prior knowledge of monads or category theory is required. Monads increase the ease with which programs may be modified. They can mimic the effect of impure features such as exceptions, state, and continuations; and also provide effects not easily achieved with such features. The types of a program reflect which effects occur.},
  isbn = {978-0-89791-453-6},
  langid = {english}
}
% == BibTeX quality report for wadlerEssenceFunctionalProgramming1992:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 19th ACM SIGPLAN-SIGACT symposium")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{wadlerHowMakeAdhoc1989,
  title = {How to Make Ad-Hoc Polymorphism Less Ad Hoc},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '89},
  author = {Wadler, P. and Blott, S.},
  year = {1989},
  pages = {60--76},
  publisher = {{ACM Press}},
  address = {{Austin, Texas, United States}},
  doi = {10.1145/75277.75283},
  url = {http://portal.acm.org/citation.cfm?doid=75277.75283},
  urldate = {2022-06-07},
  isbn = {978-0-89791-294-5},
  langid = {english}
}
% == BibTeX quality report for wadlerHowMakeAdhoc1989:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 16th ACM SIGPLAN-SIGACT symposium")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{walkerFontTuningReview2008,
  title = {Font Tuning: {{A}} Review and New Experimental Evidence},
  shorttitle = {Font Tuning},
  author = {Walker, Peter},
  year = {2008},
  month = nov,
  journal = {Visual Cognition},
  volume = {16},
  number = {8},
  pages = {1022--1058},
  issn = {1350-6285, 1464-0716},
  doi = {10.1080/13506280701535924},
  url = {http://www.tandfonline.com/doi/abs/10.1080/13506280701535924},
  urldate = {2023-01-13},
  abstract = {This paper reflects on the kinds of evidence able to confirm that letter and word identification in reading can be supported by encoding the underlying visual structure of the text, and specifically by deriving structural descriptions for letters. It is proposed that structure-driven processes are intimately linked to the implementation of font-specific rules for translating visual features into elements of a letter’s structural description. Evidence for such font tuning comes from studies exploring the impact of font-mixing on reading fluency, and from studies showing how the benefits of experience with a novel typeface can generalise to letters that have yet to be seen in the typeface. After reviewing this evidence, three new experiments are reported which explore font tuning in the context of the lexical decision task. The time course of font tuning, which is monitored by changing the time interval between successive test stimuli, is shown to be sensitive to the overall probability with which successive stimuli appear in the same typeface. In addition, font tuning is shown to reflect item-by-item fluctuations in this probability. Finally, the effects of font-switching are shown to generalise beyond the particular letters present in the text, and to be confined to 1-back transitions. It is concluded that font tuning reflects the implementation of a set of font-specific translation rules held in working memory, and is moderated by the reader’s implicit knowledge of the constraints present in the sequencing of successive portions of text.},
  langid = {english}
}
% == BibTeX quality report for walkerFontTuningReview2008:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{walstonMethodProgrammingMeasurement1977,
  title = {A Method of Programming Measurement and Estimation},
  author = {Walston, C. E. and Felix, C. P.},
  year = {1977},
  journal = {IBM Systems Journal},
  volume = {16},
  number = {1},
  pages = {54--73},
  issn = {0018-8670},
  doi = {10.1147/sj.161.0054},
  url = {http://ieeexplore.ieee.org/document/5388069/},
  urldate = {2023-12-30},
  langid = {english}
}
% == BibTeX quality report for walstonMethodProgrammingMeasurement1977:
% ? unused Journal abbreviation ("IBM Syst. J.")
% ? unused Library catalog ("DOI.org (Crossref)")

@misc{wankadiaRedgrepRegularExpression2013,
  title = {Redgrep: From Regular Expression Derivatives to {{LLVM}}},
  author = {Wankadia, Paul},
  year = {2013},
  month = mar,
  address = {{Linux.conf.au 2013}},
  url = {https://www.youtube.com/watch?v=ZJOgDovsIsg},
  urldate = {2021-02-09}
}
% == BibTeX quality report for wankadiaRedgrepRegularExpression2013:
% ? unused Library catalog ("YouTube")
% ? unused Running time ("36:25")

@inproceedings{warthWorldsControllingScope2011,
  title = {Worlds: Controlling the Scope of Side Effects},
  shorttitle = {Worlds},
  author = {Warth, Alessandro and Ohshima, Yoshiki and Kaehler, Ted and Kay, Alan},
  year = {2011},
  month = jul,
  volume = {6813},
  pages = {179--203},
  doi = {10.1007/978-3-642-22655-7_9},
  abstract = {The state of an imperative program—e.g., the values stored in global and local variables, arrays, and objects’ instance variables—changes as its statements are executed. These changes, or side effects, are visible globally: when one part of the program modifies an object, every other part that holds a reference to the same object (either directly or indirectly) is also affected. This paper introduces worlds, a language construct that reifies the notion of program state and enables programmers to control the scope of side effects. We investigate this idea by extending both JavaScript and Squeak Smalltalk with support for worlds, provide examples of some of the interesting idioms this construct makes possible, and formalize the semantics of property/field lookup in the presence of worlds. We also describe an efficient implementation strategy (used in our Squeak-based prototype), and illustrate the practical benefits of worlds with two case studies.},
  isbn = {978-3-642-22654-0}
}
% == BibTeX quality report for warthWorldsControllingScope2011:
% Missing required field 'booktitle'
% ? unused Library catalog ("ResearchGate")

@article{weijlandSemanticsLogicPrograms1990,
  title = {Semantics for Logic Programs without Occur Check},
  author = {Weijland, W.P.},
  year = {1990},
  month = mar,
  journal = {Theoretical Computer Science},
  volume = {71},
  number = {1},
  pages = {155--174},
  issn = {03043975},
  doi = {10.1016/0304-3975(90)90194-M},
  url = {https://linkinghub.elsevier.com/retrieve/pii/030439759090194M},
  urldate = {2022-07-15},
  langid = {english}
}
% == BibTeX quality report for weijlandSemanticsLogicPrograms1990:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{weissOxideEssenceRust2019,
  title = {Oxide: The Essence of {{Rust}}},
  shorttitle = {Oxide},
  author = {Weiss, Aaron and Patterson, Daniel and Matsakis, Nicholas D. and Ahmed, Amal},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.00982 [cs]},
  eprint = {1903.00982},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1903.00982},
  urldate = {2020-05-17},
  abstract = {Rust is a major advancement in industrial programming languages due in large part to its success in bridging the gap between low-level systems programming and high-level application programming. This success has ultimately empowered programmers to more easily build reliable and efficient software, and at its heart lies a novel approach to ownership that balances type system expressivity with usability. In this work, we set out to capture the essence of this model of ownership by developing a type systems account of Rust's borrow checker. To that end, we present Oxide, a formalized programming language close to source-level Rust (but with fully-annotated types). This presentation takes a new view of lifetimes as approximate provenances of references, and our type system is able to automatically compute this information through a flow-sensitive substructural typing judgment for which we prove syntactic type safety using progress and preservation. The result is a simpler formulation of borrow checking - including recent features such as non-lexical lifetimes - that we hope researchers will be able to use as the basis for work on Rust.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Programming Languages},
  note = {Comment: In submission}
}
% == BibTeX quality report for weissOxideEssenceRust2019:
% ? Possibly abbreviated journal title arXiv:1903.00982 [cs]

@article{wheelerFullyCounteringTrusting2010,
  title = {Fully Countering Trusting Trust through Diverse Double-Compiling},
  author = {Wheeler, David A.},
  year = {2010},
  month = apr,
  journal = {arXiv:1004.5534 [cs]},
  eprint = {1004.5534},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1004.5534},
  urldate = {2020-09-13},
  abstract = {An Air Force evaluation of Multics, and Ken Thompson's Turing award lecture ("Reflections on Trusting Trust"), showed that compilers can be subverted to insert malicious Trojan horses into critical software, including themselves. If this "trusting trust" attack goes undetected, even complete analysis of a system's source code will not find the malicious code that is running. Previously-known countermeasures have been grossly inadequate. If this attack cannot be countered, attackers can quietly subvert entire classes of computer systems, gaining complete control over financial, infrastructure, military, and/or business systems worldwide. This dissertation's thesis is that the trusting trust attack can be detected and effectively countered using the "Diverse Double-Compiling" (DDC) technique, as demonstrated by (1) a formal proof that DDC can determine if source code and generated executable code correspond, (2) a demonstration of DDC with four compilers (a small C compiler, a small Lisp compiler, a small maliciously corrupted Lisp compiler, and a large industrial-strength C compiler, GCC), and (3) a description of approaches for applying DDC in various real-world scenarios. In the DDC technique, source code is compiled twice: the source code of the compiler's parent is compiled using a trusted compiler, and then the putative compiler source code is compiled using the result of the first compilation. If the DDC result is bit-for-bit identical with the original compiler-under-test's executable, and certain other assumptions hold, then the compiler-under-test's executable corresponds with its putative source code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Programming Languages},
  note = {Comment: PhD dissertation. Accepted by George Mason University, Fairfax, Virginia, USA's Volgenau School of Information Technology and Engineering in 2009. 199 single-side printed pages.}
}
% == BibTeX quality report for wheelerFullyCounteringTrusting2010:
% ? Possibly abbreviated journal title arXiv:1004.5534 [cs]

@article{wikipediaForwardBackwardAlgorithm2020,
  title = {Forward–Backward Algorithm},
  author = {{Wikipedia}},
  year = {2020},
  month = jul,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Forward%E2%80%93backward_algorithm&oldid=966683884},
  urldate = {2020-07-08},
  abstract = {The forward–backward algorithm is an  inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions                                    o                        1             :             T                             :=                    o                        1                             ,         …         ,                    o                        T                                     \{\textbackslash displaystyle o\_\{1:T\}:=o\_\{1\},\textbackslash dots ,o\_\{T\}\}   , i.e. it computes, for all hidden state variables                                    X                        t                             ∈         \{                    X                        1                             ,         …         ,                    X                        T                             \}                 \{\textbackslash displaystyle X\_\{t\}\textbackslash in \textbackslash\{X\_\{1\},\textbackslash dots ,X\_\{T\}\textbackslash\}\}   , the distribution                         P         (                    X                        t                                                  |                                       o                        1             :             T                             )                 \{\textbackslash displaystyle P(X\_\{t\}\textbackslash{} |\textbackslash{} o\_\{1:T\})\}   . This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to efficiently compute the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm. The term forward–backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 966683884}
}

@article{wikipediaLogit2020,
  title = {Logit},
  author = {{Wikipedia}},
  year = {2020},
  month = jun,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Logit&oldid=964387041},
  urldate = {2020-07-07},
  abstract = {In statistics, the logit ( LOH-jit) function or the log-odds is the logarithm of the odds                                                 p                            1               −               p                                                  \{\textbackslash displaystyle \{\textbackslash frac \{p\}\{1-p\}\}\}    where p is probability. It is a type of function that creates a map of probability values from                         (         0         ,         1         )                 \{\textbackslash displaystyle (0,1)\}    to                         (         −         ∞         ,         +         ∞         )                 \{\textbackslash displaystyle (-\textbackslash infty ,+\textbackslash infty )\}   . It is the inverse of the sigmoidal "logistic" function or logistic transform used in mathematics, especially in statistics. In deep learning, the term logits layer is popularly used for the last neuron layer of neural networks used for classification tasks, which produce raw prediction values as real numbers ranging from                         (         −         ∞         ,         +         ∞         )                 \{\textbackslash displaystyle (-\textbackslash infty ,+\textbackslash infty )\}   .},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 964387041}
}

@inproceedings{yangDeadStoreElimination2017,
  title = {Dead Store Elimination (Still) Considered Harmful},
  booktitle = {26th {{USENIX Security Symposium}}},
  author = {Yang, Zhaomo and Johannesmeyer, Brian and Olesen, Anders Trier and Lerner, Sorin and Levchenko, Kirill},
  year = {2017},
  month = aug,
  address = {{Vancouver, BC, Canada}},
  abstract = {Dead store elimination is a widely used compiler optimization that reduces code size and improves performance. However, it can also remove seemingly useless memory writes that the programmer intended to clear sensitive data after its last use. Security-savvy developers have long been aware of this phenomenon and have devised ways to prevent the compiler from eliminating these data scrubbing operations.},
  isbn = {978-1-931971-40-9},
  langid = {english}
}
% == BibTeX quality report for yangDeadStoreElimination2017:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Zotero")

@article{yaofeichenEmpiricalStudyProgramming2005,
  title = {An Empirical Study of Programming Language Trends},
  author = {{Yaofei Chen} and Dios, R. and Mili, A. and {Lan Wu} and {Kefei Wang}},
  year = {2005},
  month = may,
  journal = {IEEE Software},
  volume = {22},
  number = {3},
  pages = {72--78},
  issn = {0740-7459},
  doi = {10.1109/MS.2005.55},
  url = {http://ieeexplore.ieee.org/document/1438333/},
  urldate = {2023-06-17},
  langid = {english}
}
% == BibTeX quality report for yaofeichenEmpiricalStudyProgramming2005:
% ? unused Journal abbreviation ("IEEE Softw.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{yodaikenHowISOBecame2021,
  title = {How {{ISO C}} Became Unusable for Operating Systems Development},
  booktitle = {Proceedings of the 11th {{Workshop}} on {{Programming Languages}} and {{Operating Systems}}},
  author = {Yodaiken, Victor},
  year = {2021},
  month = oct,
  eprint = {2201.07845},
  primaryclass = {cs},
  pages = {84--90},
  doi = {10.1145/3477113.3487274},
  url = {http://arxiv.org/abs/2201.07845},
  urldate = {2022-10-06},
  abstract = {The C programming language was developed in the 1970s as a fairly unconventional systems and operating systems development tool, but has, through the course of the ISO Standards process, added many attributes of more conventional programming languages and become less suitable for operating systems development. Operating system programming continues to be done in non-ISO dialects of C. The differences provide a glimpse of operating system requirements for programming languages.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Operating Systems,Computer Science - Programming Languages,D.3,D.4,{d.4, d.3}},
  note = {Comment: PLOS '21: Proceedings of the 11th Workshop on Programming Languages and Operating Systems October 2021}
}

@misc{zeiglerComparingDevelopmentCosts1995,
  title = {Comparing Development Costs of {{C}} and {{Ada}}},
  author = {Zeigler, Stephen F.},
  year = {1995},
  month = mar,
  url = {http://archive.adaic.com/intro/ada-vs-c/cada_art.html},
  urldate = {2023-12-29}
}

@phdthesis{zeilbergerLogicalBasisEvaluation2009,
  title = {The Logical Basis of Evaluation Order and Pattern-Matching},
  author = {Zeilberger, Noam},
  year = {2009},
  month = apr,
  address = {{Pittsburgh, PA}},
  url = {https://software.imdea.org/~noam.zeilberger/thesis.pdf},
  langid = {english},
  school = {Carnegie Mellon University}
}
% == BibTeX quality report for zeilbergerLogicalBasisEvaluation2009:
% ? unused Archive location ("CMU-CS-09-122")
% ? unused Library catalog ("Zotero")
% ? unused Number of pages ("203")

@inproceedings{ziftciWhoBrokeBuild2017,
  title = {Who Broke the Build? {{Automatically}} Identifying Changes That Induce Test Failures in Continuous Integration at {{Google}} Scale},
  shorttitle = {Who Broke the Build?},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice Track}} ({{ICSE-SEIP}})},
  author = {Ziftci, Celal and Reardon, Jim},
  year = {2017},
  month = may,
  pages = {113--122},
  publisher = {{IEEE}},
  address = {{Buenos Aires}},
  doi = {10.1109/ICSE-SEIP.2017.13},
  url = {https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45794.pdf},
  urldate = {2020-07-06},
  abstract = {Quickly identifying and fixing code changes that introduce regressions is critical to keep the momentum on software development, especially in very large scale software repositories with rapid development cycles, such as at Google. Identifying and fixing such regressions is one of the most expensive, tedious, and time consuming tasks in the software development life-cycle. Therefore, there is a high demand for automated techniques that can help developers identify such changes while minimizing manual human intervention. Various techniques have recently been proposed to identify such code changes. However, these techniques have shortcomings that make them unsuitable for rapid development cycles as at Google. In this paper, we propose a novel algorithm to identify code changes that introduce regressions, and discuss case studies performed at Google on 140 projects. Based on our case studies, our algorithm automatically identifies the change that introduced the regression in the top-5 among thousands of candidates 82\% of the time, and provides considerable savings on manual work developers need to perform.},
  isbn = {978-1-5386-2717-4},
  langid = {english}
}
% == BibTeX quality report for ziftciWhoBrokeBuild2017:
% ? unused Conference name ("2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)")
% ? unused Library catalog ("DOI.org (Crossref)")
